[{"title":"配置RouterOS从内网访问光猫","url":"/access-to-fiber-optic-modem-through-lan.html","content":"我的宽带是自己换了光猫改成了桥接的，平时用起来没问题，只是我很不爽不能直接通过内网访问到光猫的配置页面。倒不是要经常改光猫配置，只是又犯了折腾病，想要把这一块缺失的拼图补上。一开始以为是配一条静态路由的事，但是发现并不奏效。上网学习了别人的成功经验之后，发现我的方向彻底错了，好在正确的操作也并不复杂。\n因为我用的是搭载RouterOS的Mikrotik路由器，所以文中涉及的命令都将是RouterOS的语法。不过我也会讲明操作的原理，所以你也可以跟着思路针对你的路由器进行配置。唯一需要注意的就是，要确保你的路由器可以自己配置iptables规则。\n\n\n如下是我的网络拓扑，其中：\n\n光猫的管理地址为192.168.0.1，独占192.168.0.0/24网段\n路由器的地址为192.168.1.1，它和内网的各个设备占用192.168.1.0/24网段\n\n\n那么要通过路由器访问到光猫的话，只需要做下面三件事：\n\n给路由器的WAN口分配一个192.168.0.0/24网段的地址，比如192.168.0.2\n\n/ip/address/add address=192.168.0.2/24 comment=&quot;Modem access&quot; interface=ether1 network=192.168.0.0\n\n\n在防火墙的nat表中配置如下规则\n\n/ip/firewall/nat/add action=masquerade chain=srcnat comment=&quot;Access to modem subnet&quot; dst-address=192.168.0.0/24 src-address=192.168.1.0/24\n\n这条规则可以让路由器在收到向光猫发出的数据包后进行一次源地址伪装，使其看起来像是从路由器发出的，并在路由器收到响应的数据包后，再将其目标地址和端口修改会发出请求的电脑。\n\n在防火墙的mangle表中配置如下规则\n\n/ip/firewall/mangle add action=accept chain=prerouting comment=&quot;Access to modem subnet&quot; dst-address=192.168.0.0/24 src-address=192.168.1.0/24\n\n这条规则将告知路由器在收到从内网访问光猫的数据包时，直接放行这个数据包，不再进行任何其他的标记或改变路由行为的操作。这条规则并不是必须的，但是如果你在mangle表中有其他的规则，那还是加上它比较好，以避免访问到光猫的数据包被错误的标记，或者产生非预期的行为。\n上面两条RouterOS命令我也用ChatGPT翻译成了iptables命令，供参考：\n# /ip firewall nat add action=masquerade chain=srcnat comment=&quot;Access to modem subnet&quot; dst-address=192.168.0.0/24 src-address=192.168.1.0/24iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -d 192.168.0.0/24 -j MASQUERADE -m comment --comment &quot;Access to modem subnet&quot;# /ip firewall/mangle add action=accept chain=prerouting comment=&quot;Access to modem subnet&quot; dst-address=192.168.0.0/24 src-address=192.168.1.0/24iptables -t mangle -A PREROUTING -s 192.168.1.0/24 -d 192.168.0.0/24 -j ACCEPT -m comment --comment &quot;Access to modem subnet&quot;\n","categories":["瞎折腾"],"tags":["光猫","桥接","RouterOS"]},{"title":"如何允许非root进程绑定低位端口","url":"/allow-non-root-process-to-bind-low-numbered-ports.html","content":"众所周知，在Linux中，要想绑定端口号在1024以下的低位端口，是需要root权限的。但是，如果我又想绑定低位端口，又不想以root身份运行程序，该怎么办？答案是，setcap命令。\n\n\n怎么做TL;DR，使用如下命令给这个程序赋予CAP_NET_BIND_SERVICE能力即可。注意，这条命令需要以root身份执行。\nsudo setcap CAP_NET_BIND_SERVICE=+eip /path/to/binary\n\n这到底在干嘛接下来，我就把上面这条命令一点点拆开，解释清楚它到底干了什么。\nCapabilities首先介绍一下capabilities(能力)这个东西。\n在Linux内核版本2.2开始，Linux将一系列的超级管理员权限细分成了一个个可以单独开启关闭的单元，以提供更细粒度的权限控制，这些单元，就被称之为capabilities。详细的capabilities列表可以参考Man Page Capabilities(7)。\nCAP_NET_BIND_SERVICE拥有这个capability的程序，就可以绑定端口号在1024以下的特权端口。\nsetcap那么，该如何控制每个capability呢？答案就是setcap命令。上文所提到的命令，就是给指定的这个二进制程序增加CAP_NET_BIND_SERVICE这个capability。\n在capability名后面，用加号相连接的，则是开启这个capability的模式。模式有如下三种：\n\ne: Effective，意为这个capability是启用的。\np: Permitted，意为这个capability是允许被使用的。\ni: inherited，意为这个capability可以被其子进程继承。\n\n在setcap命令中，使用加号来开启这个模式，或者使用减号来关闭这个模式。\n有什么副作用这个方法确实有一些副作用，或者说是限制：\n\n这个方法对脚本无效。如果要使某个脚本拥有这个能力，则需要为其解释器赋予这个能力，而这明显是一个巨大的安全隐患。\nLinux会为使用了setcap或suid的程序禁用掉LD_LIBRARY_PATH。\n\n除了手动指定，还有没有其他办法Systemd也支持在service的配置文件中指定capabilities，其用法示例如下：\n[Service]# 该服务仅可以使用哪些capabilitiesCapabilityBoundingSet=CAP_NET_BIND_SERVICE# 以非特权用户运行程序时需要设定此参数AmbientCapabilities=CAP_NET_BIND_SERVICE\n\n参考资料\nMan Page Capabilities(7)\nMan Page setcap(8)\nMan Page cap_from_text(3)\ngetcap, setcap and file capabilities - insecure.ws\nIs there a way for non-root processes to bind to “privileged” ports on Linux? - Stack Overflow\nLinux的capabilities机制\n\n","categories":["小技巧"],"tags":["setcap","low numbered ports"]},{"title":"Servlet和Spring MVC是不是线程安全的","url":"/are-servlet-and-spring-mvc-thread-safe.html","content":"太长不看：默认情况下servlet不是线程安全的；Spring bean是否线程安全取决于这个bean的scope。\n\n\nServlet为什么不是线程安全的单个servlet实例中的方法会被多个线程同时调用很多次，而servlet类中的变量是引用传递，多个线程同时存取某个变量时，就会产生线程安全问题。\n以下方法可以让servlet或其中的一部分代码块变成线程安全的：\n\n使用syncronized关键字。\n\nimport java.io.*;import javax.servlet.*;import javax.servlet.http.*;public class MyServlet extends HttpServlet &#123;    public void doGet (HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123;        //pre-processing        synchronized (this) &#123;            //code in this block is thread-safe        &#125;        //other processing;    &#125;&#125;\n\n\n使用SingleThreadModel类\n\n如果在Sun Java System Web Server 7.0中部署单线程servlet时，servlet引擎会创建一个servlet实例池，并保持内存中有多个servlet实例的副本。要想调整实例池中的实例数量，可以调整Web Server中的singleThreadedServletPoolSize属性。这种情况下，服务器的性能可能会降低，如果池中的所有实例全部被占用，那么新到来的请求就必须要在队列中等待某个实例被释放。\nimport java.io.*;import javax.servlet.*;import javax.servlet.http.*;public class myServlet extends HttpServlet implements SingleThreadModel &#123;    //servlet methods...&#125;\n\nSpring Bean什么时候是线程安全的，什么时候不是首先，singleton的bean不是线程安全的。在不使用@Lazy注解时，框架会在启动时就初始化好singleton bean。但是框架不会管理开发人员怎么用这些singleton bean，换言之，开发人员要自己处理线程安全问题。\n而被@RequestScope注解标记的bean是线程安全的，因为这些bean不会在线程之间共享，而是在每次请求到来时都会创建新的实例。\nSession scope的bean也不是线程安全的，因为这些bean会与用户的会话绑定，每有一个新用户发来请求，就会创建一个新的bean。但是，用户发来的请求可能是并行的，所以也有可能产生线程安全问题。\nPrototype bean是不是线程安全取决于它与哪种bean绑定。如果它是被一个singleton bean所依赖，因为singleton bean不是线程安全的，这个prototype bean也将被多个线程访问，所以此时prototype bean不是线程安全的；如果它被一个request scope的bean所依赖，那么这个prototype bean就是线程安全的，因为它会随着request scoped bean消亡而消亡，同时不会被多个线程共享。\n除此之外，如果一个bean是无状态的，那么无论是什么scope，它都是线程安全的。\n参考文档\nHandling Threading Issues\nSpring bean thread safety guide\n\n","categories":["学知识"],"tags":["Java","多线程","线程安全"]},{"title":"在macOS中搭建自己的DNS服务器","url":"/build-dns-server-on-macos.html","content":"最近实验了一下配置nginx多站点，顺便也给自己跑在本机上的几个服务上了个域名(当然是直接用Hosts强行解析到127.0.0.1的……)。但是吧，用Hosts强行解析，总觉得有点别扭，所以试着在本机搭一个DNS服务器。\n\n\n前提条件\n一台安装有macOS的电脑(不过，本文使用的dnsmasq在任何一个UNIX-like操作系统上也可以使用，所以要说成”一台安装有UNIX-like操作系统的电脑”也可以。至于Windows？抱歉我懒得去试。)\nHomebrew或类似的包管理工具(或者您要是愿意，编译安装也不是不可以，只要您能解决一路上遇到的问题)\n一个终端模拟器\n\n安装安装过程很简单，使用包管理工具安装即可\nbrew install dnsmasq\n\n配置安装成功之后，编辑/usr/local/etc/dnsmasq.conf文件，修改如下内容：\n# Never forward plain names (without a dot or domain part)domain-needed# Never forward addresses in the non-routed address spaces.bogus-priv# 将所有.local的域名全部解析到本机回环地址address=/local/127.0.0.1address=/local/::1# 不读入/etc/hostsno-hosts# 如果不想dnsmasq载入/etc/resolv.conf，则解除该行注释#no-resolv\n\n然后我这里希望仍然使用路由器作为主要的DNS服务器，dnsmasq仅用来解析.local域名，所以还需要配置系统的&#x2F;etc&#x2F;resolver。注意：这一步操作仅在macOS中测试通过，不保证其他操作系统下的可用性\n# 首先创建/etc/resolver目录sudo mkdir -p /etc/resolver# 然后配置local域名使用127.0.0.1上的DNS解析echo &quot;nameserver 127.0.0.1&quot; &gt; local\n\n测试在这之前，我已经在本机配置了nginx服务器，并将Aria2前端配置了域名aria.boris1993.local，所以我使用浏览器直接访问这个域名，打开成功，Q.E.D.注意：nslookup貌似不会读取&#x2F;etc&#x2F;resolver的配置，至少在我的电脑上，nslookup aria.boris1993.local的结果是NXDOMAIN\n后续果然，没文化真可怕。现在得知，macOS支持mDNS，系统会取电脑的主机名，将其转为全小写，并使用横线替换空格，最后在其后面附加.local，来作为本机的域名。比如我的主机名是Boris-MacBook Pro，那么系统生成的本机域名就是boris-macbook-pro.local，使用这个域名就可以访问本机的服务了。上面这一顿操作猛如虎，白干了，23333\n","categories":["瞎折腾"],"tags":["DNS Server"]},{"title":"更改Google Play市场使用的区域","url":"/change-google-play-market-location.html","content":"首先需要注意的一点是，目前所谓Market Unlocker等伪装工具已经失效了，Google判定你商店区域的依据之一，是你登记在Google Payments中的当前住址。你登记的住址在哪，你的商店就在哪。\n那么事情就简单了，如果要转区的话，在保证你目前该账户 余额小于$10(或等额货币) 时，更改Google Payments中该账户当前住址为你想要的地区的住址即可，然后在手持设备上 删除这个账户 并 重新登录 ，然后就可以看到转区成功了。\n不过转区前区域的余额是无法在转区后的区域中使用的，比如你从美区转到日区，那么你账户里的美刀就冻在里面花不出去了，除非你再转回美区。\n\n参考资料：\n\nGOOGLE 帳戶的地區判定對使用者有哪些影響\n\n","categories":["其他"],"tags":["Google","Google Play"]},{"title":"100块自制短信转发器","url":"/building-a-sms-forwarder-with-air780e.html","content":"迫于手上有张KnowRoaming的卡用来收短信，但是为了这玩意再照顾个手机又很烦，正好看到合宙有卖4G模块，还有人已经实现了短信转发功能。本着生命在于折腾的精神，果断下单了一张开始搞。\n\n\n成本\nAir780E + ESP32S3 套餐 - ￥66.80\n\n因为KnowRoaming手机卡只能收短信，不能发短信，也没有数据流量，而Air780e正好不支持连接Wi-Fi，所以需要搭配ESP32S3。ESP32S3用来连接Wi-Fi，并作为主控板，通过UART来连接和驱动Air780e。\n\n\n16pin排母 - ￥15\n\n合宙只给送了排针，没有排母（甚至不卖这玩意），但是直接把ESP32S3和Air780e焊死显然不合适，所以只能自己再买排母。￥15一盒里面有5对，所以实际分摊下来只有￥3。\n\n\nUSB Type-C转Type-B双公头转接头 - ￥9.9\n\n开发过程中可以用线凑合，但是我最后打算直插在UPS的USB口上，所以转换头必不可少。\n\n\n找师傅焊接的手工费 - ￥50\n\n这没啥说的，自己又没有电烙铁又没这技术，就找了个修手机的铺子，让老板帮忙焊的。老板一边焊，一边说他这含银焊锡用在这上面都浪费……\n焊接和组装先上个全家福~\n\n焊排针和排座没啥好说的，就照着下图的样子焊就行。\n\n\n\nESP32S3\nAir780e\n\n\n\n\n\n\n\n另外因为Air780e默认不会上电自动开机，所以还要把PWR键上方的焊盘短接来让它通电就自动开机。（暂且借晨旭的博客中的图片，因为我焊的时候忘了这茬了……）\n\n然后SIM卡芯片朝板子，缺口朝内，插进Air780e的SIM卡槽。\n\n最后按照如图的方向把排针对其插上就组装好了。一定要注意方向，毕竟有5V供电的针脚，插反了把5V怼到GND上会发生什么就不好说了，或许可以参考伊朗电工Mehdi Sadaghdar的各种作死实战。\n\n刷机和测试ESP32S3接上USB转换头，再插到电脑上，就可以愉快的刷机了。代码我已经写好并发布在GitHub上面了，下载下来，照着README里面刷机的部分改好代码，用合宙的LuaTools刷进去，就可以了。注意板子的固件要用我的repo里面的，LuaTools提供的固件刷进去之后可用内存不够，会发不出去HTTP请求。如果不会刷机，可以看看ESP32C3烧录教程。\n刷完机板子会自动重启，这时候可以观察板子上面指示灯的状态，或者直接在LuaTools里面看日志，来看板子的初始化状态。LED灯各种状态的含义也可以看README里面的解释。\n等板子初始化完成之后，就可以给这个手机号发短信了，板子收到短信后就会把发信号码和短信内容发送到配置的通知通道。开发的时候我已经测试过普通短信、长短信、中文、英文、日语，都可以正常接收。\n\n","categories":["瞎折腾"],"tags":["LuatOS","合宙","短信转发","Air780E","ESP32S3"]},{"title":"浅谈MySQL中的校对规则(collation)","url":"/brief-talk-about-mysql-collations.html","content":"在我们开发过程中，最常见到的三种校对规则(collation)就是utf8mb4_general_ci、utf8mb4_unicode_ci，和utf8mb4_bin。那么这三种排序规则之间有什么区别，在开发过程中又该怎么选择？这里就简单说一下我所了解到的知识，和我的理解。\n\n\n校对规则不会导致乱码之前听到一名同事说，“这个东西(collation)你再研究下，搞不好中文会乱码的”。显然这位同事没搞清楚字符编码(encoding)和校对规则(collation)的区别。\n字符集是一套符号和编码，它实实在在决定了每个字符应当以怎样的规则被编码为二进制数据，以及在取出一系列二进制数据之后，又应当以怎样的规则还原为字符。比如我们喜闻乐见的“锟斤拷”就是因为GBK编码与Unicode编码之间转换出现问题导致的乱码。\n而校对规则本身并不会参与字符的编码，所以它当然不会产生文字乱码的问题。当然，使用了不合适的校对规则，也会对数据产生一定的影响，具体有什么影响后面我们慢慢说。\n校对规则是什么校对规则是一套规范，它指明了数据在数据库中应当以哪种方式被比较和排序，包括排序的规则、是否大小写敏感，以及是否对重音标记敏感。\n在MySQL中，校对规则通常按照字符集_语言或地区_字符敏感特性的规则来命名。\n这三种校对规则有什么区别首先，根据它们的后缀_ci可以知道，这三种校对规则都是对大小写不敏感(Case Insensitive)的。\nutf8mb4_general_ci是对大小写和重音字符都不敏感的。比如，拉丁字符ÀÁÅåāă是等同于字符a的。\nutf8mb4_unicode_ci是大小写不敏感，但是对重音字符敏感的。即，拉丁字符Å和å是等同的，但是和a是不同的。\nutf8mb4_bin则是直接比较每个字符的Unicode码点(code point)。\n从效率上来讲，utf8mb4_bin &gt; utf8mb4_general_ci &gt; utf8mb4_unicode_ci。\n校对规则会产生什么影响不同的排序规则，不仅对大小写和重音字符的处理不同，对于一些语言中特有的字符的处理也是不同的。\n比如一个斯洛伐克人，他向数据库中插入了两条数据，其主键分别是poistný和poistny。但是因为他用了重音不敏感的utf8mb4_general_ci，导致数据库判定这两个字符串是一样的，而抛出了重复主键的错误。[^3]\n另一个例子是，对于德语中的ß，如果使用utf8mb4_general_ci，那么它等同于s，而如果使用utf8mb4_unicode_ci，它则等同于ss。\n所以，如果数据涉及的语言中包含有特殊的字符或者重音符号，而其比对结果的正确性又很重要时，那么就应当选择unicode_ci系列的校对规则，否则，general_ci就够用了。当然，在必要的情况下，也可以选择这个语言对应的规则，比如utf8_swedish_ci。\n对于中文来说，我在网上并没有找到相关的文章，同时根据自己的开发经验，general_ci和unicode_ci对于中文都没有什么明显的问题，所以两个都可以用。\n[^1]: Re: utf8_unicode_ci vs utf8_general_ci - MySQL Forums[^2]: MySQL collation charts[^3]: 斯洛伐克人的例子\n","categories":["学知识"],"tags":["MySQL","collation"]},{"title":"MySQL Workbench中各个列属性的含义","url":"/column-flags-in-mysql-workbench.html","content":"\nPK: 主键(Primary Key)\nNN: 非空(Not Null)\nUQ: 唯一索引(Unique Index)\nBIN: 二进制(Binary) 将数据储存为二进制字符串\nUN: 无符号的(Unsigned)\nZF: 零填充的(Zero Fill) 如：INT(5)的列中，12会被填充为00012\nAI: 自增长的(Auto Increment)\nG: 生成出来的(Generated) 如：根据公式从其它列中生成的数据\n\n\n\n[^1]: What do column flags mean in MySQL Workbench?[^2]: Columns Tab - MySQL Workbench Manual\n","categories":["学知识"],"tags":["MySQL","column flag"]},{"title":"Concourse实战 - 监控GitHub release并自动构建镜像","url":"/concourse-practise-build-and-push-docker-image.html","content":"距离上一篇Concourse相关的文章发布，已过去两年有余，期间因为没什么使用场景，不知道该怎么继续写下去，于是就断了。这次，我终于有机会将Concourse用到我自己的home lab，并成功完成了一条pipeline。\n\n\n背景及需求偶然在网上看到了一个可以多端直播推流的工具，叫Ant Media Server，但是它的安装程序并不支持我正在用的Ubuntu 22.04 LTS，同时它也没有提供制作好的Docker镜像，只能自己手动构建。可手动构建也太不优雅，根本不能忍，所以萌生了一个需求：监控Ant Media Server的GitHub releases，如果有新的版本发布，那么就自动构建新的Docker镜像，并推送到我的Docker Hub中。\n开始动手首先，我要实现在Concourse里面监控GitHub release。github-release这个resource type就是干这件事的，所以我们可以在pipeline中定义这样一个resource：\nresources:- name: ant-media-server  type: github-release  source:    owner: ant-media    repository: Ant-Media-Server    # 默认监视的是&quot;release-&quot;开头的tag    # 但Ant Media Server的tag都是以&quot;ams-&quot;开头的    # 所以需要指定一下    tag_filter: &quot;ams-v?([^v].*)&quot;\n\n资源光在resources里面定义好还不够，我们需要在pipeline里面用get这个task来让Concourse做出从这个资源获取数据的操作。所以，开始写pipeline咯。\njobs:- name: build-image # pipeline的名字  public: true # 公开就意味着用户不需登录也能在dashboard中看到  plan:    - get: ant-media-server # 这里要写上面定义的resource的名字      trigger: true # 这个资源将作为一个触发器\n\n这样就实现了让Concourse监控这个GitHub release，并在发布新release的时候触发pipeline运行。而这个task在运行的时候，会将release中的artifact下载到ant-media-server这个目录中，所以我们也不用担心下载文件的问题。同时它还会把release的版本号写在version这个文件中，后面我们可以利用这个文件来生成Docker镜像的tag。\n有了Ant Media Server的成品文件，按照官方文档的说法，接下来只要做两件事：下载Dockerfile，执行docker build命令就行。但是放在pipeline里面，就没这么简单了。\n先做第一件事，下载Dockerfile。感谢jgriff&#x2F;http-resource这个仓库，它可以实现在Concourse里面通过HTTP下载一个文件。那么接下来pipeline里面可以这么写：\n# 因为这不是Concourse官方提供的resource type# 所以需要在这里定义一个名为http-resource的resource type# 并声明由jgriff/http-resource这个Docker镜像来实现resource_types:  - name: http-resource    type: docker-image    source:      repository: jgriff/http-resourceresources:# 前略- name: ant-media-server-dockerfile  type: http-resource # 上面定义好这个resource type之后，就可以在这里用了  source:    # 指定要下载的文件    url: https://raw.githubusercontent.com/ant-media/Scripts/master/docker/Dockerfile_Processjobs:- name: build-image # pipeline的名字  public: true # 公开就意味着用户不需登录也能在dashboard中看到  plan:  # 让这两个task并行执行，节省时间  - in_parallel:    - get: ant-media-server      trigger: true    # 下载ant-media-server-dockerfile这个resource指定的文件    - get: ant-media-server-dockerfile\n\n现在Dockerfile可以下载到了，但是它是被保存在ant-media-server-dockerfile/body这个文件里面的，我们需要把它移动到ant-media-server这个目录里，才能保证后面成功运行docker build。所以接下来要用mv命令把文件移过去。\njobs:- name: build-image  public: true  plan:  # 前略  - task: move-dockerfile    config:      # task运行在Linux环境      platform: linux      # task将通过ubuntu这个Docker镜像运行      image_resource:        type: docker-image        source:           repository: ubuntu      # 将这两个资源传给镜像      inputs:       - name: ant-media-server      - name: ant-media-server-dockerfile      # 因为修改了ant-media-server这个资源的内容      # 所以要将其输出，这样后续的task才能取到修改后的内容      outputs:      - name: ant-media-server      run:         path: mv        args: [&quot;ant-media-server-dockerfile/body&quot;, &quot;ant-media-server/Dockerfile&quot;]\n\n有了Dockerfile，接下来就可以开始着手构建了。不用想，对于构建Docker镜像这样常见的task，Concourse预先制作好了concourse/oci-build-task这个镜像来给我们用。\n但是首先我们需要创建一个包含着build args的文件，因为文档的docker build命令中提到了--build-arg AntMediaServer=&lt;Replace_With_Ant_Media_Server_Zip_File&gt;这个参数，而Ant Media Server的zip文件名又会随着release而变化，同时oci-build-task的参数BUILD_ARGS_*并不支持shell命令，也就是说我不能通过BUILD_ARGS_AntMediaServer=ant-media-server-community-$(cat version).zip这样的方法来生成，那么只能用oci-build-task的BUILD_ARGS_FILE参数，传进去一个生成好的build args file。\n所以我们需要在pipeline中增加这两步来完成镜像的构建操作。\njobs:- name: build-image  public: true  plan:  # 前略  # 生成build args file  - task: generate-build-args    config:      platform: linux      image_resource:        type: docker-image        source:           repository: ubuntu      inputs:       - name: ant-media-server      outputs:      - name: ant-media-server      run:         # 这里我曾经试过         # path: echo        # args: [&quot;AntMediaServer=ant-media-server-community-$(cat ant-media-server/version).zip&quot;, &quot;&gt;&quot;, &quot;ant-media-server/build_args.txt&quot;]        # 但是没成功，因为Concourse会把args做字符串拼接处理，最后当成一整个字符串传给命令        # 所以其实变成了 echo &quot;AntMediaServer=ant-media-server-community-$(cat ant-media-server/version).zip &gt; ant-media-server/build_args.txt&quot;        # 很明显这只能把这串字符串打在屏幕上，并不能生成文件        # 所以只能通过调用sh来执行命令，把命令当成参数传给sh        path: sh        args:           - -exc          - &#x27;echo &quot;AntMediaServer=ant-media-server-community-$(cat ant-media-server/version).zip&quot; &gt; ant-media-server/build_args.txt&#x27;  # 开始构建镜像  - task: build-image    privileged: true    config:      platform: linux      image_resource:        type: registry-image        source:           repository: concourse/oci-build-task      # 构建所需的文件都在ant-media-server这个资源中      # 所以将它传给这个task      inputs:      - name: ant-media-server      # 将task输出的资源命名为image，并将其输出      outputs:      - name: image      params:        CONTEXT: ant-media-server        BUILD_ARGS_FILE: ant-media-server/build_args.txt      # 缓存构建结果，加速将来的新的构建      caches:      - path: cache      run:        path: build\n\noci-build-task成功后，会把镜像保存到image/image.tar文件中。要将它上传到Docker Hub，我们还需要定义一个registry-image类型的resource，来指定要将镜像上传到哪里。\n因为上传Docker Hub需要登陆，而把token写在pipeline里面是非常蠢的行为，所以我把登陆信息放到了Vault中。向Vault放登陆信息很简单，在/concourse这个path中新建两个secret就可以了：\n\n/shared/dockerhub_username，key是value，value填写Docker Hub的用户名\n/shared/dockerhub_token，key是value，value填写Docker Hub的access token\n\n之所以我把登陆信息放到/shared这个path下，是因为我在Vault中配置了这个path作为一个公共的path，在构建的时候要根据实际情况来修改，比如改成team的名字，或者放在/{team}/{pipeline}/下面。具体请参考Concourse与Vault集成相关的文档，这里不再赘述。\n放好登陆信息后，就可以添加这样一个resource：\nresources:# 前略- name: ant-media-server-docker  type: registry-image  icon: docker  source:    repository: &quot;((dockerhub_username))/ant-media-server&quot;    username: &quot;((dockerhub_username))&quot;    password: &quot;((dockerhub_token))&quot;\n\n然后在pipeline最后增加两个task，一个是读取ant-media-server/version的值，将其写在名为tag的变量中，后面我们会用这个变量来指定镜像的tag；另一个就是对ant-media-server-docker这个资源执行put的操作，将image/image.tar这个镜像上传到Docker Hub。\njobs:- name: build-image  public: true  plan:  # 前略  # load_var用来从文件读入数据，并将其放在一个变量中  - load_var: tag    # 指定要读的文件    file: ant-media-server/version    # 为避免自动识别给我识别错，干脆直接指定文件内容的格式    # trim就是纯文本，读取之后会去掉头尾的空白    format: trim  - put: ant-media-server-docker    params:      image: image/image.tar      # 上传时，同时更新latest和相关semver的镜像      # 比如上传2.5.3时，会同时更新2.5，2，latest这三个tag      bump_aliases: true      # 这里注意要指明从local var source中寻找变量，也就是开头的.:      # 否则会找不到这个变量      version: &quot;((.:tag))&quot;\n\n完整的pipeline至此这个pipeline就完成了，下面我附上已经部署过的版本，供参考。\n---resource_types:  - name: http-resource    type: docker-image    source:      repository: jgriff/http-resourceresources:- name: ant-media-server  type: github-release  source:    owner: ant-media    repository: Ant-Media-Server    tag_filter: &quot;ams-v?([^v].*)&quot;- name: ant-media-server-dockerfile  type: http-resource  source:    url: https://raw.githubusercontent.com/ant-media/Scripts/master/docker/Dockerfile_Process- name: ant-media-server-docker  type: registry-image  icon: docker  source:    repository: &quot;((dockerhub_username))/ant-media-server&quot;    username: &quot;((dockerhub_username))&quot;    password: &quot;((dockerhub_token))&quot;jobs:- name: build-image  public: true  build_log_retention:    # 只保留最近5次的构建记录，以节省空间    builds: 5  plan:  - in_parallel:    - get: ant-media-server      trigger: true    - get: ant-media-server-dockerfile  - load_var: tag    file: ant-media-server/version    format: trim    reveal: true  - task: move-dockerfile    config:      platform: linux      image_resource:        type: docker-image        source:           repository: ubuntu      inputs:       - name: ant-media-server      - name: ant-media-server-dockerfile      outputs:      - name: ant-media-server      run:         path: mv        args: [&quot;ant-media-server-dockerfile/body&quot;, &quot;ant-media-server/Dockerfile&quot;]  - task: generate-build-args    config:      platform: linux      image_resource:        type: docker-image        source:           repository: ubuntu      inputs:       - name: ant-media-server      outputs:      - name: ant-media-server      run:         path: sh        args:           - -exc          - &#x27;echo &quot;AntMediaServer=ant-media-server-community-$(cat ant-media-server/version).zip&quot; &gt; ant-media-server/build_args.txt&#x27;  - task: build-image    privileged: true    config:      platform: linux      image_resource:        type: registry-image        source:           repository: concourse/oci-build-task      inputs:      - name: ant-media-server      outputs:      - name: image      params:        CONTEXT: ant-media-server        BUILD_ARGS_FILE: ant-media-server/build_args.txt      caches:      - path: cache      run:        path: build  - put: ant-media-server-docker    params:      image: image/image.tar      bump_aliases: true      version: &quot;((.:tag))&quot;\n","categories":["学知识"],"tags":["concourse","concourse-ci","vault"]},{"title":"学习Concourse CI - 快速入门","url":"/concourse-quick-start.html","content":"最近公司需要用到一个名叫Concourse CI的CI/CD工具，那么我当然就要学习一下啦。顺便还能水一篇，啊不，写一篇博客，当作学习过程中的笔记。\n\n\n准备数据库Concourse使用PostgreSQL数据库来存储数据，所以首先要初始化好一个数据库。\n如果要使用自建的数据库，那么可以参考这篇官方文档。\n我这里用的是Railway的数据库实例，准备步骤如下：\n-- 给Concourse创建个单独的schemaCREATE SCHEMA concourse;-- 创建用户CREATE ROLE concourse WITH ENCRYPTED PASSWORD &#x27;concourse&#x27;;-- 使新用户可以登陆ALTER ROLE concourse WITH LOGIN;-- 给用户concourse赋予schema级的权限GRANT USAGE,CREATE ON SCHEMA concourse TO concourse;-- 给用户concourse赋予所有表的全部权限GRANT ALL ON ALL TABLES IN SCHEMA concourse TO concourse;-- 给用户concourse赋予所有序列的全部权限GRANT ALL ON ALL SEQUENCES IN SCHEMA concourse TO concourse;\n\n安装Concourse CI这里我将用两台服务器完成Concourse的部署，一个用来部署web节点，一个用来部署worker节点。\nWeb节点Concourse的web节点中会运行一个名为TSA的服务用来注册worker节点，所以首先我们要在web节点创建TSA服务所需的SSH密钥对。\ncd ~/docker/concourse# 用于让web节点生成和验证用户的session tokenssh-keygen -t rsa -b 4096 -m PEM -f ./session_signing_key# 生成TSA端的密钥对ssh-keygen -t rsa -b 4096 -m PEM -f ./tsa_host_key# 稍后要将worker节点的公钥放在这里面# 其实就是SSH的authorized_keystouch authorized_worker_keys\n\n然后编写docker-compose.yml：\nversion: &#x27;3&#x27;services:  concourse:    image: concourse/concourse:latest    restart: always    container_name: concourse    # 因为Concourse要开好几个端口，我懒得一个个配，直接host网络拉倒    network_mode: host    privileged: true    # 让Concourse启动web节点    command: web    # 把刚刚创建的密钥挂载进容器    volumes:      - /home/boris1993/docker/concourse:/keys    environment:      TZ: Asia/Shanghai      # HTTP代理配置，按需      # 为啥要配懂得都懂，就是网络加速，如果你的网络能顺畅拉资源那不配也没问题      HTTP_PROXY: http://127.0.0.1:8899      HTTPS_PROXY: http://127.0.0.1:8899      ALL_PROXY: socks5://127.0.0.1:8899      # Web节点监听8085端口      CONCOURSE_BIND_PORT: 8085      # 外部访问地址，因为我只在内网用，所以就配个内网IP就行      CONCOURSE_EXTERNAL_URL: http://192.168.1.123:8085      # 密钥配置      CONCOURSE_SESSION_SIGNING_KEY: /keys/session_signing_key      CONCOURSE_TSA_HOST_KEY: /keys/tsa_host_key      CONCOURSE_TSA_AUTHORIZED_KEYS: /keys/authorized_worker_keys      # 数据库配置      CONCOURSE_POSTGRES_HOST: containers-us-east-123.railway.app      CONCOURSE_POSTGRES_USER: concourse      CONCOURSE_POSTGRES_PORT: 5511      CONCOURSE_POSTGRES_PASSWORD: concourse      CONCOURSE_POSTGRES_DATABASE: railway      # 配置一个本地用户用于首次登陆      CONCOURSE_ADD_LOCAL_USER: concourse:concourse      # 将这个本地用户加入main team，即将其作为管理员      CONCOURSE_MAIN_TEAM_LOCAL_USER: concourse\n\n接下来执行docker compose up -d启动容器，过几分钟就可以在http://192.168.1.123:8085打开Concourse的页面了。首次启动可能耗时比较久，因为要花时间初始化数据库里面的各种表。\nWorker节点上面启动的web节点只是用来给我们看的，它并不能执行任何的构建任务，所以还需要启动至少一个worker节点来运行构建任务。\n首先还是生成密钥：\ncd ~/docker/concourse-worker# 只需要生成worker的SSH密钥ssh-keygen -t rsa -b 4096 -m PEM -f ./worker_key\n\n生成了worker节点的SSH密钥对之后，我们需要把worker_key.pub中的内容添加到web节点的authorized_worker_keys文件中，以通知web节点可以接受这个worker的加入请求。authorized_worker_keys文件改好后需要重启web节点的Docker容器以使修改生效。\n接下来编写docker-compose.yml：\nversion: &#x27;3&#x27;services:  concourse-worker:    image: concourse/concourse:latest    restart: always    container_name: concourse_worker    network_mode: host    privileged: true    # 让Concourse以worker模式运行    command: worker    volumes:      # 密钥所在的位置      - /home/ubuntu/docker/concourse:/keys      # worker节点的数据目录      - /home/ubuntu/docker/concourse/data:/opt/concourse/    environment:      # 节点名字      CONCOURSE_NAME: &#x27;worker-1&#x27;      # 在Docker中运行的话，必须手动指定运行环境是containerd      CONCOURSE_RUNTIME: containerd      CONCOURSE_CONTAINERD_DNS_SERVER: 8.8.8.8      # web节点TSA服务的位置      CONCOURSE_TSA_HOST: 192.168.1.123:2222      # worker节点的密钥      CONCOURSE_TSA_PUBLIC_KEY: /keys/tsa_host_key.pub      CONCOURSE_TSA_WORKER_PRIVATE_KEY: /keys/worker_key      CONCOURSE_WORK_DIR: /opt/concourse/worker\n\n然后执行docker compose up -d启动即可。\n安装Fly CLI虽然Concourse带有一个Web界面，但是我们在Web界面里面干不了什么，因为它的所有管理操作都需要通过它的Fly CLI来完成。\n要安装Fly CLI，你可以从刚才打开的Dashboard里面下载，也可以到Concourse的GitHub Releases中下载。\nmacOS用户可能会想，我能不能用Homebrew来安装这个东西？一开始我也是这么想的，但是后面我发现，fly的版本是要跟着web节点的版本走的，所以死了这条心，老老实实从Dashboard里面下载吧。\n检查worker的状态为了确保worker节点是成功连接到web节点，我们需要用fly命令来检查worker节点的状态。\n# 添加一个名为default的target，登陆至http://192.168.1.123:8085# 需要点击下面显示的URL，在浏览器中完成登陆过程$ fly login -t default -c http://192.168.1.123:8085logging in to team &#x27;main&#x27;navigate to the following URL in your browser:  http://192.168.1.123:8085/login?fly_port=49290or enter token manually (input hidden):target saved# 列出这个target的workers# 看到刚刚启动的worker节点，即说明这个worker成功连上了$ fly -t default workersname          containers  platform  tags  team  state    version  ageworker-1      0           linux     none  none  running  2.4      14h14m\n\nHello World世间万物都可以从一个hello world学起，Concourse也不例外。我们可以跟着Concourse Tutorial[^3]中Hello World一节的描述，把这个task执行起来。\n$ git clone https://github.com/starkandwayne/concourse-tutorial.gitCloning into &#x27;concourse-tutorial&#x27;...remote: Enumerating objects: 5, done.remote: Counting objects: 100% (5/5), done.remote: Compressing objects: 100% (5/5), done.remote: Total 3794 (delta 0), reused 4 (delta 0), pack-reused 3789Receiving objects: 100% (3794/3794), 11.18 MiB | 25.00 KiB/s, done.Resolving deltas: 100% (2270/2270), done.$ cd concourse-tutorial/tutorials/basic/task-hello-world$ fly -t default execute -c task_hello_world.ymluploading task-hello-world doneexecuting build 1 at http://localhost:8080/builds/1initializingwaiting for docker to come up...Pulling busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d...sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d: Pulling from library/busybox0669b0daf1fb: Pulling fs layer0669b0daf1fb: Verifying Checksum0669b0daf1fb: Download complete0669b0daf1fb: Pull completeDigest: sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049dStatus: Downloaded newer image for busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049dSuccessfully pulled busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d.running echo hello worldhello worldsucceeded\n\n可以看到，Concourse收到这个task之后，下载了一个Busybox的Docker镜像，然后执行了echo hello world这条命令。那么，Concourse是怎么知道要如何执行一个task呢？这就得从上面运行的task_hello_world.yml说起了。\n一个task的配置文件Task是Concourse的流水线(pipeline)中最小的配置单元，我们可以把它理解成一个函数，在我们配置好它的行为之后，它将永远按照这个固定的逻辑进行操作。\n上面的task_hello_world.yml就是配置了一个task所要进行的操作，它的内容不多，我们一块一块拆开来看。\n---platform: linuximage_resource:  type: docker-image  source: &#123;repository: busybox&#125;run:  path: echo  args: [hello world]\n\nplatform属性指定了这个task要运行在哪种环境下。需要注意，这里指的是worker运行的环境，比如这里指定的linux，就意味着Concourse将会挑选一个运行在Linux中的worker。\nimage_resource属性指定了这个task将会运行在一个镜像容器中。其中的type属性说明这个镜像是一个Docker镜像，source中{repository: busybox}说明了要使用Docker仓库中的busybox作为基础镜像。\nrun属性就是这个task实际要执行的任务，其中的path指定了要运行的命令，这里可以是指向命令的绝对路径、相对路径，如果命令在$PATH中，那么也可以直接写命令的名称；args就是要传递给这个命令的参数。\n如果要执行的命令非常复杂，我们也可以把命令写在一个shell脚本中，然后在run.path中指向这个脚本，比如这样：\nrun:  path: ./hello-world.sh\n\n这样一来，就很清楚了。这个task会在一台Linux宿主机中执行，它将在一个busybox镜像中运行echo hello world这条命令。\n把多个task串起来虽然我们在上面已经有了一个能用的task，但是上面说了，task只是一个pipeline的最小组成部分。而且在正式环境中，一个CI&#x2F;CD任务可能会用到多个task来完成完整的构建任务。那么，怎么把多个task串起来呢？手动去做这件事显然不现实，所以就有了pipeline。\n这里我们还是用Concourse Tutorial[^3]中的示例来演示。\n首先我们先看一下这个配置文件的内容：\n---jobs:  - name: job-hello-world    public: true    plan:      - task: hello-world        config:          platform: linux          image_resource:            type: docker-image            source: &#123;repository: busybox&#125;          run:            path: echo            args: [hello world]\n\n一个pipeline可以有多个job，这些job决定了这个pipeline将会以怎样的形式来执行。而一个job中最重要的配置，是plan，即需要执行的步骤。一个plan中的作业步，可以用来获取或更新某个资源，也可以用来执行某一个task。\n上面这个pipeline只有一个名为job-hello-world的job，这个job里面只有一个作业步，名为hello-world，是一个task，操作是在一个busybox镜像中执行echo hello world命令。\n在使用这个pipeline之前，我们需要把它注册到Concourse中。\n# -t 指明要操作的target# -c 指明pipeline的配置文件# -p 指明pipeline的名字$ fly -t default set-pipeline -c pipeline.yml -p hello-worldjobs:  job job-hello-world has been added:+ name: job-hello-world+ plan:+ - config:+     container_limits: &#123;&#125;+     image_resource:+       source:+         repository: busybox+       type: docker-image+     platform: linux+     run:+       args:+       - hello world+       path: echo+   task: hello-world+ public: trueapply configuration? [yN]: ypipeline created!you can view your pipeline here: http://localhost:8080/teams/main/pipelines/hello-worldthe pipeline is currently paused. to unpause, either:  - run the unpause-pipeline command:    fly -t default unpause-pipeline -p hello-world  - click play next to the pipeline in the web ui\n\n现在一个新的pipeline就被注册到Concourse中了。在它的Web UI中也能看到这个pipeline。\n\n但是，这个pipeline现在还是暂停状态的，需要把它恢复之后才能使用。那么怎么恢复呢？其实上面set-pipeline操作的输出已经告诉我们了。\n\nthe pipeline is currently paused. to unpause, either:- run the unpause-pipeline command:fly -t default unpause-pipeline -p hello-world- click play next to the pipeline in the web ui\n这个pipeline目前是被暂停的，如果要恢复，可以使用下面两种方法之一：- 运行unpause-pipeline命令：fly -t default unpause-pipeline -p hello-world- 在Web UI中点击pipeline的播放按钮\n\n在成功恢复pipeline之后，我们可以看到原来蓝色的paused字样变成了灰色的pending字样，说明现在这个pipeline正在等待任务。\n接下来我们就可以手动执行一下这个pipeline，来检查它是否正常。具体操作说起来太啰嗦，我直接借用Concourse Tutorial里面的一个动图来替我说明。\n\n自动触发job虽然我们在Web UI上点一下加号就能触发job开始执行，但是CI&#x2F;CD讲究的就是一个自动化，每次更新都手动去点一下，显然谁都受不了这么折腾。所以，Concourse也提供了几种自动触发job执行的方法。\n一种方法是向Concourse API发送一个POST请求。这种就是webhook，没什么特殊的，在版本控制系统里面配置好webhook的参数就好了。\n另一种方法是让Concourse监视某一个资源，在资源发生改变之后自动触发job执行。下面我详细说说这个功能。\n这里我们假设一个场景：我们有一个Git仓库，里面有一个名为test.txt的文件。我们想在每次这个仓库收到新commit之后，打印出test.txt的内容。\n按照这个思路，我在Concourse中注册了如下的pipeline：\n---# 先定义一个Git资源resources:  - name: resource-git-test    type: git    source:      # 这里换成你自己的一个Git仓库      uri: https://gitee.com/boris1993/git-test.git      branch: master  # 如果想要这个任务定期执行，那么可以在这里定义一个计时器  - name: timer    type: time    source:      # 这里定义这个计时器      interval: 2mjobs:  # 定义一个job  - name: job-show-file-content    public: true    plan:      # 第一步：获取resource-git-test中定义的资源      - get: resource-git-test        # 在资源发生更新的时候触发        trigger: true      # 如果要让任务定期重复执行，那么这里也要将定时器作为一个资源      # 并打开trigger开关      - get: timer        trigger: true      # 第二步：在控制台打印文件内容      - task: show-file-content        config:          platform: linux          inputs:            # resource-git-test中定义的资源将作为这个步骤的输入资源            # 即让resource-git-test中的文件对该步骤可见            - name: resource-git-test          image_resource:            type: docker-image            # 因为我们都懂的原因，Docker中心仓库有可能会连不上            # 而在执行构建的时候，Concourse会到仓库检查镜像的版本            # 所以这里用registry_mirror配置了一个Docker仓库的镜像站            source: &#123;repository: busybox, registry_mirror: https://dockerhub.azk8s.cn&#125;          run:            path: cat            # 在引入input资源后，工作目录下就可以看到这个资源相关的文件夹            args: [&quot;./resource-git-test/test.txt&quot;]\n\n创建git-test仓库、编辑test.txt等等操作不是重点，也没啥难度，这里不啰嗦了。在完成编辑文件，和push到远程仓库后，我们等待Concourse检查远程仓库更新，并执行构建步骤。\n在pipeline视图中点击resource-git-test这个资源，就可以看到这个资源的检查历史，展开某条记录后，还可以看到这条历史相关的构建。\n\n在Concourse检查到git仓库的更新后，就会执行下面指定的构建步骤。结果大概会是这个样子的：\n\n结束语至此，我们完整的配置了一个简单的pipeline。后面我会根据文档，或者根据工作中遇到的情况，继续补充权限管理、复杂的case等相关的博文。\n[^1]: Concourse CI[^2]: Concourse - GitHub[^3]: Concourse Tutorial\n","categories":["学知识"],"tags":["concourse","concourse-ci"]},{"title":"学习Concourse CI - 使用变量","url":"/concourse-using-vars.html","content":"在实际编写配置文件时，我们不可能保证所有相关的值都是一成不变的。有的值可能会根据运行时的状态来指定，也有可能它是类似token一样的机密信息，必须从一个可信渠道获取。为了应对这种情况，我们就需要在配置文件中引入变量。\n\n\n声明变量在Concourse的语法规则中，如果要声明一个变量，就用双括号把变量名字包裹起来，就像这样：((a-variable))。借用Concourse Tutorials中的一个例子，我们看一下具体在配置文件中，是如何声明一个变量的。\n---jobs:  - name: show-animal-names    plan:      - task: show-animal-names        config:          platform: linux          image_resource:            type: docker-image            source: &#123;repository: busybox&#125;          run:            path: env            args: []          params:            CAT_NAME: ((cat-name))            DOG_NAME: ((dog-name))\n\n这里需要穿插一个内容，就是task的params属性。它的作用是把里面指定的键值对作为环境变量传到容器中。\n我们可以看到，它声明了两个变量：cat-name和dog-name，分别作为环境变量CAT_NAME和DOG_NAME的值。这样，就完成了变量的声明。\n为变量传值变量声明好了，那就要传值了。Concourse支持通过命令行参数、通过文件，以及通过密钥管理器(credentials manager)来传入变量的值。\n这里有一点需要注意，通过命令行参数或者文件传入的值，是不会根据外部条件变化的，你可以将其理解成fly会先把配置文件做文本替换然后再注册到Concourse。如果要修改变量的值，那么就需要重新运行fly set-pipeline命令来修改。\n通过命令行参数传值我们可以在fly命令中使用-v参数来为变量传值，它的格式是-v key=value，要对多个变量赋值就要使用多个-v参数。\n如果要为上文提到的配置文件传值，那么我们可以这样执行命令：\n# 为变量赋值并注册pipelinefly -t tutorial sp -p parameters -c pipeline.yml -v cat-name=garfield -v dog-name=odie\n\n然后我们可以执行fly -t tutorial trigger-job -j parameters/show-animal-names -w这条命令来触发任务执行，来检查传值是否成功。\n通过文件传值要使用文件传值，我们首先需要创建一个包含变量名和值的YAML文件。为了方便起见我直接用heredoc创建，当然你可以选择任何你喜欢的方式。\n# 从标准输入创建文件# 当接收到EOF这个字符串时停止接收内容$ cat &gt; credentials.yml &lt;&lt;EOFheredoc&gt; cat-name: garfieldheredoc&gt; dog-name: odieheredoc&gt; EOF\n\n这样我们就得到了一个这样的文件：\ncat-name: garfielddog-name: odie\n\n然后在fly命令中，我们就可以用-l参数来通过文件为变量赋值：\nfly -t tutorial sp -p parameters -c pipeline.yml -l credentials.yml\n\n通过密钥管理器传值如上文所述，通过参数和文件赋值，不仅存在诸多不便，同时还无法保证机密信息的安全性。所以，Concourse也支持与密钥管理器集成。Concourse支持多种密钥管理器，具体的支持列表可以在官网的Credential Management处查询。这里我们使用Vault进行演示。\n由于Vault的安装和配置与本文无关，所以这里略过不讲。\nConcourse的配置Concourse默认是没有配置密钥管理器的，如果要将Vault与Concourse集成起来，那么需要为web节点配置如下环境变量：\n# Vault的地址，协议可以是HTTP也可以是HTTPSCONCOURSE_VAULT_URL=https://vault.example.com:8200# 你也可以为Concourse指定CA证书所在的位置CONCOURSE_VAULT_CA_CERT=path/to/ca.crt\n\n配置完毕后重启web节点，就完成了Concourse的配置。\n在Vault中的准备工作根据The Vault credential manager的描述，Concourse会在Vault中按照以下的规则查找变量的值，并取出其中value这个key所对应的内容：\n\n/concourse/TEAM_NAME/PIPELINE_NAME/VAR_NAME\n/concourse/TEAM_NAME/VAR_NAME\n\n以上文提到的那个pipeline为例子，我使用系统自带的main这个team，并将这个pipeline命名为了parameters，那么在寻找cat-name这个变量时，Concourse就会先后在/concourse/main/parameters/cat-name和/concourse/main/cat-name中查找。\n所以为了满足这个条件，我们需要在Vault中完成以下几件事：\n\n新建一个secret engine，将其命名为concourse\n新建一个secret，path填写main/parameters/cat-name，secret data新增一条key为value，值为garfield的记录\n再新建一个secret，path填写main/parameters/dog-name，secret data新增一条key为value，值为odie的记录\n\n这样，就在Vault中设定好了变量的值。\n测试此时，我们回到Concourse，触发这个task执行，如果配置正确的话，就可以看到这样的输出了：\n\n[^1]: Concourse CI[^2]: Concourse - GitHub[^3]: Concourse Tutorial\n","categories":["学知识"],"tags":["concourse","concourse-ci","vault"]},{"title":"我的信用卡优化记录 - 2025年","url":"/credit-card-clean-up-2025.html","content":"上个月翻小红书的时候，被推送了些信用卡权益的帖子，自己想想我从2016年毕业办下第一张招行信用卡至今，好像啥权益也没捞到，净往外花钱了。再一研究，发现什么返现啊贵宾厅啊之类的权益，也并不需要花很大精力或者遥不可及。那么，开始优化！\n\n\n注销的卡开始之前先盘一下库存和年费：\n\n招行银联金卡 - 6笔消费刷免年费\n招行全币种VISA卡 - 有效期内免年费\n招行全币种JCB卡 - 有效期内免年费\n招行运通金卡 - 6笔消费刷免年费\n招行英雄联盟联名银联金卡 - 6笔消费刷免年费\n招行和卡银联金卡 - 6笔消费刷免年费\n交行魔都优逸白金银联卡 - 6笔消费刷免年费\n浦发魔都白金银联卡 - 终身免年费\n\n看得出来，光是要把年费刷掉，就要费一点精力，而且这些卡几乎没有任何权益，所以非必要就注销。\n\n招行：\n只留下银联金卡、VISA和JCB。银联用来备用，而且买苹果设备还能享受24期免息分期。JCB绑了日区Apple ID做iCloud付款卡，而现在日区又不能新绑非日本发行的JCB卡，所以这张万万不能没。VISA反正免年费，留着以防万一。\n运通在当时申请的时候还是有效期内免年费的，但是现在运通卡支持人民币结算后，就要收年费了。所以注销。该说不说，最早拿到的是运通Clear卡，卡面很好看（我也是冲着卡面才办的），但后面换发就变成了百夫长经典金卡。\n英雄联盟卡是还没毕业那会就申请的校园信用卡，为的是打英雄联盟对局就能得银行积分，但是就算最上头的那段时间似乎也没攒下多少积分，而且那会似乎都已经是活动的尾声了。现在留着也没用，所以注销。\n和卡……嘛，它其实是顶替了崩坏三联名八重樱银联卡，但到期的时候已经没有联名卡板了，给我换成了和卡。本来就是收藏来用的，所以注销（想来当初就不应该激活的）。\n\n\n交行：这卡好像是刚工作那会，银行业务员直接来公司给我们推销的。想想，刚毕业，也没啥钱，就能下个白金卡，自然是忍不住诱惑就办了。结果最后沦为逢年过节回老家的时候刷地铁用。没权益还要惦记着刷免年费，所以注销。\n浦发：好像也是刚工作的时候银行业务员直接来公司推销的，当时一听，白金卡，还终身免年费，那干脆办下来，放着不用也没啥损失。结果好像去年的时候，这张卡因为太久不用，在银行系统里变成了一个很奇怪的状态，就是卡虽然没注销，但是在浦大喜奔里面又没法查到状态，点进去就会报错。既然如此，干脆销了一了百了。\n\n新办的卡说起权益，我主要关心消费返现和机场贵宾厅。今年4月份去香港旅游回来的那天，我累到坐在登机口的长椅上几乎晕厥，那时候就想，如果能给我个沙发让我舒舒服服小眯一会就好了。那机场的贵宾厅岂不是就能满足？但是毕竟每年我也就是往返老家以及出去旅游一两次的时候才会坐飞机，满打满算一年也就坐4到8趟，而刚性年费的高端白金卡提供的几十次甚至无限次贵宾厅，对我来说绝对是溢出的。\n所以最后需求就固定成，有消费返现，有机场贵宾厅，权益达成条件不要太复杂，可以免年费，而且最好能集中在同一家银行，这样可以更集中地累计积分。\n经过研究之后，我最终得到了这样的一个方案（注意，以下关于权益的描述仅代表我个人的理解，请以银行app中的实际描述为准）：\n\n广发多利白金卡\n权益：\n美团和滴滴消费返现10%（还有另外几个渠道也会返10%，但我不常用就不写了）。\n支付宝、微信、云闪付消费返1%。\n每自然月最多积累100元返现。每自然月消费满3000元后，次月就可以到app的权益中心用累计的返利金兑换返利券（目前是攒满100就能兑5张20的，消费入账过几天后会自动抵扣，不用手动操作）。\n\n\n缺点：这张卡消费没有积分。\n年费：首年免年费，消费12笔或连续12个月绑定第三方支付（微信、支付宝、京东三选一）免次年年费。说白了就是往支付宝上一绑就不用操心了。\n使用场景：上班的时候用美团点外卖，以及偶尔用滴滴打车，返利金攒够100元并且总消费满3000元就换别的卡刷。毕竟消费没有积分，这个月溢出的返利金也不会结转到次月，再继续刷下去又成了有出没进。\n\n\n广发万事达真情白金卡\n权益：\n京东、拼多多、美团等渠道消费8倍积分。\n微信、支付宝消费1.5倍积分。\n境外消费2倍积分。\n\n\n年费：首年免年费，消费12笔或连续12个月绑定第三方支付（微信、支付宝、京东三选一）免次年年费。既然这个在京东消费有8倍积分，那就挂在京东支付上免年费。\n使用场景：既然美团消费8倍积分，那返利卡刷满之后，再点外卖就刷这个，此外就是在京东买东西也刷这个。\n\n\n广发美国运通真情白金卡\n权益：\n网购消费可以得三倍积分，亲测下馆子吃饭用支付宝付款，或者用外卖平台点外卖，然后用这张卡付款，就能得三倍积分。\n在运通体系中是蓝盒子SELECT级别，绑定运通微信服务号后还能享受运通卡组织的一些权益，一大堆但都聊胜于无，就不写了。\n\n\n年费：首年免年费，当年年费周期内消费12笔免次年年费。\n使用场景：返利卡刷满之后就主刷这个，用日常各种消费积累积分。吃吃喝喝啥的咋也能刷出来12笔，顺便就把年费免了。\n\n\n广发银联鼎极白金卡（臻瑞版）\n权益：\n当月消费满3000元后，次月起两个月内可免费享受2次广发冠名机场和高铁站贵宾厅。\n境内消费2倍积分，境外消费3倍积分。\n可以15000积分兑换100航空里程。\n消费积分在5年到期后可以再延续5年有效期，也就是说你累积的积分可以10年不过期。\n附带一张Infinite级别的VISA卡，可以享受VISA卡组织的权益。\n除此之外还有很多别的权益，但我用不上，就不写了。\n\n\n年费：首年免年费，激活后365天内消费满48笔，或激活日起连续12个月绑定第三方支付（微信、支付宝、京东三选一）免次年年费。说白了也是往支付宝一绑就不用操心了。\n使用场景：这张卡消费权益不如上面几张，但是它的重点是可以用15000积分换100航司里程，我逢年过节往返老家主要坐国航的航班，那么除了坐飞机积累的里程，还可以用这张卡兑换国航里程。所以这其实是一张扔抽屉里面的工具卡，不消费，只用它延长积分有效期以及兑换航司里程，\n\n\n广发万事达给力外币卡\n权益：境外消费随机返现，返现直接入账，不占用多利白金卡的返利金池。别的记不清了，这张卡暂时还是下架状态，也没法再去查证。\n缺点：app上没有细说“随机返现”的细则，暂不清楚是笔笔返随机金额，还是有的消费给返有的不给。另外，虽然这张卡是万事达World级别，但是不能参与万事达消费大挑战活动。\n年费：首年免年费，该卡刷卡达3笔，或当前年费周期下本人所有信用卡消费15笔，免次年年费。因为我日常就刷上面那些广发信用卡，所以这张就会连带着免掉年费。\n使用场景：绑Netflix之类的所有能绑的境外平台，薅月费年费的羊毛，以及出境游可以带上。\n\n\n\n以上这些就是日常消费的主力卡，平时靠多利白金卡和万事达真情白金卡薅返利金和积分的羊毛，如果有坐飞机出行的需求，就拿鼎极白金卡提前一个多月买好往返机票，这就几乎能凑满贵宾厅的3000元门槛，就算不够也就缺几百块，点点外卖下下馆子，或者给公交卡和Steam充个值就有了。而且都是同一家银行，积分非常集中，并且年费全部都可以免掉。需求达成。\n除了上面这些，我还办了另外几张锦上添花的。\n\n中国银行全币种国际芯片威士白金卡 - 首先这张卡是免年费的，其次它是VISA Infinite级别，还有开户礼羊毛可以薅。计划薅完开户礼后就放在抽屉里当作备用。\n中国银行JCB招财猫白金卡 - 这张卡也是免年费的，而且是日元入账，免去了日元 -&gt; 美元 -&gt; 人民币的二次汇率折损。同样也有开户礼羊毛可以薅。此外，这张卡是JCB Platinum级别，每年可以免费享受6次日本和夏威夷的贵宾厅，并且可以绑定Lounge Key，每年免费享受2次日本和夏威夷以外的贵宾厅。所以计划是薅完开户礼后就当备用，将来去日本旅游的时候带上，以及将来绑到日区iCloud消费。\n中国银行莫奈睡莲万事达卡 - 这张卡也免年费，是万事达World级别，并且可以参与万事达消费大挑战活动，而且最重要的是卡面很好看。目前还是放在抽屉里备用，将来出境游的时候可以带上试试参与消费大挑战活动。\n光大阳光车主数字卡财富版紫蕴麒麟 - 这张卡我主要看中它每年给5点贵宾休息室点数，以及月均资产达标3000元就能免年费，可以低成本的拉低我每年贵宾厅的需求成本。这张卡暂时还没办，因为我是这个月才往光大银行存了3000元，要在下个月办卡才能免掉年费。\n\n这几张卡在实际使用体验后，我会再发文分享我的使用心得。\n以上。\n","categories":["生活记录"],"tags":["信用卡","薅羊毛"]},{"title":"为webapp设定单独的context.xml","url":"/customize-context-xml-for-individual-webapp.html","content":"要给某个webapp设定单独的context.xml，只需要在${WEBAPP_ROOT}/webapp目录下新建一个META-INF目录，并将context.xml放进去，就可以了。\n","categories":["小技巧"],"tags":["Tomcat"]},{"title":"设计并实现一个简易的短URL服务","url":"/designing-and-implementing-a-short-url-service.html","content":"突然就对短链接服务的原理来了兴趣，于是就查了些资料，自己实现了一个很简陋的演示性的短链接服务。\n\n\n短链接服务是怎么工作的短链接服务这玩意，说来其实非常简单，就是给用户传来的URL起个别名，然后把别名与原链接的映射关系记录在数据库里。\n用户访问短链接时，请求首先会到短链接服务的服务器；短链接服务端收到请求，取出对应的原URL，最后通知用户端的浏览器做个跳转。\n301跳转？还是302跳转？尽管按照语义来讲，301跳转更合适，因为一个短URL必定只对应一个长URL，但是看起来生产上更多使用302跳转，因为这样的话请求会经过短网址提供商的服务器，短网址提供商就可以收集到用户的一些信息，然后把这些信息变现。\n如何生成短链接上面说到，短链接服务的核心就是要给长链接生成一个“别名”，那么这个别名应该怎么生成呢？\n我相信不少人一上来就会想到哈希算法，比如给原URL做个MD5，虽然不是不行，就是哈希算法有碰撞这么个问题，虽然影响不大吧，但处理起来还是个麻烦。\n上网一顿冲浪，我发现其实这个生成的算法非常简单，就是直接用发号器生成一个ID，把这个ID跟原链接绑定就行。足够简单，而且不会碰撞。\n不过既然都提到这两种算法了，不如顺便介绍一下。\n发号器方案发号器方案本质上就是生成分布式ID，如果要简单处理，那么可以使用Redis的incr操作，或者取数据库的自增序列；复杂情况的话，可以让数据库集群中每个节点各负责生成某一范围的数字，或者使用雪花算法等UUID生成算法。\n在得到发号器生成的数字之后，再将其转换为62进制数，就可以当成短URL的ID了。这么做的原因，一方面是可以一定程度上防止直接暴露序列的值产生的安全问题；另一方面，因为为了保证序列够用，发号器返回的数字会比较大，将低进制数转换为高进制数可以显著减少字符数量。\n哈希算法方案\n\n将长网址 md5 生成 32 位签名串,分为 4 段, 每段 8 个字节\n对这四段循环处理, 取 8 个字节, 将他看成 16 进制串与 0x3fffffff(30位1) 与操作, 即超过 30 位的忽略处理\n这 30 位分成 6 段, 每 5 位的数字作为字母表的索引取得特定字符, 依次进行获得 6 位字符串\n总的 md5 串可以获得 4 个 6 位串,取里面的任意一个就可作为这个长 url 的短 url 地址\n\n摘自 短网址(short URL)系统的原理及其实现\n\n技术选型解决了理论问题，接下来就要面对现实问题：用什么实现，和跑在哪里。\n因为这只是一个演示性的短链接服务，目前定位是就我一个人玩，所以我一方面不想花时间在部署和维护上，另一方面也想趁机玩点没玩过的东西。所以我决定把这玩意放在CloudFlare Workers上面，用TypeScript语言开发，数据存放在CloudFlare Workers KV数据库里。这样，我就只需要关心代码怎么写，其他的包括维护数据库、估算服务器压力这些事都不用担心。\n数据库中我需要用两个表，一个表用来存放当前的序列值，和短URL -&gt; 原URL的映射，这个表是服务的核心；另一个表用来存放长URL -&gt; 短URL的映射，这么设计的原因是，针对相同的长URL，我不需要在生成新的短URL，既节省空间，也能稍微节省点能源不是。\n而生成短链接的算法，我当然选择最简单的数据库序列。但因为CloudFlare Workers KV并不支持真正的序列，所以我在数据库里面用一个专门的key当作序列来用。这个选型有一个风险就是，在高并发状态下我无法保证序列的值不会重复，因为取出序列 -- 生成ID -- 保存新的序列这个操作不是原子性的，高并发状态下可能会有多个请求同时取到相同的序列，进而生成相同的ID，最后就会产生错误的结果。不过，还是那句话，就我一个人用的玩意，暂时先不考虑那么多。\n流程这个服务的流程分两大部分，生成新的短URL，和查询短URL并完成跳转。查询操作没什么梗，查到了就返回，查不到就404呗。\n生成新的短URL的话，大致就是这么个流程：\ngraph TD;\n    start[开始];\n    finish[结束];\n    request_received[收到生成的请求];\n    check_existing_record&#123;检查是否已经生成过&#125;;\n    return_existing_record[返回已有的短URL];\n    fetch_current_sequence[查询当前的序列];\n    calculate_base62[计算序列的62进制数值];\n    increase_sequence_number[序列增1];\n    save_to_database[将短URL和新的序列存入数据库];\n    return_new_generated_short_url[返回生成的短URL];\n    \n    start --&gt; request_received;\n    request_received --&gt; check_existing_record;\n\n    check_existing_record --&gt;|Y| return_existing_record;\n    return_existing_record --&gt; finish;\n\n    check_existing_record --&gt;|N| fetch_current_sequence;\n    fetch_current_sequence --&gt; calculate_base62;\n    calculate_base62 --&gt; increase_sequence_number;\n    increase_sequence_number --&gt; save_to_database;\n    save_to_database --&gt; return_new_generated_short_url;\n    return_new_generated_short_url --&gt; finish;\n\n代码实现这里就只放具体实现相关的代码了，完整的代码库可以到参考文档第一条的GitHub仓库看到。\nimport * as url from &#x27;url&#x27;;import &#123; RequestBody, ResponseBody, ShortUrl &#125; from &#x27;./model&#x27;;// 起始的序列值const INITIAL_SEQUENCE_NUMBER = 100000;export interface Env &#123;    [x: string]: any;&#125;export default &#123;    async fetch(        request: Request,        env: Env,        ctx: ExecutionContext    ): Promise&lt;Response&gt; &#123;        switch (request.method) &#123;            case &#x27;POST&#x27;:                return await handlePostRequest(request, env);            case &#x27;GET&#x27;:            default:                return await handleGetRequest(request, env);        &#125;    &#125;,&#125;;async function handleGetRequest(    request: Request,    env: Env): Promise&lt;Response&gt; &#123;    // 取URL中的path部分    let url_parts = url.parse(request.url);    let path = url_parts.pathname;    // 如果没有path部分，或者path有多层    // 那么视为无效请求    // 合法的短URL格式为：https://mydomain.com/RlB2PdD    if (path == null || path.split(/\\/(?=.)/).length !== 2) &#123;        console.info(&quot;No short URL key provided or invalid path. Returning 400&quot;);        return new Response(&quot;No short URL key provided or the path is invalid.&quot;, &#123;            status: 400        &#125;);    &#125;        let pathParts = path?.split(&quot;/&quot;);    // 专门处理下favicon.ico的请求    // 可能是我的实现有问题，不一定必须    if (pathParts[1] === &quot;favicon.ico&quot;) &#123;        return new Response();    &#125;    // 取出path，即短URL的key    let key = pathParts[1];    console.info(`Looking for the target URL with key $&#123;key&#125;`);    // 对env.SHORT_URL操作，就是对SHORT_URL这个KV数据库做操作    // 这里就是从数据库中查询这个key对应的长URL    let shortUrlJson = await env.SHORT_URL.get(key);    if (shortUrlJson === null) &#123;        console.info(`No target URL found for key $&#123;key&#125;`);        return new Response(&quot;No target URL found&quot;, &#123;            status: 404        &#125;);    &#125;    // 构造返回的JSON，然后返回一个HTTP 302让浏览器跳转    let shortUrlObject = JSON.parse(shortUrlJson) as ShortUrl;    console.info(`Target URL for key $&#123;key&#125; is $&#123;shortUrlObject.url&#125;`);    return Response.redirect(shortUrlObject.url, 302);&#125;async function handlePostRequest(    request: Request,    env: Env): Promise&lt;Response&gt; &#123;    let requestBody = await request.json() as RequestBody;    let targetUrl = requestBody.url!;    console.info(`Creating a short URL for target $&#123;targetUrl&#125;`);    // 查询这个长URL是否已经有对应的短URL    // SHORT_URL_MAPPING表记录的是长URL对应的短URL    let existingShortUrl = await env.SHORT_URL_MAPPING.get(targetUrl) as string;    if (existingShortUrl !== null) &#123;        // 查到了，就直接返回        console.info(`Existing short URL key $&#123;existingShortUrl&#125; found for $&#123;targetUrl&#125;`);        let responseBody = new ResponseBody(existingShortUrl);        return new Response(            JSON.stringify(responseBody),            &#123;                status: 201,                headers: &#123;                    &#x27;content-type&#x27;: &#x27;application/json&#x27;                &#125;            &#125;);    &#125;    // 取出当前的序列值，将其转换为62进制，作为短URL的key    let curentSequence = await getCurrentSequence(env);    let key = string10to62(curentSequence);    let data = new ShortUrl(targetUrl);    // 保存短URL，更新序列    await env.SHORT_URL.put(key, JSON.stringify(data));    await env.SHORT_URL_MAPPING.put(targetUrl, key);    await env.SHORT_URL.put(&quot;sequence&quot;, `$&#123;++curentSequence&#125;`);    console.info(`Created a new short URL key $&#123;key&#125; for $&#123;targetUrl&#125;`);    // 返回生成的结果    let responseBody = new ResponseBody(key);    return new Response(        JSON.stringify(responseBody),        &#123;            status: 201,            headers: &#123;                &#x27;content-type&#x27;: &#x27;application/json&#x27;            &#125;        &#125;);&#125;/** * 取出当前的序列值，如果数据库中未初始化， * 那么就将初始序列写入数据库，然后返回初始序列。 * 这个方法不涉及序列的更新 */async function getCurrentSequence(env: Env): Promise&lt;number&gt; &#123;    let currentSequence = await env.SHORT_URL.get(&quot;sequence&quot;);    if (currentSequence === null) &#123;        await env.SHORT_URL.put(&quot;sequence&quot;, `$&#123;INITIAL_SEQUENCE_NUMBER&#125;`);        return INITIAL_SEQUENCE_NUMBER;    &#125;    return currentSequence;&#125;/** * 将10进制数转换为62进制 */function string10to62(number: number) &#123;    var chars = &#x27;0123456789abcdefghigklmnopqrstuvwxyzABCDEFGHIGKLMNOPQRSTUVWXYZ&#x27;.split(&#x27;&#x27;);    var radix = chars.length;    var qutient = +number;    var arr = [];    do &#123;        let mod = qutient % radix;        qutient = (qutient - mod) / radix;        arr.unshift(chars[mod]);    &#125;    while (qutient);    return arr.join(&#x27;&#x27;);&#125;\n\n一些改进空间因为针对相同的长URL并不需要每次都返回相同的短URL，所以长URL -&gt; 短URL表中，我可以给每条记录都加一个TTL，在有效期内，每次针对相同的长URL的生成请求都会返回同一个短URL，同时刷新TTL；而超过有效期后，这条映射就会被删除，对应的长URL则会生成新的短URL。这样一定程度上既可以防止恶意刷接口炸数据库，同时也可以清除掉不太可能再被用到的数据。\n而在如上改动的影响下，必然会出现多个短URL对应同一个长URL的情况，这多少也是浪费了一些空间。所以我感觉可以在短URL -&gt; 长URL映射表中，增加一个最后访问时间字段，每有一个短URL的请求，就更新这个时间到请求的时间。再启动一个定时任务，定时扫描每个短链接的最后访问时间，并将在指定时间（如半年）内没有被访问过的短链接删除。（我觉得，应该没有人把短链接当成永久链接吧？就算不考虑被删，万一服务商跑路了呢？\n此外，还可以给短URL -&gt; 长URL映射表中再增加一个访问次数字段，以便结合其他收集到的数据来做分析。\n参考文档\ncf-worker-short-url - GitHub\n短网址服务(TinyURL)生成算法\n短 URL 系统是怎么设计的？ - iammutex的回答 - 知乎\n短网址(short URL)系统的原理及其实现\n\n","categories":["学知识"],"tags":["短链接"]},{"title":"关闭Vercel的部署结果通知","url":"/disable-vercel-deployment-notification.html","content":"每次Vercel部署之后，它都会在部署的commit下面发个类似这样的留言：\n\nSuccessfully deployed to the following URLs:\nblog – .&#x2F;\nblog-boris1993.vercel.app\nboris1993.com\nwww.boris1993.com\n\n而且GitHub还会给我发邮件通知这个留言的内容，但是这个消息说实话没啥用，白白麻烦人而已，后来发现，在项目根目录创建一个名为vercel.json的文件，里面写上这样的配置就行：\n&#123;    &quot;github&quot;: &#123;        &quot;silent&quot;: true    &#125;&#125;\n\n这个配置的作用就是让Vercel不再往这个repo的commit下面评论部署状态。提交之后，Vercel就会在这次部署开始遵循vercel.json的设定，不会再发送评论，自然也就不会有那封“骚扰邮件”了。\n","categories":["小技巧"],"tags":["Vercel"]},{"title":"自己实现一个BlockingQueue并优化","url":"/enhancing-my-blocking-queue-implementation.html","content":"前阵看见个面试题\n\n请实现一个泛型类BlockingQueue：\n\n构造函数里指定队列容量\nvoid put(T item) 队列满了会阻塞，直到队列有空间\nT take()，队列空时会阻塞，直到队列有元素\n\n\n我寻思做做看，结果写出来的答案我自己都看不下去，那干脆看看Java里面是咋实现的。\n\n\n我的实现先看看我写的玩意是啥德行吧。实话说这玩意看着都不像能正常工作的样子（虽然实际上它还真能用）\npublic class BlockingQueue&lt;T&gt; &#123;    private final Object[] items;    private final int maxSize;    private volatile int count;    public BlockingQueue(final int size) &#123;        this.items = new Object[size];        this.maxSize = size;    &#125;    public synchronized void put(final T item) throws InterruptedException &#123;        while (this.count &gt;= maxSize) &#123;            wait();        &#125;        while (true) &#123;            if (this.count &lt; maxSize) &#123;                notifyAll();                break;            &#125;        &#125;        synchronized (this) &#123;            items[count] = item;            count++;        &#125;    &#125;    public synchronized T take() throws InterruptedException &#123;        while (count == 0) &#123;            wait();        &#125;        while (true) &#123;            if (count &gt; 0) &#123;                notifyAll();                break;            &#125;        &#125;        final T item = (T) items[0];        for (int i = 1; i &lt; count; i++) &#123;            items[i - 1] = items[i];        &#125;        count--;        return item;    &#125;&#125;\n\n这段代码主要有俩问题，一个是这些while循环看着很不爽；另一个是在取出元素后逐个将元素左移的操作效率非常的低；而且满天飞的wait()和notifyAll()也会影响代码的可读性。\n那么接下来看看这段代码怎么优化成接近Java自己的实现。\n优化阻塞BlockingQueue的一个机制是，在队列空时取元素的操作会被阻塞，而在队列满时放元素的操作会被阻塞。在Java的ArrayBlockingQueue中，它并没有使用synchronized加锁，而是用了ReentrantLock对象。此外对于阻塞线程和唤醒线程操作，它也没有用wait()和notifyAll()，而是通过两个Condition对象notEmpty和notFull实现。\n所以改进方案就是，去掉方法定义中的synchronized关键字，换成用ReentrantLock加锁解锁；去掉wait()和notifyAll()，改成用notEmpty和notFull管理状态以及负责唤醒，同时可以借助这两个变量的语义增强代码的可读性。\npublic class BlockingQueue&lt;T&gt; &#123;    private final Object[] items;    private final int maxSize;    private int count = 0;    private final ReentrantLock lock;    private final Condition notEmpty;    private final Condition notFull;    public BlockingQueue(final int size) &#123;        this.items = new Object[size];        this.maxSize = size;        this.lock = new ReentrantLock();        this.notEmpty = lock.newCondition();        this.notFull = lock.newCondition();    &#125;    public void put(final T item) throws InterruptedException &#123;        // 尝试上锁，使其线程安全        final ReentrantLock lock = this.lock;        lock.lockInterruptibly();        try &#123;            while (count &gt;= maxSize) &#123;                // 在队列满时使当前线程进入等待状态                // 直到其signal()方法被调用                notFull.await();            &#125;            // 存入元素，更新元素数量            items[count] = item;            count++;            // 通知此时队列非空，唤醒在调用take()方法时因队列空而被阻塞的线程            notEmpty.signal();        &#125; finally &#123;            // 最后无论如何要保证锁被释放            lock.unlock();        &#125;    &#125;    public T take() throws InterruptedException &#123;        // 尝试上锁，使其线程安全        final ReentrantLock lock = this.lock;        lock.lockInterruptibly();        try &#123;            while (count == 0) &#123;                // 在队列空时使当前线程进入等待状态                // 直到其signal()方法被调用                notEmpty.await();            &#125;            // 取出最早被放入的元素            final T item = (T) items[0];            // 领头的元素出列了，后面的往前挪一个位置            for (int i = 1; i &lt; count; i++) &#123;                items[i - 1] = items[i];            &#125;            // 更新元素数量            count--;            // 通知此时队列非满，唤醒在调用put()方法时因队列满而被阻塞的线程            notFull.signal();            return item;        &#125; finally &#123;            // 最后无论如何要保证锁被释放            notFull.signal();        &#125;    &#125;&#125;\n\n优化取出元素后更新队列内容如果这是一个非常大的队列，那么用一个for循环将所有元素往左移来更新队列内容的方式会消耗巨量的时间，显然它的效率是非常低的。相比于每次取出元素后都重排数组中的元素，Java中ArrayBlockingQueue则是利用循环数组的思路，通过两个指针来指示当前该从什么位置取或该向什么位置存（这不是经典的双指针玩法么，前段时间刷LeetCode还做过呢，咋就没想起来），这样每次存取元素后，只需要更新指针指向的位置就行，效率可想而知非常高。\npublic class BlockingQueue&lt;T&gt; &#123;    private final Object[] items;    private int count = 0;    private int takeIndex = 0;    private int putIndex = 0;    private final ReentrantLock lock;    private final Condition notEmpty;    private final Condition notFull;    public BlockingQueue(final int size) &#123;        this.items = new Object[size];        this.lock = new ReentrantLock();        this.notEmpty = lock.newCondition();        this.notFull = lock.newCondition();    &#125;    public void put(final T item) throws InterruptedException &#123;        final ReentrantLock lock = this.lock;        lock.lockInterruptibly();        try &#123;            while (count &gt;= items.length) &#123;                notFull.await();            &#125;            items[putIndex] = item;            count++;            // 存完元素之后，写指针往后挪一位            // 如果指针挪完之后突破了数组末尾，那么写指针循环到数组头            // 因为在队列满的时候线程会被阻塞，所以不会出现元素被覆盖的情况            if (++putIndex == items.length) &#123;                putIndex = 0;            &#125;            notEmpty.signal();        &#125; finally &#123;            lock.unlock();        &#125;    &#125;    public T take() throws InterruptedException &#123;        final ReentrantLock lock = this.lock;        lock.lockInterruptibly();        try &#123;            while (count == 0) &#123;                notEmpty.await();            &#125;            final T item = (T) items[takeIndex];            items[takeIndex] = null;            count--;            // 取出当前元素后，读指针往后挪一位            // 如果指针挪完之后突破了数组末尾，那么读指针循环到数组头            // 因为在队列空的时候线程会被阻塞，所以不会出现读到无效元素的情况            if (++takeIndex == items.length) &#123;                takeIndex = 0;            &#125;            notFull.signal();            return item;        &#125; finally &#123;            notFull.signal();        &#125;    &#125;&#125;\n","categories":["学知识"],"tags":["Java","BlockingQueue"]},{"title":"找到并处理Docker容器中的僵尸进程","url":"/finding-the-docker-container-of-a-zombie-process.html","content":"刚刚登录到我的服务器之后，看到motd提示有一个僵尸进程。本来处理僵尸进程很简单，杀掉它的父进程就行了。但是紧接着我发现这个进程是属于一个Docker容器的，因为我想要更优雅地处理掉它，就顺藤摸瓜找到了对应的容器并将其重启了。这里就记录下我的排查过程以供参考。\n\n\n因为僵尸进程在ps中的状态是Z，所以我首先用ps aux | grep &#39;Z&#39;找到这个僵尸进程的PID。此外因为僵尸进程无法被直接杀死，只能杀掉其父进程来将其连根拔起，所以我还需要用pstree命令找到它的父进程PID。\n$ ps aux | grep &#x27;Z&#x27;USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDroot      630024  0.0  0.0      0     0 ?        Z    00:55   0:00 [wget] &lt;defunct&gt;boris19+ 1180864  0.0  0.0   9696  2332 pts/0    S+   23:52   0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox Z$ pstree -p -s 630024systemd(1)───containerd-shim(2642178)───node(2642199)───wget(630024)$ ps aux | grep 2642199boris19+ 1181119  0.0  0.0   9696  2288 pts/0    S+   23:53   0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 2642199root     2642199  0.0  0.8 21690116 35956 ?      Ssl  Sep07   3:21 node /app/dist/index.js\n\n这里可以看到，僵尸wget的父进程是node，它的PID是2642199，而node的父进程是containerd-shim，也就是说这是一个Docker容器里的进程。我不确定这时候直接杀掉node能不能解决问题。但是老话说，来都来了，那不如继续挖下去。所以我开始尝试去找这个node是哪个容器运行的。\n在这我稍微走了点弯路。一开始我是想通过搜索index.js关键词来找到容器，所以执行了下docker ps | grep index.js，但是显然这行不通，不然也不会有这篇文了。在稍微网上冲浪之后，我学到了一个新命令systemd-cgls，它可以递归展示出cgroup的内容。因为Docker用到的技术之一就是cgroup，那么想必这就是突破口。在执行了它之后，它打出来了一大片东西，就像这样：\nControl group /:-.slice├─user.slice │ └─user-1000.slice │   ├─user@1000.service │   │ └─init.scope │   │   ├─1180199 /lib/systemd/systemd --user│   │   └─1180200 (sd-pam)│   └─session-35890.scope │     ├─1180196 sshd: boris1993 [priv]│     ├─1180374 sshd: boris1993@pts/0│     ├─1180376 -zsh│     ├─1182022 systemd-cgls│     └─1182023 pager├─init.scope │ └─1 /sbin/init└─system.slice   ├─irqbalance.service   │ └─941 /usr/sbin/irqbalance --foreground  ├─docker-27434de50f56b4e096bf5f38bafc76f5c74622c758be1a3f2b9531a3549f4550.scope   │ └─3204915 /jellyfin/jellyfin  ├─docker-adf03dfa49427d2d651cd9101c3033adb00396ed45c390dd6bfda0b9b73eeae3.scope   │ └─3775 /watchtower  ├─open-vm-tools.service   │ └─812 /usr/bin/vmtoolsd  ├─containerd.service   │ ├─   1017 /usr/bin/containerd  │ ├─   2197 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 0e752935d348c3a41d66fe539a06268ee29c6975d1d8f0da41bd0c52b5adf337&gt;  │ ├─   2479 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 6f5e80ca8977314f3d3a9a47a93509438040a6908bde34cc578cdd3e8263c01a&gt;  │ ├─   2497 /usr/bin/containerd-shim-runc-v2 -namespace moby -id db3750e57842656c285e9641c4391df926721c94722bfb323789967c3a76d23f&gt;  │ ├─   2886 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 46520cd2506055c50ef361f50381a65b5077ee7b7d2607e26290d70f8b7292ba&gt;  │ ├─   3449 /usr/bin/containerd-shim-runc-v2 -namespace moby -id d45b7c4e806f1c0e681f317b548f795ff09eab53bc4d1a6bde05aa1fbb5064ee&gt;  │ ├─   3451 /usr/bin/containerd-shim-runc-v2 -namespace moby -id adf03dfa49427d2d651cd9101c3033adb00396ed45c390dd6bfda0b9b73eeae3&gt;  │ ├─   3554 /usr/bin/containerd-shim-runc-v2 -namespace moby -id fd2c2b6dfd77b23c0cdd59d50b33bbf70ab8046bf27ffeb595fbeadadf0a4a99&gt;  │ ├─   3837 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 6195cb19666ad7c58ce26a115ba4f2ea3b32185acb0c33ff80cbade860693fd0&gt;  │ ├─   3840 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 559a99dc9580a0ddea8501df71bbce3affb90cf3a91152b8274ae508c20e0f32&gt;  │ ├─   3901 /usr/bin/containerd-shim-runc-v2 -namespace moby -id b11d850487921e336178dd9267b0f281eea279d0135d1d5b853e63f945246baf&gt;  │ ├─   3964 /usr/bin/containerd-shim-runc-v2 -namespace moby -id f3f9400aeab6331d55166b1c80cd3e27dc2083664a6126cccf279340ab738364&gt;  │ ├─   5852 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 722497f0a71df4060151468e590b5a69a49332ba379b17c9504077b18bbd1dfc&gt;\n\n好在它的输出就像less一样，可以上下卷动，也可以用ed命令（就像vi一样），那么自然而然，我可以用/2642199来找到那一行。\n├─docker-ce8f490423d791ed5f6d1aa2b705d797fc3fd2ddc34816d47c228f6fb9a20c63.scope │ └─2642199 node /app/dist/index.js\n\n好，现在我们知道了容器的ID是ce8f490423d7。虽然现在就可以重启它了，但是我的好奇心不允许我这么做，我想知道是哪个容器。所以\n$ docker ps | grep ce8f490423dce8f490423d7   boris1993/qinglong-bot:latest                                      &quot;docker-entrypoint.s…&quot;   12 days ago     Up 12 days (healthy)   0.0.0.0:3001-&gt;3000/tcp, :::3001-&gt;3000/tcp                                                                                     qinglong-bot\n\n啊好吧，竟然是我那个青龙的机器人。这里面我只用了wget来做容器的liveness check，我不理解这么简单的操作怎么就能整出一个僵尸进程来。算了，以后有空再分析吧。重启容器，杀掉僵尸，睡觉。\n$ docker restart ce8f490423dce8f490423d\n\n这时候再用ps aux | grep &#39;Z&#39;就看不到有僵尸进程，也就说明处置成功了。\n","categories":["小技巧"],"tags":["Docker","Zombie Process","僵尸进程"]},{"title":"记给Ubuntu的LVM卷扩容","url":"/expanding-ubuntu-lvm-partition.html","content":"刚刚看到我虚拟机里面的Ubuntu报硬盘空间不足，看了下发现是在安装那会，自动分区就只用了一半的空间，遂着手扩容。在这里记录下操作过程备忘。\n\n\n看见硬盘空间不足，首先下意识df -h了一下，确实满了。\n$ df -hFilesystem                         Size  Used Avail Use% Mounted ontmpfs                              392M  3.5M  389M   1% /runefivarfs                            64K   41K   19K  70% /sys/firmware/efi/efivars/dev/mapper/ubuntu--vg-ubuntu--lv   48G   45G  314M 100% /tmpfs                              2.0G     0  2.0G   0% /dev/shmtmpfs                              5.0M     0  5.0M   0% /run/lock/dev/sda2                          2.0G  266M  1.6G  15% /boot/dev/sda1                          1.1G  6.2M  1.1G   1% /boot/efitmpfs                              392M   12K  392M   1% /run/user/1000\n\n但是ESXi里面显示这个虚拟机是有一个100GB的虚拟磁盘的，而df只显示出将近50GB，那就是说要么有50GB没分，要么出别的问题了。首先排除最简单的情况，看看是不是有一半的硬盘没分进去。\n$ sudo lsblk -o NAME,FSTYPE,SIZE,MOUNTPOINT,LABELNAME                      FSTYPE        SIZE MOUNTPOINT                    LABELloop0                                  10.1M /snap/canonical-livepatch/282 loop1                                  10.7M /snap/canonical-livepatch/286 loop2                                   104M /snap/core/16928              loop3                                 104.2M /snap/core/17200              loop4                                  55.7M /snap/core18/2829             loop5                                  55.4M /snap/core18/2846             loop6                                    64M /snap/core20/2379             loop7                                  63.7M /snap/core20/2434             loop9                                  73.9M /snap/core22/1663             loop10                                   71M /snap/prometheus/86           loop11                                 38.8M /snap/snapd/21759             loop12                                 44.3M /snap/snapd/23258             loop13                                 73.9M /snap/core22/1722             sda                                     100G                               ├─sda1                    vfat            1G /boot/efi                     ├─sda2                    ext4            2G /boot                         └─sda3                    LVM2_member  96.9G                                 └─ubuntu--vg-ubuntu--lv ext4         48.5G /                             sr0                                    1024M                               \n\n从上面的命令输出可以看到，sda这块盘的100GB都认出来了，sda3分区也确实分了96.9GB，但是ubuntu--vg-ubuntu--lv这个卷只分配了48.5GB，也就是整个LVM的一半。\n查阅了教程，了解到可以用lvdisplay来检查LVM逻辑卷的空间，并可以用vgdisplay检查volume group的空间。遂分别执行。\n$ sudo vgdisplay  --- Volume group ---  VG Name               ubuntu-vg  System ID               Format                lvm2  Metadata Areas        1  Metadata Sequence No  2  VG Access             read/write  VG Status             resizable  MAX LV                0  Cur LV                1  Open LV               1  Max PV                0  Cur PV                1  Act PV                1  VG Size               &lt;96.95 GiB  PE Size               4.00 MiB  Total PE              24818  Alloc PE / Size       12409 / 48.47 GiB  Free  PE / Size       12409 / 48.47 GiB  VG UUID               zaKkwe-YemQ-dRcV-mrhe-v181-0Ixc-0KVaVL$ sudo pvdisplay  --- Physical volume ---  PV Name               /dev/sda3  VG Name               ubuntu-vg  PV Size               &lt;96.95 GiB / not usable 3.00 MiB  Allocatable           yes   PE Size               4.00 MiB  Total PE              24818  Free PE               12409  Allocated PE          12409  PV UUID               3FWWGz-CbIQ-BI9e-vMMd-p0Hu-eWUa-rZSeK8\n\n其实看不看都一样，已经知道根源是空间没全部分给这个卷。那么接下来就可以用lvextend来扩展指定的卷。我直接把所有的空余空间全都分配进去。\n$ sudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv   Size of logical volume ubuntu-vg/ubuntu-lv changed from 48.47 GiB (12409 extents) to &lt;96.95 GiB (24818 extents).  Logical volume ubuntu-vg/ubuntu-lv successfully resized.\n\nOK，卷扩容成功。但这时候实际的文件系统大小还没变，接下来需要用resize2fs扩容文件系统。\n$ sudo resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv resize2fs 1.47.0 (5-Feb-2023)Filesystem at /dev/mapper/ubuntu--vg-ubuntu--lv is mounted on /; on-line resizing requiredold_desc_blocks = 7, new_desc_blocks = 13The filesystem on /dev/mapper/ubuntu--vg-ubuntu--lv is now 25413632 (4k) blocks long.\n\n完事，这时候再df -h就能看到根挂载点有将近100GB的空间了，扩容成功。\n$ df -hFilesystem                         Size  Used Avail Use% Mounted ontmpfs                              392M  3.5M  389M   1% /runefivarfs                            64K   41K   19K  70% /sys/firmware/efi/efivars/dev/mapper/ubuntu--vg-ubuntu--lv   96G   45G   47G  50% /tmpfs                              2.0G     0  2.0G   0% /dev/shmtmpfs                              5.0M     0  5.0M   0% /run/lock/dev/sda2                          2.0G  266M  1.6G  15% /boot/dev/sda1                          1.1G  6.2M  1.1G   1% /boot/efitmpfs                              392M   12K  392M   1% /run/user/1000\n","categories":["小技巧"],"tags":["Ubuntu","LVM"]},{"title":"利用观察者模式实现动态刷新Bean中的配置","url":"/dynamically-refresh-config-with-observer-pattern.html","content":"公司有的项目里面还是采用把API key明文写在application.properties或者某个单独的properties文件来管理，这既不安全也不符合公司的规定。所以我通过利用观察者模式和Spring的事件机制，将其改为加密存储于配置中心，并且实现了在配置中心更新后，服务中生效的配置也可以立即更新。\n\n\n背景谈论实现之前，我先说一下我有哪些工具可以使用：\n\nSecretKeyService：一个可以保存对称及非对称加密密钥的服务\nCredentialVault：一个用来存放密钥的服务，密钥一旦存入，就只能再通过API的方式得到其原文\nConfigurationCenter：配置中心，不提供任何加密功能，在配置内容更新后可以通过Spring的事件通知到使用了这个配置的服务\n\n接到这个需求后，我下意识地觉得应该用CredentialVault解决问题，但是仔细想想，发现并不能。因为按照公司要求，API key的密码必须定期轮换，而老的密码在轮换之后就会马上失效。尽管我们有两套API key来避免前面的问题，但是更换API key密钥对又需要修改配置文件，并经历code review及上线部署，依旧需要一定的人工操作。我的设想是，只需要在一个地方设定好要生效的API key，接下来所有用到这个API key的服务都能自动更新。这样看来，似乎带有通知功能的配置中心是唯一解。但是配置中心不提供加密功能，所以还需要SecretKeyService提供一个对称加密密钥来把API key的密码加密，然后将它放到配置中心。\n记不住这些名字没关系，后面你也看不到它们了。\n实现在确定要用的工具之后，就可以开始着手将它们拼装在一起了。因为原本公司的代码需要保密，所以下面的代码更多是展示思路，大概率你不能直接拷出来放到你的项目中用。\nPropertyLoader因为直接把配置源从文件换成配置中心有一定的风险，保险起见我们决定逐步迁移配置，在迁移期间需要同时支持配置文件和配置中心。所以，就诞生了PropertyLoader。\npublic class PropertyLoader &#123;    // 负责加解密的服务，它会从SecretKeyService取得密钥并对给定字符串完成加解密    private final EncryptionDecryptionService encryptionDecryptionService;    // 配置中心的客户端    private final ConfigurationCenterClient configurationCenterClient;    // 配置文件    private final Properties properties;    public PropertyLoader(        final EncryptionDecryptionService encryptionDecryptionService,        final ConfigurationCenterClient configurationCenterClient,        final Properties properties,    ) &#123;        this.encryptionDecryptionService = encryptionDecryptionService;        this.configurationCenterClient = configurationCenterClient;        this.properties = properties;    &#125;    /**     * 从配置中心或配置文件取得一个key的值     *      * @param key Property或配置中心条目的key     * @return 取到的值，不会为null     * @throws PropertyNotFoundException 当在配置中心和配置文件中都找不到这个key时抛出，这通常意味着我们把某条配置漏掉了     */    public String loadProperty(final String key) &#123;        // 首先尝试从配置中心取值，如果取不到则返回null        String value = configurationCenterClient.getValue(key);        if (value == null) &#123;            value = properties.getProperty(key);        &#125;        if (value == null) &#123;            throw new PropertyNotFoundException(key);        &#125;        return value;    &#125;    /**     * 从配置中心或配置文件取得一个key的值并将其解密     *      * @param key Property或配置中心条目的key     * @return 取到的值，不会为null     * @throws PropertyNotFoundException 当在配置中心和配置文件中都找不到这个key时抛出，这通常意味着我们把某条配置漏掉了     */    public String loadEncryptedProperty(final String key) &#123;        final String encryptedValue = configurationCenterClient.getValue(key);        if (encryptedValue == null) &#123;            // 如果从配置中心拿不到值，那就从property文件中拿未加密的原文            final String plainValue = properties.getProperty(key);            if (plainValue == null) &#123;                throw new PropertyNotFoundException(key);            &#125;            return plainValue;        &#125;        return encryptionDecryptionService.decrypt(encryptedValue);    &#125;    /**     * 检查配置文件或配置中心是否有指定的key     *      * @param key 要检查的key     * @return 当这个key存在时返回true，反之返回false     */    public boolean hasProperty(final String key) &#123;        return configurationCenterClient.getKeys().contains(key) || properties.containsKey(key);    &#125;&#125;\n\n实现观察者模式上面说到了，我需要在监听到配置中心发出的事件后，更新相关对象中的配置。显然，在每个对象中都实现一个Spring事件监听器是很蠢的，我们应该在一处监听Spring事件，然后将其以某种方式广播到相关的对象。看起来，观察者模式是个不错的选择。\n观察者模式包含两个组件：notifier和subject。Subject作为观众，观察着某个事件；notifier则负责将事件通知给各个subject。\nOK，理论有这些就够了。接下来我们把它实现。首先是subject。\n观察者模式 - subject首选我们用一个抽象类定义一个观察者要有的行为，需要成为观察者的类将会继承这个抽象类。\npublic abstract class ConfigRefreshObserverSubject &#123;    // 观察到配置更新后，从PropertyLoader获取新的值    private final PropertyLoader propertyLoader;    // 这个观察者关心的key    // 因为要计算在这个对象中当前生效的配置的签名，所以要配置这个对象关心哪些key    // 然后取出这些key对应的value来计算签名    private final List&lt;String&gt; monitoredKeys;    // 当前生效的配置的一个签名    // 可以将其理解为当前配置的一个哈希，实际上是综合当前生效的配置生成的一个UUID    // 在收到配置刷新事件后，会将新配置的签名与当前签名比较    // 仅在签名不一致时刷新对象中的配置    private String configSignature;    public ConfigRefreshObserverSubject(        final PropertyLoader propertyLoader,        final List&lt;String&gt; monitoredKeys    ) &#123;        this.propertyLoader = propertyLoader;        this.monitoredKeys = monitoredKeys;        // 在初始化时计算当前生效配置的签名        this.configSignature = calculateConfigurationSignature();    &#125;    /**     * 这个方法留给实际成为subject的类去实现具体它要怎么刷新自己的配置     */    public abstract void refreshConfigImpl();    /**     * 这个方法留给notifier调用，实现更新配置及刷新配置签名     */    public final void refreshConfig() &#123;        refreshConfigImpl();        configSignature = calculateConfigurationSignature();    &#125;    public PropertyLoader getPropertyLoader() &#123;        return this.propertyLoader;    &#125;    public String getConfigSignature() &#123;        return this.configSignature;    &#125;    protected String calculateConfigurationSignature() &#123;        final StringBuilder newConfigurationSignatureSeedBuilder = new StringBuilder();        for (String key : monitoredKeys) &#123;            if (!propertyLoader.hasProperty(key)) &#123;                // 如果找不到某个关心的key，那么说明要么初始配置有问题，要么刷新的配置有问题                // 这时候尽早抛出异常引发开发人员关注                throw new IllegalArgumentException(&quot;Missing property: &quot; + key + &quot; for class: &quot; + this.getClass().getSimpleName());            &#125;            newConfigurationSignatureSeedBuilder.append(propertyLoader.loadProperty(key));        &#125;        return UUID.nameUUIDFromBytes(newConfigurationSignatureSeedBuilder.toString().getBytes()).toString();    &#125;&#125;\n\n观察者模式 - notifier在观察者模式中，notifier将作为一个单例存在，各个subject会注册到这个notifier，并在接收到事件后被notifier逐个通知。\npublic class ConfigRefreshObserverNotifier &#123;    // 用于存放各个subject的注册表    // 在某些情况下，同一个subject可能会被重复注册，所以这里我会把subject的类名作为key放在map中    // 方便在注册时检查是否重复注册    private static final Map&lt;String, ConfigRefreshObserverSubject&gt; subjects = new HashMap&lt;&gt;();    private ConfigRefreshObserverNotifier() &#123;        // notifier作为一个单例，我们不希望它被实例化        throw new UnsupportedOperationException(&quot;ConfigRefreshObserverNotifier shouldn&#x27;t be instantiated&quot;);    &#125;    /**     * 注册subject     * @param subject 待注册的subject对象     */    public static void register(final ConfigRefreshObserverSubject subject) &#123;        if (!isSubjectRegistered(subject)) &#123;            subjects.put(subject.getClass().getSimpleName(), subject);        &#125;    &#125;    /**     * 向各个subject发出配置更新的通知     */    public static void notifyObservers() &#123;        for (final ConfigRefreshObserverSubject subject : subjects.values()) &#123;            // 计算新配置的签名            final String newConfigSignature = subject.calculateConfigurationSignature();            // 仅当签名不同，即配置有变化时，才通知对应的subject更新            if (!newConfigSignature.equals(subject.getConfigSignature())) &#123;                subject.refreshConfig();            &#125;        &#125;    &#125;    private static boolean isSubjectRegistered(final ConfigRefreshObserverSubject subject) &#123;        final String subjectClassName = subject.getClass().getSimpleName();        return subjects.containsKey(subjectClassName);    &#125;&#125;\n\n观察者模式 - 实际的观察者类有了subject，接下来就可以让实际要监控配置更新的类继承ConfigRefreshObserverSubject，将它变为一个观察者，并实现更新配置的逻辑。这部分其实很简单，就是给对应的字段重新赋值。\npublic class SomeServiceClient extends ConfigRefreshObserverSubject &#123;    private String endpoint;    private String username;    private String password;    public SomeServiceClient(final PropertyLoader propertyLoader) &#123;        super(            propertyLoader,            List.of(&quot;some-service-endpoint&quot;, &quot;some-service-username&quot;, &quot;some-service-password&quot;)        );        initializeProperties();        ConfigRefreshObserverNotifier.register(this);    &#125;    private void initializeProperties() &#123;        final PropertyLoader propertyLoader = super.getPropertyLoader();        this.endpoint = propertyLoader.loadProperty(&quot;some-service-endpoint&quot;);        this.username = propertyLoader.loadProperty(&quot;some-service-username&quot;);        this.password = propertyLoader.loadEncryptedProperty(&quot;some-service-password&quot;);    &#125;    @Override    public void refreshConfigImpl() &#123;        initializeProperties();    &#125;    // 实际这个类的业务实现与本文无关，就省略了    // 说白了无非就是往endpoint发请求，带上username和password来认证&#125;\n\n上面的代码应该很容易理解，在配置刷新前，SomeServiceClient就用当前生效的配置去发请求，在配置刷新后，这个观察者就可以马上得知这个事件并从配置中心取得最新的值替换当前生效的配置。这个过程几乎是瞬间完成的，不会对业务产生影响。\n监听配置刷新事件上面洋洋洒洒实现了一堆东西，但最重要的一个还没有实现，那就是配置刷新事件的监听器。因为配置中心会通过Spring事件来发布，所以只需要找个地方实现一个@EventListener方法就行。\n@Configurationpublic class BeanFactory &#123;    @Bean    public SomeServiceClient someServiceClient(        final ResourceLoader resourceLoader,        final EncryptionDecryptionService encryptionDecryptionService     ) &#123;        final Properties properties = ResourcesUtils.loadProperties(            resourceLoader,            ResourcesUtils.CLASSPATH_META_INF + &quot;/some-service.properties&quot;        );        final PropertyLoader propertyLoader = new PropertyLoader(encryptionDecryptionService, properties);                return new SomeServiceClient(propertyLoader);    &#125;    @EventListener    public void handleConfigRefreshEvent(final ConfigRefreshEvent event) &#123;        if (event.getProjects().contains(&quot;my-config-project&quot;)) &#123;            ConfigRefreshObserverNotifier.notifyObservers();        &#125;    &#125;&#125;\n\n以上，就完成了一个利用配置中心的通知机制实现的配置动态更新功能。\n","categories":["学知识"],"tags":["设计模式","观察者模式"]},{"title":"Filter与interceptor的区别","url":"/filter-vs-interceptor.html","content":"Filter\nFilter是Servlet规范制定的，受Servlet容器支持，接口定义在javax.servlet包中。\nFilter是被Web Server（如Tomcat）调用。\nFilter 需要在 web.xml 中定义之后才会起作用。\nFilter只在请求的前后起作用，Servlet对于Filter来说是一个黑盒。\nFilter的执行顺序是：\nvoid init(FilterConfig) - 容器在初始化Filter时调用，在Filter生命周期内仅会被调用一次。方法可以抛出ServletException。\ndoFilter(ServletRequest, ServletResponse, FilterChain) - Web容器每一次请求都会调用该方法。该方法将容器的请求和响应作为参数传递进来， FilterChain用来调用下一个Filter。\nvoid destroy() - 当容器销毁 Filter 实例时调用该方法，可以在方法中销毁资源，该方法在 Filter 的生命周期只会被调用一次。\n\n\n\nInterceptor\nInterceptor是Spring容器内的，由Spring框架提供支持。接口HandlerInterceptor定义在org.springframework.web.servlet包中。\nInterceptor是被Spring调用。\nInterceptor可以深入到方法前后、异常抛出前后等，比起Filter有更大的弹性。\nInterceptor还允许用户介入请求的生命周期，可以在请求过程中获取信息，通常与请求更加耦合。\nInterceptor的执行顺序是：\n请求到达DispatcherServlet\nDispatcherServlet将请求发送至Interceptor，Interceptor执行preHandle()方法\n请求到达Controller\n请求结束后，Interceptor执行postHandle()方法\n\n\n\n\n\n参考文章\nSpring Interceptor vs Filter 拦截器和过滤器区别\nSpring Interceptor vs Filter - 掘金\nChapter 10. Filters and Interceptors - Jersey 2.37 User Guide\n\n","categories":["学知识"],"tags":["Java","Servlet","Spring MVC","Filter","Interceptor"]},{"title":"关闭IPv6或调整IPv6优先级以解决微软模拟飞行更新下载过慢的问题","url":"/fixing-downloading-too-slow-msfs-2020.html","content":"\n太长不看：关闭IPv6或许可以解决。如果你平时要用到IPv6，那就在更新之前先禁用IPv6，更新完了再打开；如果开不开IPv6对你来说没区别的话，那永久关闭也不是不可以。当然如果你愿意，下文提到的改注册表的方案也是可用的。\n\n\n这两天在给微软模拟飞行下载更新的时候，就算挂着网易UU，速度也一直很慢，时常在0.5MB&#x2F;s~5MB&#x2F;s之间波动，不论怎么换节点也不能跑出正常的速度。今天经过一顿上网冲浪，发现这个问题的根源，在IPv6。\n其实网上针对这个问题已经有一些解决方案了，但内容看下来都一样，不外乎教你怎么关掉IPv6。但我一方面要用到IPv6，另一方面也不想那么粗暴的解决问题，所以又稍微做了点研究，顺便写了个小工具方便其他有这个问题的玩家。 经过一段时间的试用后发现，临时&#x2F;永久关闭IPv6实际上是最优雅的方案……\n\n\n起先为了测试，我先禁用了IPv6，然后开启网易UU并开始更新微软模拟飞行，结果很符合预期，下载速度达到了150MB&#x2F;s，那么可以确定，IPv6就是这个问题的元凶。\n解决这个问题的突破点，在微软的这篇文档：为高级用户配置 Windows IPv6 的指南，里面提到我可以设定首选IPv4还是IPv6，于是我就在保留IPv6启动的同时，看了眼注册表里HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\Tcpip6\\Parameters路径下的DisabledComponents这个键，它的值转换成二进制是0000 1000，也就是说，目前我是首选使用IPv6进行连接的。\n那么，根据文档中的计算方法，我把第5个位改成了1，也就是改成了0010 1000，然后重启电脑，再挂上加速器并开始更新，这次果然速度也提升到了150MB&#x2F;s，说明这个修改有效了。\n顺便我又到https://test-ipv6.com/测试了一下，结果是你已经有 IPv6 地址了，但你的浏览器不太愿意用，那么可以确定，我现在仍然可以使用IPv6，但默认情况下系统将优先使用IPv4。\n考虑到其他玩家可能也有类似的困扰，我顺手写了一个小工具来做这件事，代码开源在GitHub。你们可以到Releases下载我编译好的版本。如果你访问GitHub有困难，也可以到GitEE的发行版中下载，它们是一样的。\n","categories":["瞎折腾"],"tags":["微软模拟飞行","微软模拟飞行更新慢","微软模拟飞行下载慢"]},{"title":"Floyd和Brent判圈算法","url":"/floyd-and-brent-cycle-detection-algorithm.html","content":"今天刷141. Linked List Cycle和142. Linked List Cycle II学到了两个新的判断链表是否存在环的算法 - Floyd龟兔赛跑算法和Brent判圈算法。\n\n\nFloyd龟兔赛跑算法这个算法的核心思想是，如果有限状态机、迭代函数或者链表上存在环，那么在某个环上以不同速度前进的两个指针必定在某个时间会相遇。也就是说，对于一个链表，可以分别使用两个指针进行遍历，慢指针一次走一步，快指针一次走两步，如果快慢指针相遇了，那么说明链表中存在环。\n当发现有环之后，让快指针停止，慢指针从当前位置继续向前遍历，并计算走过的步数。在慢指针与快指针再次相遇之后，慢指针走过的步数，就是环的长度。\n如果要计算环的入口，那么可以将一个指针移动到链表的起点，另一个指针不动，然后使两个指针每次同时向前走一步，当二者再次相遇的时候，指针所在的位置就是环的起点。至于这个操作的原理，原谅我数学很差，想不明白，所以借浅谈判圈算法 - xyZGHio一文中的解释：\n\n这里令起始处为A、环的入口处为B，在判断是否有环阶段时快慢相遇之处为C。并记AB长度为a、记BC长度为b、环的长度为r。且在判断是否有环过程中，快指针每次走2步、慢指针每次走1步。则快、慢指针相遇时，快指针走过的长度是慢指针走过长度的2倍。此时不难看出，当快、慢指针相遇时，快、慢指针走过的长度均是环长度的整数倍。故如果期望找到环的入口位置，即B处。则只需在两个指针相遇之时，将其中任意一个指针放置到起始处A，而另一个指针依然位于相遇处C。然后两个指针按照每次均走1步的速度向前走，当二者再次相遇之时，即是B处。原因在于，对于相遇后继续往前走的指针而言，由于其已经走过了若干圈环的长度，此时只需再走a步即可到达环的入口。这个地方换个角度想会更容易理解，如果该指针先走a步再走若干圈环的长度，其必然位于环的入口处；而对于相遇后从起始处A开始走的指针而言，其显然走a步后，必然也会位于环的入口处。故此时两个指针第二次相遇之时，说明他们均已经走完a步。即到达环的入口处。\n\n代码：\n/** * https://leetcode.com/problems/linked-list-cycle-ii */public class Solution &#123;    /**     * 找环的入口     */    public ListNode detectCycle(ListNode head) &#123;        if (head == null) &#123;            return null;        &#125;        ListNode intersect = getIntersect(head);        if (intersect == null) &#123;            return null;        &#125;        ListNode ptr1 = head;        ListNode ptr2 = intersect;        while (ptr1 != ptr2) &#123;            ptr1 = ptr1.next;            ptr2 = ptr2.next;        &#125;        return ptr1;    &#125;    /**     * 找快慢两个指针交会的位置     */    private ListNode getIntersect(ListNode head) &#123;        ListNode slow = head;        ListNode fast = head;        while (fast != null &amp;&amp; fast.next != null) &#123;            slow = slow.next;            fast = fast.next.next;            if (slow == fast) &#123;                return slow;            &#125;        &#125;        return null;    &#125;&#125;\n\nBrent判环算法Brent算法跟Floyd算法比较起来，其优点是缩短了判断是否有环的耗时（根据Wikipedia的说法，Brent算法的平均耗时比Floyd算法少36%），但是这个算法无法找到环的入口。\n这个算法同样会使用快和慢两个指针，判断是否有环的依据仍然是看两个指针是否会相遇，但是快指针和慢指针的走法与Floyd算法不同。这个算法中，快指针每一次会向前2n步（n为从1开始算起的回合数），即第一回合快指针走2步，第二回合走4步，以此类推。回合结束后，慢指针直接传送到快指针所在的位置。在每个回合的快指针移动过程中判断快指针是否与慢指针交会，如果交会，那么就判定存在环。\n代码：\n/** * https://leetcode.com/problems/linked-list-cycle */public class Solution &#123;    public boolean hasCycle(ListNode head) &#123;        ListNode slow = head;        ListNode fast = head;        // 当前回合快指针已走的步数        int stepsMovedByFast = 0;        // 当前回合快指针最多能走的步数        int stepLimitForFast = 2;        while (fast != null &amp;&amp; fast.next != null) &#123;            fast = fast.next;            stepsMovedByFast++;            // 如果快指针往前走的时候与慢指针相会            // 那就说明链表中存在环            if (fast == slow) &#123;                return true;            &#125;            // 快指针走完了能走的步数，即回合结束            if (stepsMovedByFast == stepLimitForFast) &#123;                // 快指针在下一回合能走的步数翻倍                stepLimitForFast *= 2;                // 清零计步器                stepsMovedByFast = 0;                // 慢指针传送到快指针所在的位置                slow = fast;            &#125;        &#125;        return false;    &#125;&#125;\n\n另外讲一个趣闻：这个算法还有个名字叫The Teleporting Turtle（会传送的乌龟），因为套用Floyd龟兔赛跑算法的概念，把快指针看作兔子，慢指针看作乌龟的话，那么乌龟是靠传送而不是行走前进的。\n","categories":["学知识"],"tags":["LeetCode","判圈算法","Floyd判圈算法","Brent判圈算法","龟兔赛跑算法","The Teleporting Turtle"]},{"title":"使用FlatMap“展平”一系列数据","url":"/flatten-a-group-of-data-by-flatmap.html","content":"在处理业务数据时，偶尔会需要从一系列逗号分隔的数据中取出各个元素并去重。本文将介绍如何通过Java 8中的FlatMap简化这个操作。\n\n\n介绍需求假设现在有这样子的一个排班信息：\n\n\n\n日期\n班组\n\n\n\n2019-01-02\nUnit-2, Unit-3\n\n\n2019-01-03\nUnit-2\n\n\n2019-01-04\nUnit-1, Unit-3\n\n\n2019-01-01\nUnit-2, Unit-3\n\n\n2019-01-05\nUnit-3\n\n\n2019-01-06\nUnit-1, Unit-2\n\n\n2019-01-07\nUnit-1\n\n\n2019-01-08\nUnit-2, Unit-3\n\n\n而目标是，从中取出这个表中的所有班组，并去重和排序，即最终的结果是Unit-1, Unit-2, Unit-3。\n准备示例数据在代码中，我们这样来构造这些数据：\n// 准备假数据WorkSchedule workSchedule1 = new WorkSchedule(&quot;2019-01-01&quot;, &quot;Unit-2, Unit-3&quot;);WorkSchedule workSchedule2 = new WorkSchedule(&quot;2019-01-02&quot;, &quot;Unit-2&quot;);WorkSchedule workSchedule3 = new WorkSchedule(&quot;2019-01-03&quot;, &quot;Unit-1, Unit-3&quot;);WorkSchedule workSchedule4 = new WorkSchedule(&quot;2019-01-04&quot;, &quot;Unit-2, Unit-3&quot;);WorkSchedule workSchedule5 = new WorkSchedule(&quot;2019-01-05&quot;, &quot;Unit-3&quot;);WorkSchedule workSchedule6 = new WorkSchedule(&quot;2019-01-06&quot;, &quot;Unit-1, Unit-2&quot;);WorkSchedule workSchedule7 = new WorkSchedule(&quot;2019-01-07&quot;, &quot;Unit-1&quot;);WorkSchedule workSchedule8 = new WorkSchedule(&quot;2019-01-08&quot;, &quot;Unit-2, Unit-3&quot;);List&lt;WorkSchedule&gt; workSchedules = new ArrayList&lt;&gt;(8);workSchedules.add(workSchedule1);workSchedules.add(workSchedule2);workSchedules.add(workSchedule3);workSchedules.add(workSchedule4);workSchedules.add(workSchedule5);workSchedules.add(workSchedule6);workSchedules.add(workSchedule7);workSchedules.add(workSchedule8);\n\n不使用流的做法如果不使用流，那么我们可以通过for循环取数据，和使用TreeSet实现去重和排序。\nTreeSet&lt;String&gt; unitNames = new TreeSet&lt;&gt;();for (WorkSchedule workSchedule : workSchedules) &#123;    String[] arrUnitName = workSchedule.getUnits().split(&quot;,&quot;);    for (String unitName : arrUnitName) &#123;        unitNames.add(unitName.trim());    &#125;&#125;\n\n使用流的做法如果使用流，则可以这样做：\nList&lt;String&gt; units = workSchedules.stream()        // 根据逗号，将班组名拆分成 String[]        // 这一步将生成8个新的 String[] 对象        .map(i -&gt; i.getUnits().split(&quot;,&quot;))        // 从上面的 String[] 生成一个新的        .flatMap(Arrays::stream)        // 切掉每个元素两端的空格        .map(String::trim)        // 去重        .distinct()        // 排序        .sorted()        // 收集到一个List中        .collect(Collectors.toList());\n\n如果上面代码的注释还是无法让您理解，那么，我还准备了每一步流操作的示意图。(感谢Intellij IDEA提供了这个强大的功能)\n首先，在map方法中，List的每个元素都会根据逗号分隔，并生成一个String[]对象。\n\nflatmap方法会将上面map方法返回的各个流“拼接”成为一个流，即“展平”。\n\n接下来继续使用map方法，对流中的每个元素进行trim操作。\n\n然后调用distinct和sorted方法实现去重和排序。\n\n最后，使用collect方法，将流中的数据“收集”到一个List中。\n\n","categories":["小技巧"],"tags":["Java","代码技巧","FlatMap"]},{"title":"在Google Cloud Logging的日志输出中增加类名、方法名和行数","url":"/gcp-logging-show-class-and-method-name.html","content":"由于我司目前的项目都运行在Google Cloud Platform(以下简称GCP)上，那么自然而然的，我们选择了使用GCP的Logging来查看日志。在使用过程中，我们发现了一个问题，那就是我们无法直观的看到日志是从什么地方打印出来的，经常需要通过日志内容，在代码里面通过全文搜索来定位。这样就产生了一个需求：可不可以把这条日志所在的类、方法，和行数一起打印在日志中？\n\n\n日志是怎么生成的既然要实现自动化地修改日志的内容，那么我们首先得要知道，日志的内容是怎么产生的。根据我们的logback-spring.xml中的配置，我们可以定位到，日志是从StackdriverJsonLayout这个类中生成出来的。\n打开这个文件，阅读源码，发现有一个名为toJsonMap的方法很像我们这次的目标，为了大家省事，我从GitHub上把这段源码拿到了这里。\n/** * Convert a logging event into a Map. * @param event the logging event * @return the map which should get rendered as JSON */@Overrideprotected Map&lt;String, Object&gt; toJsonMap(ILoggingEvent event) &#123;    Map&lt;String, Object&gt; map = new LinkedHashMap&lt;&gt;();    if (this.includeMDC) &#123;        event.getMDCPropertyMap().forEach((key, value) -&gt; &#123;            if (!FILTERED_MDC_FIELDS.contains(key)) &#123;                map.put(key, value);            &#125;        &#125;);    &#125;    if (this.includeTimestamp) &#123;        map.put(StackdriverTraceConstants.TIMESTAMP_SECONDS_ATTRIBUTE,                TimeUnit.MILLISECONDS.toSeconds(event.getTimeStamp()));        map.put(StackdriverTraceConstants.TIMESTAMP_NANOS_ATTRIBUTE,                TimeUnit.MILLISECONDS.toNanos(event.getTimeStamp() % 1_000));    &#125;    add(StackdriverTraceConstants.SEVERITY_ATTRIBUTE, this.includeLevel,            String.valueOf(event.getLevel()), map);    add(JsonLayout.THREAD_ATTR_NAME, this.includeThreadName, event.getThreadName(), map);    add(JsonLayout.LOGGER_ATTR_NAME, this.includeLoggerName, event.getLoggerName(), map);    // 注意看这个if块，我们的日志语句就是在这里被处理的    if (this.includeFormattedMessage) &#123;        String message = event.getFormattedMessage();        if (this.includeExceptionInMessage) &#123;            IThrowableProxy throwableProxy = event.getThrowableProxy();            if (throwableProxy != null) &#123;                String stackTrace = getThrowableProxyConverter().convert(event);                if (stackTrace != null &amp;&amp; !stackTrace.equals(&quot;&quot;)) &#123;                    message += &quot;\\n&quot; + stackTrace;                &#125;            &#125;        &#125;        map.put(JsonLayout.FORMATTED_MESSAGE_ATTR_NAME, message);    &#125;    add(JsonLayout.MESSAGE_ATTR_NAME, this.includeMessage, event.getMessage(), map);    add(JsonLayout.CONTEXT_ATTR_NAME, this.includeContextName, event.getLoggerContextVO().getName(), map);    addThrowableInfo(JsonLayout.EXCEPTION_ATTR_NAME, this.includeException, event, map);    addTraceId(event, map);    add(StackdriverTraceConstants.SPAN_ID_ATTRIBUTE, this.includeSpanId,            event.getMDCPropertyMap().get(StackdriverTraceConstants.MDC_FIELD_SPAN_ID), map);    if (this.serviceContext != null) &#123;        map.put(StackdriverTraceConstants.SERVICE_CONTEXT_ATTRIBUTE, this.serviceContext);    &#125;    if (this.customJson != null &amp;&amp; !this.customJson.isEmpty()) &#123;        for (Map.Entry&lt;String, Object&gt; entry : this.customJson.entrySet()) &#123;            map.putIfAbsent(entry.getKey(), entry.getValue());        &#125;    &#125;    addCustomDataToJsonMap(map, event);    return map;&#125;\n\n看起来篇幅好像很大，但是仔细一看，不难明白，它就是生成了一个Map，而这个Map里面放的，就是最后我们在GCP Logging里面将要看到的日志。好了，目标找到了，那就准备动手吧。\n修改日志内容要修改的方法找到了，但是我们仍不知道应该怎么取得类名、方法和行号。经过又一顿网上冲浪，加上在源码里面一刨一刨的翻，和一点小小的灵感，最后得知这些信息全部可以通过ILoggingEvent#getCallerData()方法取得。请原谅因为时间过去的有点久，我已经找不到当时给我指路的文章，所以也就不能放在下面的参考中了。\n那么我们就开始着手实现这个需求吧。首先在项目里面新建一个类，因为我们的目的是重写toJsonMap这个方法，所以要让它继承StackdriverJsonLayout。\npublic class GcpLoggingLayout extends StackdriverJsonLayout &#123;    @Override    public Map&lt;String, Object&gt; toJsonMap(ILoggingEvent event) &#123;        // 先调用原本的方法，把日志的信息全部生成好        Map&lt;String, Object&gt; map = super.toJsonMap(event);        StackTraceElement[] callerData = event.getCallerData();        // 如果没有callerData，那后面也没法操作了        if (callerData == null || callerData.length == 0) &#123;            return map;        &#125;        // 第一个元素就是这个日志所在位置的信息        StackTraceElement stackTraceElement = callerData[0];        // 我只需要类名，不需要前面的包名        // 而 getClassName() 会返回类的全限定名        // 所以我把它拆了，取最后一个元素，也就是类名        String[] classNameParts = stackTraceElement.getClassName().split(&quot;\\\\.&quot;);        String callerClass = classNameParts[classNameParts.size - 1];        String methodName = stackTraceElement.getMethodName();        String lineNumber = stackTraceElement.getLineNumber().toString();        // 这个是原本的日志信息        String originalMessage = map.get(JsonLayout.FORMATTED_MESSAGE_ATTR_NAME);        // 把类名、方法名、行数附加在原本的信息前面        String enhancedMessage = &quot;class=&quot; + callerClass + &quot; &quot;                                + &quot;method=&quot; + methodName + &quot; &quot;                                + &quot;line=&quot; + lineNumber + &quot; &quot;                                + originalMessage;        // 把改好的日志放回去        map.put(JsonLayout.FORMATTED_MESSAGE_ATTR_NAME, enhancedMessage);        return map;    &#125;&#125;\n\n这样我们的日志里就带上了类名、方法名，和行号了。但是到了这里并没有结束，因为上面的代码有一个问题：框架打出来的日志也带上了这些信息。虽然不影响使用，但是它不好看，我不想要。所以我决定继续优化一下。\n继续优化稍加思索，我提出了两个新的需求：\n\n要根据包名过滤，只给指定的包里面的类加上这些信息。这样，我就可以排除掉所有无关的包。\n还要根据类名过滤，遇到被指定的类，我就不加上这些信息。因为对于启动类(就是放main方法那个)，和一些用于健康检查的类，它们打出的日志也没必要加上这些信息。\n\n但是很明显，原厂的功能并不足以实现这两个需求。我为了这两个需求，新增了两个参数：appPackage用来指定应用的包名，excludedClassNames用来指定要被排除的类。\n增加了参数，那必然也要修改代码。\npublic class GcpLoggingLayout extends StackdriverJsonLayout &#123;    private String appPackage;    private List&lt;String&gt; excludedClassNames;    public void setAppPackage(String appPackage) &#123;        this.appPackage = appPackage;    &#125;    public String getAppPackage() &#123;        return this.appPackage;    &#125;    // 这里需要注意    // 我为了 logback-spring.xml 写起来方便，所以在XML中这个属性是一个逗号分隔的字符串    // 但是在代码里面，我为了用 contains 方法，所以用了一个List来存放    public void setExcludedClassNames(String excludedClassNames) &#123;        this.excludedClassNames =            Arrays.stream(excludedClassNames.split(&quot;,&quot;))                .map(String::trim)                .collect(Collectors.toList());    &#125;    // getter就没啥讲究了    public List&lt;String&gt; getExcludedClassNames() &#123;        return this.excludedClassNames;    &#125;    @Override    public Map&lt;String, Object&gt; toJsonMap(ILoggingEvent event) &#123;        // 先调用原本的方法，把日志的信息全部生成好        Map&lt;String, Object&gt; map = super。toJsonMap(event);        StackTraceElement[] callerData = event.getCallerData();        // 如果没有callerData，那后面也没法操作了        if (callerData == null || callerData.length == 0) &#123;            return map;        &#125;        // 第一个元素就是这个日志所在位置的信息        StackTraceElement stackTraceElement = callerData[0];        // 为了通过包名匹配，所以先拿出来全限定名        String callerClassFullName = stackTraceElement.getClassName();        // 如果不是应用所在的包打出来的日志，那么不处理        if (!callerClassFullName.contains(appPackage)) &#123;            return map;        &#125;        String[] classNameParts = stackTraceElement.getClassName().split(&quot;\\\\.&quot;);        String callerClass = classNameParts[classNameParts.size - 1];        // 如果这个类是被排除的，那就不处理        if (excludedClassNames.contains(callerClass)) &#123;            return map;        &#125;        String methodName = stackTraceElement.getMethodName();        String lineNumber = String.valueOf(stackTraceElement.getLineNumber());        // 这个是原本的日志信息        String originalMessage = map.get(JsonLayout.FORMATTED_MESSAGE_ATTR_NAME).toString();        // 把类名、方法名、行数附加在原本的信息前面        String enhancedMessage = &quot;class=&quot; + callerClass + &quot; &quot;                                + &quot;method=&quot; + methodName + &quot; &quot;                                + &quot;line=&quot; + lineNumber + &quot; &quot;                                + originalMessage;        // 把改好的日志放回去        map.put(JsonLayout.FORMATTED_MESSAGE_ATTR_NAME, enhancedMessage);        return map;    &#125;&#125;\n\n除了修改代码，日志配置文件logback-spring.xml也要针对我们新增的两个属性作出修改。\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;configuration&gt;  &lt;include resource=&quot;org/springframework/cloud/gcp/autoconfigure/logging/logback-appender.xml&quot;/&gt;  &lt;include resource=&quot;org/springframework/cloud/gcp/logging/logback-json-appender.xml&quot;/&gt;  &lt;include resource=&quot;org/springframework/boot/logging/logback/defaults.xml&quot;/&gt;  &lt;!-- CONSOLE输出部分略 --&gt;  &lt;appender name=&quot;CONSOLE_JSON&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;    &lt;encoder class=&quot;ch.qos.logback.core.encoder.LayoutWrappingEncoder&quot;&gt;      &lt;!-- 指向我们上面写的GcpLoggingLayout --&gt;      &lt;layout class=&quot;com.boris1993.myapplication.config.GcpLoggingLayout&quot;&gt;        &lt;!-- 应用的包名 --&gt;        &lt;appPackage&gt;com.boris1993.myapplication&lt;/appPackage&gt;        &lt;!-- 要排除的类名 --&gt;        &lt;!-- 逗号后面的空格可有可无，为了好看我选择加上，反正最后会被trim掉 --&gt;        &lt;excludedClassNames&gt;APP, HealthCheck&lt;/excludedClassNames&gt;        &lt;!-- 其他GCP Logging配置略 --&gt;      &lt;/layout&gt;    &lt;/encoder&gt;  &lt;/appender&gt;  &lt;!-- springProfile配置略 --&gt;&lt;/configuration&gt;\n\n然后就可以部署到GCP上面看效果了，或者在本机使用对应的profile启动，然后检查控制台里面打印出来的JSON。\n[^1]: Stackdriver Logging[^2]: StackdriverJsonLayout.java - GitHub\n","categories":["小技巧"],"tags":["Java","Spring Boot","Google Cloud Logging","Stackdriver"]},{"title":"为了改桥接，我决定破解中兴F450G V2光猫","url":"/hacking-zte-f450g-v2-onu.html","content":"还记得我之前买了个猫棒来替换光猫么？用了一个来月，发现这玩意真的不稳定，短则几分钟长则一两天它必定自己重启一次，导致我的网络时不时就会断线。这玩意不好使，我也没有别的光猫，只好找电信装维师傅给我改桥接。可哪成想一个师傅踢皮球让我找另一个师傅，另一个一听我说是悦me直接就说不能改，只有SDN光猫能改。那你们逼我的，我自己改。\n\n\n光猫信息我的这款光猫是EPON上行悦me网关，型号是中兴 ZXHN F450G，区域是上海，硬件版本V2.0。\n\n准备工作开始折腾光猫之前，一定要把当前光猫里存的配置备份好，尤其是LOID，万一折腾砸了，还可以改回去，或者重新注册，让局端重新下发正确的配置。我已经有了telecomadmin的密码，就直接登进后台，把配置抄了下来。\n\n开始动手\n首先我要大力感谢这篇帖子，那天我就是照着这里面的内容操作的。https://www.chinadsl.net/thread-127666-1-1.html\n\n复位光猫，开启telnet首先，拔掉光纤，用细针捅住光猫后面的复位孔，直到光猫前面板下方的两个灯都熄灭，只剩下上面的电信logo亮着，以彻底复位光猫。这一步是为了重新打开telnet，因为这个光猫在插上光纤之后telnet就会被禁用，只有拔掉光纤并彻底复位才能再把telnet打开。\n复位后光猫会自动重启，拿根网线，把电脑接上光猫的LAN口（别接LAN 2，那是给IPTV用的），电脑设定静态IP地址192.168.1.100（其实除了192.168.1.1之外，整个192.168.1.0/24网段随便你选），子网掩码255.255.255.0。\n接下来持续ping 192.168.1.1，光猫启动好之后会有响应，接下来用telnet连接192.168.1.1，使用用户名root和密码Zte521登进光猫shell。\n需要通过telnet做的配置首先用以下命令实现在光纤插入后telnet仍然可用。\n# 打开telnetsendcmd 1 DB set TelnetCfg 0 TS_Enable 1# 允许局域网登陆sendcmd 1 DB set TelnetCfg 0 Lan_Enable 1# 插上光纤后不关闭telnetsendcmd 1 DB set TelnetCfg 0 Lan_EnableAfterOlt 1# 最多允许5个用户同时登陆telnet，避免自己被锁在外面sendcmd 1 DB set TelnetCfg 0 Max_Con_Num 5# 打开23端口sendcmd 1 DB set PortControl 3 PortEnable 1# 保存配置sendcmd 1 DB save\n\n然后可以用sendcmd 1 DB p TelnetCfg确认下配置。后面的操作也可以用sendcmd 1 DB p 表名这条命令确认配置，就不再重复了。\n&lt;Tbl name=&quot;TelnetCfg&quot; RowCount=&quot;1&quot;&gt;        &lt;Row No=&quot;0&quot;&gt;                &lt;DM name=&quot;TS_Enable&quot; val=&quot;1&quot;/&gt;                &lt;DM name=&quot;Wan_Enable&quot; val=&quot;0&quot;/&gt;                &lt;DM name=&quot;Lan_Enable&quot; val=&quot;1&quot;/&gt;                &lt;DM name=&quot;TS_Port&quot; val=&quot;23&quot;/&gt;                &lt;DM name=&quot;TSLan_Port&quot; val=&quot;23&quot;/&gt;                &lt;DM name=&quot;TS_UName&quot; val=&quot;root&quot;/&gt;                &lt;DM name=&quot;TS_UPwd&quot; val=&quot;Zte521&quot;/&gt;                &lt;DM name=&quot;TSLan_UName&quot; val=&quot;root&quot;/&gt;                &lt;DM name=&quot;TSLan_UPwd&quot; val=&quot;Zte521&quot;/&gt;                &lt;DM name=&quot;Max_Con_Num&quot; val=&quot;5&quot;/&gt;                &lt;DM name=&quot;ProcType&quot; val=&quot;0&quot;/&gt;                &lt;DM name=&quot;Lan_EnableAfterOlt&quot; val=&quot;1&quot;/&gt;                &lt;DM name=&quot;TS_Sprtwl_Mode&quot; val=&quot;0&quot;/&gt;                &lt;DM name=&quot;WanWebLinkToTS&quot; val=&quot;0&quot;/&gt;        &lt;/Row&gt;&lt;/Tbl&gt;\n\n然后禁用掉TR069。电信局端通过TR069协议下发配置，而我们自己改的桥接肯定跟局端的配置不一样，不关掉这玩意相当于白忙活。\n# 禁用TR069的58000端口sendcmd 1 DB set PortControl 6 PortEnable 0sendcmd 1 DB save\n\n虽然原帖中说还要把远程管理配置乱改以斩草除根，我是懒得改，应该关了端口就行，因为一来直到现在一个星期了我的配置都还在，二来重新设备注册的时候也会卡在等待ITMS下发配置，所以TR069应该是已经被掐了。\n然后需要骗过光猫，让它认为ITMS下发配置已经成功。一开始搞的时候没做这一步，就发现过几分钟网页就打不开，nslookup一下发现解析全去了192.168.1.1，又打开个网站说试试，结果直接被光猫劫持到了设备注册页面。直接在光猫的表里面改掉ITMS状态，再清理DNS缓存之后就正常了。\nsendcmd 1 DB set PDTCTUSERINFO 0 Status 0sendcmd 1 DB set PDTCTUSERINFO 0 Result 1sendcmd 1 DB save\n\n接下来改掉光猫的最大用户数。其实要改桥接的话，这个改不改都没关系，反正我也不用光猫上网。但还是顺手改了。也不需要改什么最大用户数，直接关掉限制完事。\nsendcmd 1 DB set CltLmt 8 Enable 0sendcmd 1 DB save\n\n至此telnet里面要搞的东西就都搞完了。虽然原帖里面还提到改掉telecomadmin的密码，但是既然TR069都废了，密码自然也不会再变。正好之前从装维师傅那要到的密码还能用，我也懒得再改。\n登进后台配桥接插上光纤，用telecomadmin登进光猫后台。先看一眼网关注册信息，因为光猫复位不会丢LOID，所以插上光纤之后，光路（OLT）认证应该就是认证成功的状态；而上面在telnet里面也改掉了ITMS认证状态，所以管理（ITMS）注册也应该是注册成功的状态。\n\n进入网络的网络设置，点开连接名称下拉菜单，看有没有INTERNET_B_VID，如果有，而且连接模式是路由，那就删了它。然后下拉菜单选新建WAN连接，连接模式选桥接，业务类型选上网，IP模式选IPv4&amp;IPv6，点保存，光猫端就配置完成了。\n\n齐活到这光猫就改好桥接了，从光猫上扯根网线连上自己的路由器拨号，就可以上网了。正好改完桥接那天，RouterOS有更新，看PPPoE接口的uptime可以看出来，从RouterOS重启之后到现在，链接就没断过。之前用猫棒的时候，uptime就没多于过2天。\n\n","categories":["瞎折腾"],"tags":["光猫","桥接","F450G"]},{"title":"在Dev Container里写Hexo博客","url":"/hexo-in-dev-container.html","content":"我们都知道，Docker和容器化技术让运维有了质的飞跃，从此我们不必再担心软件运行所需的繁杂环境，只需要拉取镜像并运行就可以一步到位部署好软件的运行环境。但是在开发过程中，我们仍然需要在本机安装各种依赖，一不小心又会把本机的环境搞乱掉，更不提不同版本的语言之间可能存在的冲突。因此，就出现了Dev Container，顾名思义就是在容器环境中开发，这样我们在开发时也可以享受到统一且隔离的开发环境。\n正好我最近也在开始接触Dev Container并使用它给一个开源项目成功提交了代码，正好趁热打铁，把写博客的Hexo环境从本机挪到Dev Container中。\n\n\n配置VSCode我的计划是用VSCode来在容器中写博客，那么为了让VSCode支持Dev Container，我们需要安装Dev Containers这个VSCode扩展。哦对，你还得有个Docker，不论是本机还是在哪个服务器上。我这就用本机的Docker演示了。\n装好扩展之后，就可以开始写Dev Container的配置了。因为不同项目对应的开发容器必然是不同的，所以Dev Container的配置会放到项目目录中。通常来说，Dev Container相关的文件都会放到项目根目录下的.devcontainer目录中，所以我们可以在项目目录下创建.devcontainer/devcontainer.json，并填入如下内容：\n&#123;    // Dev Container的名字    &quot;name&quot;: &quot;blog&quot;,    // 基础Docker镜像    &quot;image&quot;: &quot;node:lts-alpine&quot;,    // 容器创建好之后要执行的命令    &quot;postCreateCommand&quot;: &quot;sh .devcontainer/post_create.sh&quot;,    // 要转发到主机的端口    &quot;forwardPorts&quot;: [4000]&#125;\n\n因为基础镜像node:lts-alpine中并不包含我们需要的git、gpg、hexo等环境，所以我们需要通过postCreateCommand指定容器在启动之后执行命令来安装这些依赖。post_create.sh的内容如下：\n#!/bin/shapk add --no-cache gpg gitnpm install -g hexo-cli\n\n然后用快捷键ctrl-shift-p（macOS就是⌘-shift-p）唤出命令面板，选择Dev Containers: Rebuild and Reopen in Container来构建容器并进入容器开发。如果将来devcontainer.json或相关的配置发生变化，我们也可以在命令面板中选择Dev Containers: Rebuild Container来重建容器。\n编写容器的Dockerfile上面提到的通过postCreateCommand来安装依赖的方式，虽然可行，但是不优雅，毕竟每次创建容器都要重新安装一次，就很浪费。所以我们也可以给我们的Dev Container写一个Dockerfile，这样就可以只初始化一次然后一直用下去了。\nFROM node:lts-alpineLABEL devcontainer.metadata = &#x27;[&#123; \\    &quot;forwardPorts&quot;: [4000] \\&#125;]&#x27;RUN apk add --no-cache gpg git \\ &amp;&amp; npm install -g hexo-cli\n\n与部署用的Dockerfile不同，我们只需要写配置环境相关的命令就可以，不需要把工作空间拷贝进去，因为Dev Container会自动把这个目录mount到容器中。此外，在Dockerfile中还可以在devcontainer.metadata中提前指定好一些配置，以减少devcontainer.json的行数。\n配置GPG上面一步完成之后，Dev Container的配置其实就完成了。但如果你像我一样为Git配置了GPG签名，同时又是在macOS下开发，那么你还需要对GPG做一些额外的配置。首先我们需要安装pinentry-mac，然后编辑~/.gnupg/gpg-agent.conf文件，添加一行pinentry-program /usr/local/bin/pinentry-mac。如果你曾经在JetBrains IDE中点过它的Configure GPG Agent to support own pinentry，那么你就要把它添加的那行pinentry-program替换成这一条，否则我们在容器中提交更新的时候，GPG会因为找不到/Users/username/.gnupg/pinentry-ide.sh而报错。\n参考文档\nDeveloping inside a Container\nDevelop on a remote Docker host\n使用Dev Container开发 - Claws小花园\n\n","categories":["瞎折腾"],"tags":["Hexo","Dev Container"]},{"title":"从字节码看synchronized关键字是怎么工作的","url":"/how-synchronized-works-in-java.html","content":"昨天面试的时候被问到Java中的synchronized关键字是什么原理，虽然凭着记忆打出来是通过控制对象头的Monitor来实现，但是毕竟没吃透这个知识点，还是没啥底气。干脆，这次就从字节码上看看，用了synchronized关键字的方法，到底是怎么执行的。\n\n\n示例代码说起synchronized的最简单的使用场景，我马上就想起双检单例模式。\npublic class Test &#123;    private static volatile Test INSTANCE;    private Test() &#123;    &#125;    public static Test getInstance() &#123;        if (INSTANCE == null) &#123;            synchronized (Test.class) &#123;                if (INSTANCE == null) &#123;                    INSTANCE = new Test();                &#125;            &#125;        &#125;        return INSTANCE;    &#125;    public void print() &#123;        System.out.println(&quot;test&quot;);    &#125;&#125;\n\n反编译成字节码把Test类先编译了，然后用javap -c Test.class反编译，就能看到这个类的字节码了。\nCompiled from &quot;Test.java&quot;public class Test &#123;  public static Test getInstance();    Code:       0: getstatic     #7                  // 把静态变量INSTANCE加载到栈       3: ifnonnull     37                  // 如果值不是null，那么跳转到标签37       6: ldc           #8       8: dup       9: astore_0      10: monitorenter                      // 进入synchronized块      11: getstatic     #7                  // 把静态变量INSTANCE加载到栈      14: ifnonnull     27                  // 如果值不是null，那么跳转到标签27      17: new           #8                  // new一个Test对象      20: dup      21: invokespecial #13                 // 执行构造函数      24: putstatic     #7      27: aload_0      28: monitorexit                       // 退出synchronized块      29: goto          37      32: astore_1      33: aload_0      34: monitorexit      35: aload_1      36: athrow      37: getstatic     #7                  // Field INSTANCE:LTest;      40: areturn    Exception table:       from    to  target type          11    29    32   any          32    35    32   any  public void print();    Code:       0: getstatic     #14                 // Field java/lang/System.out:Ljava/io/PrintStream;       3: ldc           #20                 // String test       5: invokevirtual #22                 // Method java/io/PrintStream.println:(Ljava/lang/String;)V       8: return&#125;\n\n注意看10: monitorenter和28: monitorexit这两条字节码，这就是synchronized关键字实际做了的事。\nJava对象头和Monitor要说明白monitorenter和monitorexit实际干了点啥，那就得先整明白Java对象的对象头。\n一个Java对象，在内存中的布局包括三块区域：对象头、实例数据、和对齐填充。\n\n别的东西咱们先不看，只看对象头这部分。对象头的最后2bit就存储了锁的标志位。\n至于Monitor，Java官方文档是这么描述的：\n\nSynchronization is built around an internal entity known as the intrinsic lock or monitor lock. (The API specification often refers to this entity simply as a “monitor.”) Intrinsic locks play a role in both aspects of synchronization: enforcing exclusive access to an object’s state and establishing happens-before relationships that are essential to visibility.\nEvery object has an intrinsic lock associated with it. By convention, a thread that needs exclusive and consistent access to an object’s fields has to acquire the object’s intrinsic lock before accessing them, and then release the intrinsic lock when it’s done with them. \n同步是围绕着一个名为“内在锁”或“monitor锁”的机制构建的。（API规范文档中，通常会称其为“monitor”）内在锁一方面保证了针对一个对象的专属访问权限，另一方面保证了对可见性很重要的happens-before原则。每个对象都会有一个与其相关联的内在锁。按照约定，如果一个线程需要持续持有对一个对象的独家访问权限，那么这个线程必须先获得到这个对象的内在锁，然后在执行完毕后释放掉这个内在锁。\n\n代码执行到monitorenter指令，说明开始进入synchronized代码块，这时候JVM会尝试获取这个对象的monitor所有权，即尝试加锁；而执行到monitorexit指令，就说明要么synchronized代码块执行完毕，要么代码执行的时候抛出了异常，这时候JVM就会释放这个对象的monitor所有权，即释放锁。\n继续深入细节上面说的也是云里雾里的，咱继续往深处挖，看看具体的实现。\nMonitor这个东西，看Java源码找不到，得找虚拟机的C++源码。比如我们常用的HotSpot虚拟机中，Monitor是由ObjectMonitor类实现的：\n// 为了解释方便，仅抄录了相关的代码，并重排了位置class ObjectMonitor &#123;    public:        ObjectMonitor() &#123;            _header       = NULL;            _count        = 0;            _waiters      = 0,            _recursions   = 0;            _object       = NULL;            _owner        = NULL;            _WaitSet      = NULL;            _WaitSetLock  = 0 ;            _Responsible  = NULL ;            _succ         = NULL ;            _cxq          = NULL ;            FreeNext      = NULL ;            _EntryList    = NULL ;            _SpinFreq     = 0 ;            _SpinClock    = 0 ;            OwnerIsThread = 0 ;            _previous_owner_tid = 0;        &#125;        private:        volatile intptr_t  _count;          // reference count to prevent reclaimation/deflation                                            // at stop-the-world time.  See deflate_idle_monitors().                                            // _count is approximately |_WaitSet| + |_EntryList|        // 等待锁的线程会被封装成ObjectWaiter对象    protected:        void *  volatile _owner;                // 一个指针，指向当前拥有锁的线程        ObjectWaiter * volatile _WaitSet;       // 一个队列，保存着waiting状态的线程        ObjectWaiter * volatile _EntryList ;    // 一个队列，保存着因等待锁而被阻塞的线程&#125;\n\n当多个线程同时访问一段synchronized代码时，会发生这些操作：\n\n线程首先会进入_EntryList，在该线程获取到对象的monitor之后，_owner会指向这个线程，然后_count计数器加一。\n如果得到monitor的这个线程调用了wait()方法，那么这个线程将会释放掉monitor的所有权，_owner变量变回NULL，_count计数器也会减一，同时这个线程会进入_WaitSet等待被唤醒。\n如果这个线程执行完毕，那么它也将释放monitor，并复位_count的值，这样其他的线程也就可以获得monitor来加锁了。\n\n\n上一个线程释放掉monitor后，_EntryList中的线程就会开始争抢monitor，具体哪个线程能成功得到monitor是不确定的。\n\n而正因为Monitor对象存在于每个Java对象头的mark word中，所以每个Java对象都可以用作锁。\n参考文章\nsynchronized与对象的Monitor\nIntrinsic Locks and Synchronization\n啃碎并发（七）：深入分析Synchronized原理\nobjectMonitor.hpp - JetBrains&#x2F;jdk8u_hotspot\nWhy do we need to call ‘monitorexit’ instruction twice when we use ‘synchronized’ keyword? - StackOverflow\n\n","categories":["学知识"],"tags":["Java","面试问题","syncronized"]},{"title":"突然情绪爆发，暴哭了一场","url":"/i-cried-just-now.html","content":"刚才久违的暴哭了一场。\n嘛，其实也没暴走多久，满打满算，也就暴走了一个刘强东那么久。\n无奈，最近一段时间里，发生了太多闹心的事。\n有多闹心呢？你想想，能让一个网瘾少年连游戏都不想玩了，你就想这是有多闹心吧。\n已经不记得上次哭是什么时候了，甚至，已经忘了该怎么哭了。\n我说我连该怎么引导自己哭出来，都是查的知乎，你信吗。\n当然也不是没来由的就哭了，主要还是最近心力交瘁，最后因为一件屁事情绪爆发了。\n您要是想看个来龙去脉呢，那就继续往下，看我这个祥林嫂的絮叨吧。\n\n\n起初呢，是年中那会，经过哥们啊、领导们啊、当然还有我自己的努力，争取到了一个外派的机会\n嗯，这里插句嘴，其实现在我心里都抱着一点歉意的，因为当时领导也是有在挽留的，怎奈当时王八吃秤砣，铁了心的要出去看看这个大世界，最后就还是出来了。\n那既然外派出来了，那当然是想要好好的干啊。一方面呢，当时本来就是奔着这边有没玩过的技术，我是来学习的；另一方面呢，我毕竟是外派出来的，也挂着咱老东家的脸呢，出来混，丢啥不能丢老东家的脸呐。\n头先派我俩活，一个我忘了是啥了，另一个是给现有的一个系统设计并新增一个功能。\n这两次呢，也没跟我规定啥时候交货。我寻思着，按照这个难度，我设计啊编码啊给你做的精细点，一星期也差不多吧。\n结果周五验收的时候，负责人表示，我觉得你时间有点太长了。\n第一个活，负责人表示我快了一天就搞定，最慢也就三天，按你这效率我不如不要你了。\n第二个呢，负责人表示，这个应该四天就能解决的。\n这就不太对了吧。开始的时候也没有定一个deadline，可最后又说你不应该花这么多时间，这是什么操作啊？\n第二个活呢，我当时上交设计文档的时候，负责人表示没有什么要修改的，那么我就按照我自己的设计稿去做了。可到了交货的时候，却砍了一个功能。\n为啥我要把这个被砍了的功能拎出来说呢？因为那天上午，我就是在写这个功能。如果一开始就砍了不要，那我真的确确实实就只花了四天就完成了。\n当时想，算了算了，屁大点事，懒得反驳了，没劲。大不了我后面再证明我自己嘛。\n但是我错了。\n在上面发生的事情之后不久，我们开始从头设计和开发一个新的系统。而今天这次暴走，也是从这时候开始埋下了雷。\n在这个新项目开始不久，某一天，老东家那边的领导跟我讲，这边对我的效率好像不太满意，有一点不想再要我的意思。\n我一听，这不行啊。真就这么给我踹回去了，我无所谓，可是这多少会影响那哥们，和老东家的面子啊。\n你看啊，咱哥们，又是拼缝，又是联系，里外里没少折腾，最后就整进来这么个怂蛋，这要是后面咱哥们再想介绍谁过来，那他领导不也得犯嘀咕，他可别再给我整个怂蛋来啊。\n再看咱老东家，也是费劲巴拉的没少折腾了，结果这废物没干几天就让踹回去了，脸上也不好看啊。\n那咋整呢？废话，更玩命的干呗。豁出去命挣个脸皮呗。\n可事实整明，我还是 too young too naive 了。脸皮没那么好挣，但是精神却可以消耗的很快。\n从谈话之后，我就开始5*8的满负载工作。去卫生间，要快。喝水，要快。任何事，都不能影响我的进度。\n然后，有一天，就在临下班的时候，突然一阵头晕，我发现我的右眼的上半部分，看不见了。\n具体是什么症状呢？就像是一张损坏的图片。下半部分还是画面本来的样子。而上半部分，只剩下一片灰色。\n所幸，不是永久的。它只持续了不到一分钟。\n但是，我很害怕。我怕下次，就变成了一只眼睛半只瞎。我怕下次，就变成了两只眼睛一只瞎。\n当天挂了眼科急诊，第二天又挂了眼科门诊。好在经过各种检查，眼睛没有任何问题，诊断是一时性的供血不足。而且时间很短，没有对眼睛造成什么永久性的损伤。\n但是大夫后面说了一句话，让我不淡定了。\n“这种一过性的症状，通常不是眼睛的病变，而有可能是脑部的问题。”\n啥？老子年纪轻轻的，脑子就坏了？老子不答应！\n可我不答应没用啊，还是得拿诊断结果说话。再联系到那段时间总有右脑隐隐的偏头疼，所以也害怕是有肿瘤或者血栓在里面。\n于是就先后做了脑CT和MRI。\n又是各种请假。而且都这种情况了，我个废物还在考虑会不会拖团队的后腿。\n最后结果出来，也算是意料之中，检查都是正常。\n脑子里没有什么不该有的东西。血管也很通畅。死不掉，也瘫不了。悬下的心，算是放下了。\n既然没事了，那就继续干活吧。\n可是好景不长，没过一两天，右耳开始感觉耳鸣。\n一开始没管它，结果越是不管，声音越是大。正巧那两天虫子开始叫起来。我一时分不清是真的耳鸣，还是虫子太响。\n既然有疑惑，那就得去查。越拖着，疑心越重，没病都能拖出癔症。\n其实我是个很胆小的人。我害怕的，不是虫子骑脸，不是被疯子攮了。我怕的是，突然生大病，让远在半个中国以北的爹妈操碎心。\n看耳朵的时候，大夫倒是干脆，直接就说，耳屎太多了。开了瓶药，滴两天，把耳屎泡软了之后，找大夫给我取出来，就解决了。\n但是这事吧，它就不能平平稳稳的过去。\n取耳屎的前一天晚上，滴完了药之后，耳朵眼里开始觉得肿胀，而且，右耳几乎听不见了。\n我干哦！最后一天晚上都不让我好好过吗？而且急诊又没有耳鼻喉科！玩我呢吧！\n没办法求助万能的网络，结果是，耳屎彻底泡发了，涨起来堵住了耳道，造成传导性耳聋。\n哦。耳屎堵了啊。好吧睡觉。明天给孙子掏出来。\n第二天，也就是昨天，一大早心事重重的就醒了。赶了早班车，挂最早的号，去掏耳屎。\n中间发生了一个插曲。大夫掏耳屎用的是一个吸气的管子，来把耳屎吸出来。而这个管子，让我弄堵住了。是的耳屎太多把管子堵了。\n大夫都一脸很受不了的表情，说这玩意都给堵了。\n吸完右耳吸左耳。你以为吸完了就完了？右耳里面，还有块钉子户你敢信？\n我还得滴两天药水，再过来掏一次。\n然后一整天去体检、去退还光猫，这些杂事就按下不表。\n晚上回去之后，也不知道是心太累，还是天太热，只觉得心烦意乱。哥几个联机打游戏，我都没法专心的去玩，不知道思绪在哪，一团乱麻。\n想着，也到了吃饭的时候了吧，于是就去做饭。做饭的时候，也是心神不宁。\n菜出来了，吃了一口，咸，没法吃。\n这时候，突然情绪就爆发了。\n为什么？为什么就这么多破事？为什么工作也干不好，身体也养不好，就连菜都炒不好？\n当时就觉得想哭。但是，又哭不出来。\n就像前两天，上海一位住户，住着数百万的房子，却在台风天坏了马桶，蹲地痛哭。区别只是，我没哭出来。\n忍着恶心，扔掉饭菜，出门觅食。虽然当时一点胃口都没有，但还是强迫自己吃了一碗小馄饨。\n因为，任由自己的坏心情折腾自己，只会让自己离抑郁更近一步。老子，不答应。老子，要乐呵的活着。\n然后就是一觉醒来，到了今天5点。时间是睡饱了，可是精神还是没有恢复。\n早饭是逼着自己吃的。想出去走走，结果隔一分钟换一个目的地。\n而且，还是半憋着想哭。\n这不是个事，这样下去，解决不了。这样下去，老子要崩。\n果断掉头回住处。上网搜，“想哭哭不出来”。嗯，网络就是好，各路大神给支招。\n挑了个简单的，只需要三步。\n刚做完一步半，感觉就上来了。眼泪啊，kua的一下，就出来了。\n来的快，去的也快。感觉已经哭爽了，再也哭不出来之后，整理思绪，写下了这些絮絮叨叨的东西。\n果然啊，心情不好的时候，哭一场，就什么都过去了。\n谢谢您耐着性子，跟着这个胆小的家伙，回顾了一下这段波折的人生。\n我写这么多呢，就是为了发泄一下。您呢，就当看了一篇文笔拙劣的小说吧。\n$EOF.\n","categories":["生活记录"]},{"title":"解决命令行中Git显示中文文件名乱码","url":"/git-commandline-unescape-chinese-characters.html","content":"在用Git管理包含中文的文件时，会出现类似这样的“乱码”：\n$ git statusOn branch masterYour branch is up to date with &#x27;origin/master&#x27;.Untracked files:  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)    &quot;\\345\\271\\277\\345\\221\\212\\345\\220\\214\\346\\255\\245\\345\\271\\263\\345\\217\\260\\346\\216\\245\\345\\217\\243\\346\\226\\207\\346\\241\\2432.0.docx.new&quot;nothing added to commit but untracked files present (use &quot;git add&quot; to track)\n\n解决方法是配置Git的全局属性core.quotepath为false，即执行命令git config --global core.quotepath false，然后再执行git status，就可以看到中文正常显示了：\n$ git statusOn branch masterYour branch is up to date with &#x27;origin/master&#x27;.Untracked files:  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)    广告同步平台接口文档2.0.docx.newnothing added to commit but untracked files present (use &quot;git add&quot; to track)\n","categories":["小技巧"],"tags":["Git"]},{"title":"IDEA警告Field injection is not recommended","url":"/idea-warninig-field-injection-is-not-recommended.html","content":"前些天在开发过程中，发现IDEA在一个@Autowired注解上打了一个警告，内容是Field injection is not recommended。多年面向Spring开发的经验告诉我，使用@Autowired注解进行依赖注入，肯定是没有问题的。但是我的代码洁癖不允许我这么不明不白的留一个警告在这里。所以，带着我的洁癖，和我的好奇心，我开始研究起了这个警告。\n\n\n警告信息这个警告，和警告的处理建议，在IDEA中是这么写的：\n\n翻译过来是这个意思：\n\n不建议直接在字段上进行依赖注入。Spring开发团队建议：在Java Bean中永远使用构造方法进行依赖注入。对于必须的依赖，永远使用断言来确认。\n\n修改代码既然IDE给了警告，那就先着手修改。一开始，代码是这样子的：\npublic class AClass&#123;    @Autowired    private DependencyClass aDependency;&#125;\n\n根据提示，我将代码修改成了这样子：\npublic class AClass &#123;    private final DependencyClass aDependency;    public AClass(DependencyClass aDependency) &#123;        this.aDependency = aDependency;    &#125;&#125;\n\n然后警告就消失了，同时运行没有问题，说明这个修改是可行的。\n另外，如果你的项目中引入了Lombok，那么代码甚至可以精简成这样子：\n// 该注解指示Lombok为所有没被初始化过的final的变量创建构造方法@RequiredArgsConstructorpublic class AClass &#123;    private final DependencyClass aDependency;&#125;\n\n但是，光是改好代码还远远不够，我需要知道，为什么Spring团队会提出这一项要求，以及，直接使用@Autowired进行依赖注入有什么问题。\n依赖注入的类型经过我的了解，Spring有三种依赖注入的类型。\n基于field的注入所谓基于field的注入，就是在变量上使用@Autowired注解进行依赖注入。这是我们最熟悉的一种方式，同时，也正是Spring团队所不推荐的方式。它用起来就像这样：\n@Autowiredprivate DependencyClass aDependency;\n\n基于setter方法的注入通过setter()方法，以及在方法上加入@Autowired注解，来完成的依赖注入，就是基于setter方法的注入。它用起来就像这样：\nprivate DependencyClass aDependency;@Autowiredpublic void setADependency(DependencyClass aDependency) &#123;    this.aDependency = aDependency;&#125;\n\n注：在Spring 4.3及以后的版本中，setter上面的@Autowired注解是可以不写的。\n基于构造方法的注入将各个必需的依赖全部放在带有@Autowired注解构造方法的参数中，并在构造方法中完成对应变量的初始化，这种方式，就是基于构造方法的注入。它用起来就像这样：\npublic class AClass &#123;    // 这里 final 修饰符并不是必须的，但是我喜欢这么做    // 因为这样不仅可以在代码上防止 aDependency 被修改    // 在语义上也可以表明 aDependency 是不应该被修改的    private final DependencyClass aDependency;    @Autowired    public AClass(DependencyClass aDependency) &#123;        this.aDependency = aDependency;    &#125;&#125;\n\n注：在Spring 4.3及以后的版本中，如果这个类只有一个构造方法，那么这个构造方法上面也可以不写@Autowired注解。\n基于field的注入有什么问题基于field的注入，虽然不是绝对禁止使用，但是它可能会带来一些隐含的问题。比如，在这篇博客中，作者给出了这样的一个代码：\n@Autowiredprivate User user;private String school;public UserAccountServiceImpl()&#123;    this.school = user.getSchool();&#125;\n\n初看起来好像没有什么问题，User类会被作为一个依赖被注入到当前类中，同时这个类的school属性将在初始化时通过user.getSchool()方法来获得值。但是，这个代码在运行时，却会抛出如下的异常：\nException in thread &quot;main&quot; org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;...&#x27; defined in file [....class]: Instantiation of bean failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [...]: Constructor threw exception; nested exception is java.lang.NullPointerException\n\n即，在执行UserAccountServiceImpl()这个构造方法时出现了NPE。\n出现这个问题的原因是，Java在初始化一个类时，是按照静态变量或静态语句块 –&gt; 实例变量或初始化语句块 –&gt; 构造方法 -&gt; @Autowired的顺序[^1]，那么显而易见，在执行这个类的构造方法时，user对象尚未被注入，它的值还是null，从而产生了NPE。\n此外，在代码质量方面，因为基于field的注入用起来实在是太方便了，增加一个依赖只需要声明一个变量，然后给它加上@Autowired注解，就可以了。而这份便利，有可能会导致这个类的依赖变得越来越多，功能越来越杂，最终违反了单一功能原则。这虽然不会导致功能异常，但是这将增大后续维护的难度。（话虽然这么说，就算我用了基于构造方法的注入，但是用Lombok简化了构造方法，这么一来，增加一个依赖又变得更方便了，只需要加一行变量声明就行，如果在不注重代码质量的时候，这也会加剧类的膨胀。所以最后还是得靠工具和审查流程，以及开发者的自觉，来保证代码质量……）\n还有一点我个人的感受，就是基于field的注解会占据过多的屏幕空间。按照我个人的代码习惯，每个注入之间都要插入一行空行，来把它们分割开来。这意味着，每个注入都将占据3行。如果这个类有过多的依赖，那么很有可能光是依赖注入的部分，就会占据大半个屏幕，这会让我看起来很不舒服。当然，出现这种情况，可能同时也意味着这个类已经过于膨胀，违反单一功能原则了。\n基于setter的注入和基于构造方法的注入该怎么选择，有什么优点对于两种注入方式的取舍，Spring开发团队提供了他们的意见[^3]：\n\nSince you can mix constructor-based and setter-based DI, it is a good rule of thumb to use constructors for mandatory dependencies and setter methods or configuration methods for optional dependencies.\n\n简而言之，对于必需的依赖，使用基于构造方法的注入；对于可选的依赖，使用基于setter的注入。\n同时Spring开发团队也讲明了两种注入方式的优点。对于基于构造方法的注入，Spring团队是这么说的：\n\nThe Spring team generally advocates constructor injection as it enables one to implement application components as immutable objects and to ensure that required dependencies are not null. Furthermore constructor-injected components are always returned to client (calling) code in a fully initialized state. As a side note, a large number of constructor arguments is a bad code smell, implying that the class likely has too many responsibilities and should be refactored to better address proper separation of concerns.Spring团队提倡使用基于构造方法的注入，因为这样一方面可以将依赖注入到一个不可变的变量中(注：final修饰的变量)，另一方面也可以保证这些变量的值不会是null。此外，经过构造方法完成依赖注入的组件(注：比如各个service)，在被调用时可以保证它们都完全准备好了。与此同时，从代码质量的角度来看，一个巨大的构造方法通常代表着出现了代码异味，这个类可能承担了过多的责任。\n\n而对于基于setter的注入，他们是这么说的：\n\nSetter injection should primarily only be used for optional dependencies that can be assigned reasonable default values within the class. Otherwise, not-null checks must be performed everywhere the code uses the dependency. One benefit of setter injection is that setter methods make objects of that class amenable to reconfiguration or re-injection later.基于setter的注入，则只应该被用于注入非必需的依赖，同时在类中应该对这个依赖提供一个合理的默认值。如果使用setter注入必需的依赖，那么将会有过多的null检查充斥在代码中。使用setter注入的一个优点是，这个依赖可以很方便的被改变或者重新注入。\n\n写在最后虽然上面洋洋洒洒写(chao)了那么多，又是分析优劣，又是分析场景的，但是按照我现在仅有的开发经验来看，好像怎么注入区别都不大(除了setter注入，这个我没用过)，要说我为什么一定要用构造方法注入，最大的原因其实就是为了去掉那个警告……\n也有人说，都这么写习惯了，又没出啥问题，你把这个警告关了不就行了吗？我的回答是：\n\n[^1]: Spring Bean: Is autowired attribute initialised before constructor?[^2]: Field Dependency Injection Considered Harmful[^3]: Setter-based dependency injection\n","categories":["学知识"],"tags":["Java","Spring"]},{"title":"在Spring Boot应用中配置统一的请求响应","url":"/generify-response-format-in-spring-boot-applications.html","content":"在前后端分离的架构下，后端通常是一个RESTFul的接口，而因为HTTP的响应码数量有限，无法灵活的反映出接口执行的各种结果，在这种情况下，就需要通过自定义的结构来表达接口最终的状态和返回的信息。而我正好最近在一个项目中实现了一个基于ControllerAdvice的统一请求响应的功能，在这里记录一下实现的方式。\n\n\n创建common模块因为这是一个公共的功能，所以需要创建一个新的Maven模块，并被所有项目引用为依赖。具体操作这里不再赘述。以下的所有代码，如无特殊说明，都将存在于这个common模块中。\n定义全局的错误码首先我们需要定义一个全局的错误码，使得项目中的所有模块都可以使用统一的一套返回码来表达自己接口的状态。\n/** * 接口返回码和描述 * * @author Boris Zhao */@Getterpublic enum ReturnCode &#123;    /**     * 成功     */    OK(&quot;0000&quot;, &quot;成功&quot;),    /**     * 服务端异常，当发生未知异常时使用该错误码     */    FAIL(&quot;9999&quot;, &quot;失败&quot;),    /**     * 请求参数中包含无效参数或请求体为空     */    INVALID_REQUEST_PARAM(&quot;0001&quot;, &quot;请求参数中包含无效参数或请求体为空&quot;),    /**     * 新数据的主键与已有数据重复     */    DUPLICATED_RECORD(&quot;0002&quot;, &quot;新数据的主键与已有数据重复&quot;),    /**     * 未找到对应记录     */    NON_EXISTENT_RECORD(&quot;0003&quot;, &quot;未找到对应记录，请检查主键或操作流水号&quot;),    /**     * 签名校验失败     */    SIGNATURE_VERIFICATION_FAIL(&quot;0004&quot;, &quot;签名校验失败&quot;),    // 以下为各模块自定义的错误码    ;    private String code;    private String message;    ReturnCode(final String code, final String message) &#123;        this.code = code;        this.message = message;    &#125;    /**     * 根据状态码获取其错误信息     *     * @param code 状态码     * @return 错误码对应的错误信息。如果没有找到则返回&#123;@code null&#125;     */    public static String getMessageByCode(String code) &#123;        for (ReturnCode item : values()) &#123;            if (item.code.equals(code)) &#123;                return item.message;            &#125;        &#125;        return null;    &#125;&#125;\n\n定义统一响应结构在这个项目中，我选择在这个结构中定义三个字段：错误码errCode，错误信息errMessage，和返回的数据data。\n同时，用于构造响应体的类应该同时兼顾数据合法性和灵活性，所以我决定不允许通过构造方法或者setter来填充信息，而是使用定义好了的静态方法来完成构造。\n/** * 公共响应参数&lt;br&gt; * 成功的返回通过&#123;@link CommonResponseParams#ofSuccessful()&#125;或&#123;@link CommonResponseParams#ofSuccessful(Object)&#125;生成&lt;br&gt; * 失败的返回通过&#123;@link CommonResponseParams#ofFailure()&#125;或&#123;@link CommonResponseParams#ofFailure(ReturnCode)&#125;生成 * * @author Boris Zhao */@Datapublic class CommonResponseParams &#123;    /**     * 返回码 - 必填     */    private String errCode;    /**     * 返回描述 - 必填     */    private String errMessage;    /**     * 业务数据 - 必填     */    private Object data;    /**     * 构造一个&#123;@link CommonResponseParams&#125;对象     *     * @param errCode    返回码     * @param errMessage 返回描述     * @param data    业务数据     */    private CommonResponseParams(final String errCode, final String errMessage, final Object data) &#123;        this.errCode = errCode;        this.errMessage = errMessage;        this.data = data;    &#125;    /**     * 返回成功结果，没有响应数据     *     * @return 公共响应参数实体     */    public static CommonResponseParams ofSuccessful() &#123;        return ofSuccessful(null);    &#125;    /**     * 返回成功结果     *     * @param content 返回的数据     * @param &lt;T&gt;     返回的数据的类型     * @return 公共响应参数实体     */    public static &lt;T&gt; CommonResponseParams ofSuccessful(final T content) &#123;        return new CommonResponseParams(                ReturnCode.OK.getCode(),                ReturnCode.OK.getMessage(),                JSONArray.toJSON(content));    &#125;    /**     * 返回失败结果     *     * @return 公共响应参数实体     */    public static CommonResponseParams ofFailure() &#123;        return new CommonResponseParams(                ReturnCode.FAIL.getCode(),                ReturnCode.FAIL.getMessage(),                null);    &#125;    public static CommonResponseParams ofFailure(String errMessage) &#123;        return new CommonResponseParams(                ReturnCode.FAIL.getCode(),                errMessage,                null);    &#125;    /**     * 返回失败结果     *     * @param returnCode 错误的返回码     * @return 公共响应参数实体     */    public static CommonResponseParams ofFailure(ReturnCode returnCode) &#123;        return new CommonResponseParams(                returnCode.getCode(),                returnCode.getMessage(),                null);    &#125;    /**     * 返回带有自定义错误信息的失败结果     *     * @param returnCode 错误相关的返回码     * @param errMessage 自定义的错误信息     * @return 公共响应参数实体     */    public static CommonResponseParams ofFailure(ReturnCode returnCode, String errMessage) &#123;        return new CommonResponseParams(                returnCode.getCode(),                errMessage,                null);    &#125;&#125;\n\n定义统一的业务异常基类为了减少不必要的try-catch模版代码，业务异常必须不能为受检异常；而为了与其它的运行时异常区分开来，业务异常类就不能直接继承RuntimeException，而是需要继承于一个自定义的基类。同时，这个业务异常基类不能被直接使用，所以必须是一个抽象类。\n/** * 业务异常基类 * * @author Boris Zhao * @date 2019-12-13 */@Getterpublic abstract class BaseBizException extends RuntimeException &#123;    protected ReturnCode returnCode = null;    public BaseBizException(String message) &#123;        super(message);    &#125;    public BaseBizException(ReturnCode returnCode) &#123;        super(returnCode.getMessage());        this.returnCode = returnCode;    &#125;    /**     * 业务异常不记录stack trace     */    @Override    public synchronized Throwable fillInStackTrace() &#123;        return this;    &#125;&#125;\n\n定义统一的异常处理方法在上面的准备工作全部完成后，就可以开始着手配置统一的异常处理方法。之所以选择不使用AOP实现，是因为在这个情况下，业务接口必须返回Object类型，而这样一来，会降低代码层面的可读性。使用ControllerAdvice注解实现则没有这个限制，业务接口可以自由选择自己合适的数据类型。\n需要注意的是，因为我们所有的controller类都会带有RestController注解，所以在ControllerAdvice注解中，我们使用annotations参数指定了这个配置类仅针对带有RestController的类启用。\n/** * 统一异常处理配置类&lt;br&gt; * 包装格式见&#123;@link CommonResponseParams&#125; * * @author Boris Zhao * @date 2019-12-13 */@Slf4j@ResponseBody@ControllerAdvice(annotations = RestController.class)public class UnifiedExceptionHandler &#123;    /**     * 处理数据库连接失败抛出的异常     *     * @return 带有数据库连接失败信息的失败返回     */    @ExceptionHandler(CannotCreateTransactionException.class)    public CommonResponseParams handleCannotCreateTransactionException(CannotCreateTransactionException e) &#123;        log.error(e.getMessage(), e);        return CommonResponseParams.ofFailure(&quot;数据库连接失败&quot;);    &#125;    /**     * 处理未知的运行时错误     *     * @return 默认的失败返回     */    @ExceptionHandler(RuntimeException.class)    public CommonResponseParams handleUnknownRuntimeExceptions(RuntimeException e) &#123;        log.error(e.getMessage(), e);        return CommonResponseParams.ofFailure(e.getMessage());    &#125;    /**     * 处理公共请求参数校验失败异常     *     * @param e 参数校验失败抛出的异常     * @return 带有校验失败原因提示信息的失败返回     */    @ExceptionHandler(MethodArgumentNotValidException.class)    public CommonResponseParams handleRequestParamValidationExceptions(MethodArgumentNotValidException e) &#123;        String errMessage = Optional.ofNullable(e.getBindingResult().getFieldError())                .map(FieldError::getDefaultMessage)                .orElse(ReturnCode.INVALID_REQUEST_PARAM.getMessage());        log.error(e.getMessage());        return CommonResponseParams.ofFailure(ReturnCode.INVALID_REQUEST_PARAM, errMessage);    &#125;    /**     * 处理请求body为空的异常     *     * @return 带有请求体无效错误的失败返回     */    @ExceptionHandler(HttpMessageNotReadableException.class)    public CommonResponseParams handleHttpMessageNotReadableException() &#123;        return CommonResponseParams.ofFailure(ReturnCode.INVALID_REQUEST_PARAM);    &#125;    /**     * 处理新增数据主键重复异常     *     * @return 带有主键重复错误的失败返回     */    @ExceptionHandler(DuplicateKeyException.class)    public CommonResponseParams handleDuplicateKeyException() &#123;        return CommonResponseParams.ofFailure(ReturnCode.DUPLICATED_RECORD);    &#125;    /**     * 处理业务异常     *     * @return 业务异常对应的失败返回     */    @ExceptionHandler(BaseBizException.class)    public CommonResponseParams handleBizExceptions(BaseBizException e) &#123;        if (e.getReturnCode() != null) &#123;            ReturnCode returnCode = e.getReturnCode();            log.error(returnCode.getMessage());            return CommonResponseParams.ofFailure(returnCode);        &#125; else if (StringUtils.isNotBlank(e.getMessage())) &#123;            log.error(e.getMessage());            return CommonResponseParams.ofFailure(e.getMessage());        &#125; else &#123;            log.error(e.getMessage());            return CommonResponseParams.ofFailure();        &#125;    &#125;&#125;\n\n这里详细说一下各个方法的作用。\n第一个方法用于处理CannotCreateTransactionException异常类，这个异常会在应用无法成功连接数据库时被抛出。处理方式就是返回一个错误信息为“数据库连接失败”的失败结果。\n第二个方法用于处理RuntimeException异常，这个方法的意义在于，我们无法预见所有可能出现的异常，所以使用这个方法作为一个兜底的处理方法。\n第三个方法用于处理MethodArgumentNotValidException异常。因为这个项目中我们选择使用javax.validation.constraints包中的注解实现输入参数合法性的校验，而当校验失败时会抛出MethodArgumentNotValidException异常，并且在异常中会包含具体的校验失败的原因。同时为了保证方法的健壮性，在代码中也保证了如果无法获取到校验失败信息，就会选择INVALID_REQUEST_PARAM这个错误码作为兜底的错误信息。\n第四个方法用于处理HttpMessageNotReadableException异常。如果一个接口方法的参数中存在被@RequestBody标记的参数，但是在请求该接口时body为空时，就会抛出这个异常。在出现了这个异常后，就会返回带有INVALID_REQUEST_PARAM错误信息的失败结果。\n第五个方法用于处理DuplicateKeyException异常。因为这个项目中一部分数据的主键是由请求发起方生成的，同时数据库中也会将这一列定为主键来实现插入接口的幂等性。一旦出现网络状况不佳的情况时，发起方会尝试再次调用接口。而在重发请求时，可能数据已经在上一个请求中就已经成功插入了，只是因为网络不佳导致发起方没能接收到返回，在第二次请求中重复插入相同主键的数据，就会抛出这个异常。为了最终接口返回信息的可读性，我们选择在这里返回一个用户友好的信息。\n最后一个方法就是这里的主角了，它用于处理所有继承了BaseBizException的业务异常。这个方法中，我们对应着CommonResponseParams中不同的静态方法，实现了对应的错误处理逻辑。\n定义统一的成功响应处理方法上面洋洋洒洒写了一堆针对异常的处理逻辑，但是接口成功执行的处理逻辑也不能落下。这里我们使用RestControllerAdvice表示这是一个接口增强类，同时实现了ResponseBodyAdvice接口，用于实现实际的处理逻辑。\n在这个配置类上，我们也指定了该配置类仅针对被RestController标记的类生效。\n/** * 统一响应格式配置类&lt;br&gt; * 包装格式见&#123;@link CommonResponseParams&#125; * * @author Boris Zhao * @date 2019-12-13 */@EnableWebMvc@Configuration@RestControllerAdvice(annotations = RestController.class)public class UnifiedReturnConfig implements ResponseBodyAdvice&lt;Object&gt; &#123;    @Override    public boolean supports(MethodParameter returnType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) &#123;        return true;    &#125;    @Override    public Object beforeBodyWrite(Object body,                                  MethodParameter returnType,                                  MediaType selectedContentType,                                  Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType,                                  ServerHttpRequest request,                                  ServerHttpResponse response) &#123;        if (body instanceof CommonResponseParams) &#123;            return body;        &#125;        return CommonResponseParams.ofSuccessful(body);    &#125;&#125;\n\n上面代码的重点是在beforeBodyWrite方法中。这个方法会在HttpMessageConverter#write()方法执行前，也就是返回被发出去之前被调用。借助这个功能，我们就可以实现在业务接口返回之后，将返回信息重新包装。\n实现逻辑很简单，如果返回信息是一个CommonResponseParams对象，那么就认为这个返回信息已经被包装好了，所以不再进行二次包装，直接返回；否则就通过CommonResponseParams#ofSuccessful()方法，将返回信息包装为一个成功响应的格式，再返回到客户端。\n最后的一点配置在上文中，统一返回格式的配置已经完成了。但是有的人可能会发现，虽然在自己的项目中引用了这个模块，但是实际上却没有生效，这是因为上面的配置类都存在于另一个jar包中，导致在应用启动时这些请求并没有被自动发现。解决方法也很简单，在项目的启动类(即xxxApplication)中加上@ComponentScan注解，并在注解参数中加上UnifiedReturnConfig和UnifiedExceptionHandler所在的包名即可。\n","categories":["小技巧"],"tags":["Java","Spring Boot","统一响应"]},{"title":"在NexT主题中接入Google AdSense","url":"/hexo-next-enable-google-adsense.html","content":"这个博客从建站到现在，也差不多有两三年时间了，一直以来也都是一种“用爱发电”的状况，看到其他博主都开始通过接入广告联盟来挣零花钱，说不心动那是不可能的。所以就研究了一下怎么在这个博客里面接入Google AdSense。\n在使用本文介绍的方法前，请检查您的NexT主题版本是否为v6.0.2或后续版本，因为本文涉及的功能是在v6.0.2引入的。\n\n\n虽然网上已经有很多类似的博文了，我为什么还要再写一篇呢？主要是因为，很多文章在添加认证代码的时候，都是直接修改themes/next/layout/_partials/head.swig或者themes/next/layout/_custom/google_adsense.ejs等位置，说白了就是在NexT主题所在的目录动手了，而我并不喜欢这种侵入性强的操作。经过一点简单的搜索，我发现是可以通过数据文件来实现非侵入性的修改的，所以就想讲一下我的做法。当然借此机会还能水一篇博文何乐而不为 &#x2F;滑稽\n那么言归正传，一起来看一下我是怎么配置的吧。\n注册Google AdSense要接入AdSense，那当然要有一个AdSense的账户。进入Google AdSense主页，点击“开始使用”，跟随页面提示填好各项信息，这部分操作很简单，就不多废话了。\n但是这里可能有一个坑，就是我只能接入一个裸域名，不知道是我的问题还是AdSense就是这么要求的。我的博客地址是https://www.boris1993.com，但是在页面上输入网站地址之后，AdSense会把www这个二级域名去掉，变成裸域名https://boris1993.com，所以我只好把裸域名也关联到了我的博客站上。你们在配置的时候，记得注意一下这个地方。如果你知道具体的原因，或者解决方案，那么也请不吝赐教，在留言区告诉我。\n添加AdSense代码在注册完成后，AdSense会给你一串代码，并要求你把这串代码放在&lt;head&gt;&lt;/head&gt;标记中。注意这里就是我跟他们不一样的地方了。\n同时再次提醒确认你的NexT主题的版本是不是v6.0.2或后续版本。如果不是的话，你要么考虑升级，要么可以点❌关闭了。\n首先，我们需要在主题配置文件中指定自定义layout文件的位置。当然，这步也是在数据文件中操作的。什么？你还没有用数据文件这个功能？那你赶紧学习并且用起来吧，省的每次更新主题还要担心丢配置。\n在source/_data/next.yml中增加如下配置：\n# 配置的原文可以在themes/next/_config.yml的24行找到(针对v7.5.0)custom_file_path:  # 因为我们只需要自定义head部分，所以就只配置head这一个属性，其他都保持默认  head: source/_data/head.swig\n\n指定好head.swig的位置之后，我们继续在source/_data中创建head.swig文件，并把AdSense给我们的那串代码放在head.swig里面，就像这样：\n&lt;script data-ad-client=&quot;██████████&quot; async src=&quot;https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&quot;&gt;&lt;/script&gt;\n\n然后可以重新启动一下本地的Hexo环境，检查&lt;head&gt;&lt;/head&gt;标记中有没有出现刚才添加的代码。检查没有问题之后，就可以部署到生产环境使上面的修改生效了。为了保险起见，在生产环境上也要检查一下&lt;head&gt;&lt;/head&gt;标记中有没有出现刚才添加的代码。确认没有问题之后，就可以回到AdSense页面中点击验证按钮了。\n注意，如果AdSense告诉你检查不通过，你一定不要频繁点击再次检查，一定要先确定检查不通过的原因，并在排除问题之后再重新检查，因为每天检查次数是有限制的，如果超过限制，就只能等24小时之后再检查了（别问我是怎么知道的。\n然后AdSense会开始审核阶段，这期间我们静静等待审核结果的邮件就好了。\n因为在写这篇博文的时候，我也在等AdSense审核，所以这篇文章暂且先告一段落，等有了结果我再更新后面的内容。\n等了这么久，终于收到Google的邮件说我通过了审核，可以开始投放广告了。那么，我们也可以开始继续完成后面的工作了。\n配置广告模块进入AdSense管理后台之后，进入侧边栏的广告 -&gt; 概览。在这里我们可以配置广告会在什么位置展示了。\n因为我并不希望让我的网站满地都是广告，而影响正常内容的展示，而是，只在首页左侧边栏最下面，和在每篇博文结束的位置，各放置一个广告。所以我就需要到按广告单元这个页面手动配置两个广告模块。在这个截图里我们可以看到已经有了两个广告模块，那是因为我已经配置好了。\n\n添加广告模块点击展示广告按钮来新建一个类型为展示的广告。在顶部文本框填入广告模块的名字，比如side ad，然后在右边栏选择广告的尺寸，是自适应的，还是固定长宽的。如果是自适应尺寸，那么还需要选择广告的大致形状。配置完毕后，点击保存按钮，一个广告模块就配置好了。\n\n按照同样的操作，继续添加一个名为bottom ad的展示广告。这样，我们就添加好了两个广告模块。\n配置博客页面广告模块是创建好了，但是我们还需要配置广告展示的位置。这里我依旧使用数据文件功能，来实现非侵入性的修改。\n首先，在source/_data/目录新建两个文件，分别命名为sidebar.swig和post-body-end.swig，分别用来存放侧边栏广告和文章底部广告的代码。\n还记得刚才添加完广告模块之后弹出的那一片代码吗？把它们原样默写到这两个文件里就好啦。开玩笑的，谁记得住那一大堆东西啊。回到按广告单元这个页面，在刚才添加的广告单元那里，有一个形似&lt;&gt;的按钮，点击它，就可以看到这个广告单元的配置代码了。\n把side ad的代码复制到sidebar.swig中，来把这个侧边栏广告真正的放到侧边栏上。比如我的广告代码是这样的：\n&lt;!-- side ad --&gt;&lt;ins class=&quot;adsbygoogle&quot;     style=&quot;display:block&quot;     data-ad-client=&quot;████████████████████&quot;     data-ad-slot=&quot;██████████&quot;     data-ad-format=&quot;auto&quot;     data-full-width-responsive=&quot;true&quot;&gt;&lt;/ins&gt;&lt;script&gt;     // 因为直接执行AdSense的代码会出现 No slot size for availableWidth=0 的错误     // 并且这个广告位会无法正常展示     // 所以加一个 event listener，在页面加载完毕之后再执行     // 参考：https://stackoverflow.com/a/37822404/3833858     window.addEventListener(&#x27;load&#x27;, function() &#123;          (adsbygoogle = window.adsbygoogle || []).push(&#123;&#125;);     &#125;);&lt;/script&gt;\n\n然后把bottom ad的代码复制到post-body-end.swig，来把这个广告放到文章底部。\n&lt;!-- bottom ad --&gt;&lt;ins class=&quot;adsbygoogle&quot;     style=&quot;display:block&quot;     data-ad-client=&quot;████████████████████&quot;     data-ad-slot=&quot;██████████&quot;     data-ad-format=&quot;auto&quot;     data-full-width-responsive=&quot;true&quot;&gt;&lt;/ins&gt;&lt;script&gt;     (adsbygoogle = window.adsbygoogle || []).push(&#123;&#125;);&lt;/script&gt;\n\n接下来继续修改source/_data/next.yml中custom_file_path的配置：\ncustom_file_path:  head: source/_data/head.swig  # 加入下面两行  sidebar: source/_data/sidebar.swig # 侧边栏广告  postBodyEnd: source/_data/post-body-end.swig # 博文末尾广告\n\n这样，广告就放置好了。接下来，把博客发布上去，就可以开始打广告了。\n","categories":["瞎折腾"],"tags":["hexo","NexT","adsense"]},{"title":"JDBC错误Illegal mix of collations解决方法","url":"/illegal-mix-of-collations.html","content":"最近项目开发时遇到了一个错误，内容是Illegal mix of collations (utf8_unicode_ci,IMPLICIT) and (utf8_general_ci,IMPLICIT) for operation &#39;=&#39;经查是关联查询时两个表的排序规则(collation)不同导致的。\n\n\n解决方法也很简单，有两种方法：\n一种是在查询的SQL中指定另一张表的collation，这是一种权宜之计，治标不治本。代码是这样的：\n-- 假设左表的排序规则是 utf8_general_ci,-- 而右表的排序规则是 utf8_unicode_ciSELECT    `a`.`column1`,    `b`.`column2`FROM `table_1` AS `a`LEFT JOIN `table_2` AS `b`    ON `a`.`id` = `b`.`id`    COLLATE `utf8_general_ci`\n\n另一种方法是修改表和列的排序规则，将其统一。这个方法从根本上解决了这个问题。操作方法可以参考在 MySQL 中修改表和列的排序规则\n。\n","categories":["学知识"],"tags":["Java","JDBC"]},{"title":"搭建ELK日志平台 - 安装Elastic Search","url":"/install-elk-part-es.html","content":"最近搭建了一次ELK日志平台，在此记录一下安装步骤。由于本次模拟的是服务器不能连接互联网的情况，所以全部安装步骤皆使用RPM或tar包的方式安装。本文主要记录安装Elastic Search的过程。\n\n\n安装JRE首先这套平台是基于Java的，所以Java运行环境当然是不能少。但因为这上面不涉及Java的开发，所以不需要装JDK，装JRE就够了，还能省下一些磁盘空间。我这里选择JRE8u161。\n我这次选择使用RPM包安装。\nsudo rpm -ivh jre-8u161-linux-x64.rpm\n\n安装完毕后，验证安装是否成功：\n# 检验当前用户下是否安装成功java -version# 检验sudo环境下是否安装成功sudo java -version\n\n若都输出如下内容则说明安装成功：\njava version &quot;1.8.0_161&quot;Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)\n\n至此Java环境配置完成\n安装Elastic Search安装过程使用RPM包安装直接使用rpm命令安装该RPM包\nsudo rpm --install elasticsearch-6.2.2.rpm\n\nCentOS 7使用systemd管理开机自启动项，而且安装过程已经配置好针对systemd的启动脚本，使用如下命令激活\nsudo systemctl daemon-reloadsudo systemctl enable elasticsearch.service\n\n使用tar.gz包安装首先新建一个名为elk的用户，用于运行ELK平台\nuseradd -m elk\n\n下载好Elastic Search的安装包，将其复制到&#x2F;opt并解压，然后试运行\nsudo cp elasticsearch-6.1.3.tar.gz /optcd /optsudo tar xvzf elasticsearch-6.1.3.tar.gz# 需要将Elastic Search目录的所有权设为将要运行该软件的用户# Elastic Search不允许以root用户运行，安全方面亦不建议以root权限运行程序sudo chown -R elk:elk elasticsearch-6.1.3cd elasticsearch-6.1.3/bin./elelasticsearch\n\n启动成功后，在另一终端使用curl尝试连接Elastic Search\ncurl http://127.0.0.1:9200\n\n若有如下返回，则说明Elastic Search启动成功\n&#123;  &quot;name&quot; : &quot;LWmSd17&quot;,  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,  &quot;cluster_uuid&quot; : &quot;lkbXufQpQuiLaE5kzVKAeA&quot;,  &quot;version&quot; : &#123;    &quot;number&quot; : &quot;6.1.3&quot;,    &quot;build_hash&quot; : &quot;af51318&quot;,    &quot;build_date&quot; : &quot;2018-01-26T18:22:55.523Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;7.1.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;  &#125;,  &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;\n\n安装后的配置系统配置文件修改Elastic Search需要调整文件描述符大于65535、最大线程数大于4096、以及vm.max_map_count大于262144。所以修改操作系统配置文件以满足此要求。\n在/etc/security/limits.conf插入如下内容\n* hard nofile 65536* soft nofile 65536* hard nproc  4096* soft nproc  4096\n\n在/etc/sysctl.conf中插入如下内容\nvm.max_map_count=262144\n\n然后执行sysctl -p，并重新登录，使配置生效。若配置成功，则可见Elastic Search启动过程中相关的警告信息将不再出现。\nElastic Search配置文件修改以下文件位置根据安装方法不同而不同若使用RPM包方式安装，则文件位于&#x2F;etc&#x2F;elasticsearch若使用tar包方式安装，则文件位于解压出来的目录的conf文件夹中\n\n修改cluster.name我们应当将集群名设置成一个能清晰地表明该集群的作用的名字，如logging-prod。\n\n修改node.name为每个Elastic Search节点起一个清晰易懂的名字绝不会是一件坏事。节点名字可以是一个自定义的名字，如prod-data-2，也可以使用${HOSTNAME}来把本机的主机名作为该节点的节点名。\n\n其他详细配置要想了解更多配置，可以参考Elasticsearch Reference\n的Set up Elasticsearch部分。\n\n\n配置自动启动如果使用RPM包方式安装，则此步可忽略。\n若使用tar包方式安装，则进入Elastic Search的bin目录后运行\n./elasticsearch -d -p ../logs/elasticsearch.pid\n\n使Elastic Search以daemon模式启动并监控启动过程。\n参考文献\nElastic Search Installation Guide\n\n","categories":["瞎折腾"],"tags":["ELK","Elastic Search"]},{"title":"搭建ELK日志平台 - 安装Kibana","url":"/install-elk-part-kibana.html","content":"上一次我们安装好了Elastic Search和Logstash，本次我们继续安装Kibana。\n\n\n安装KibanaKibana也提供了RPM安装包，所以还是一样的套路：\nsudo rpm -ivh kibana-6.2.2-x86_64.rpm\n\n配置Kibana及防火墙编辑/etc/kibana.yml\n这里比较关键的一点，是要指定Elastic Search的位置。如果Elastic Search是安装在本机，并监听默认的9200端口的话，则不需要修改该配置。\n# The URL of the Elasticsearch instance to use for all your queries.#elasticsearch.url: &quot;http://localhost:9200&quot;\n\n另外Kibana默认仅能从本机访问，若要开放给局域网，还需要修改Kibana监听的地址和端口号，并配置防火墙允许该端口通信：\n# Kibana is served by a back end server. This setting specifies the port to use.#server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &#x27;localhost&#x27;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.# 如果要绑定到特定的某一块网卡，那么就将这里的地址设为那块网卡的IP地址server.host: &quot;0.0.0.0&quot;\n\nCent OS 7使用firewalld管理防火墙，所以使用如下命令开放Kibana的端口：\nsudo firewall-cmd --zone=public --add-port=5601/tcp --permanentsudo firewall-cmd --reload\n\n启动Kibana我们这里同样使用systemd来管理Kibana的起停和自启动。\nsudo systemctl enable kibanasudo systemctl start kibana\n\n然后即可使用浏览器访问Kibana\n配置index patternKibana启动后，会要求配置索引，根据提示步骤配置即可。配置过程结束后，可到Discover页检查是否读到数据。\n参考文档Kibana User Guide\n","categories":["瞎折腾"],"tags":["ELK","Kibana"]},{"title":"Java 8的一个SNI的bug","url":"/java-8-sni-bug.html","content":"联动上一篇博文，在解决了OpenResty上那个SNI的问题之后，我们发现有一个Java应用也有类似的问题。而最后发现，这是因为我们当前版本的Java中有一个bug……\n简单来说，就是在Java 1.8u141之前，HttpsURLConnection#setDefaultHostnameVerifier()方法会破坏SNI，而正好我们的代码里有这么一行：\nHttpsURLConnection.setDefaultHostnameVerifier((hostname, session) -&gt; true);\n\n翻了下框架的代码，发现如果我不设定这个值，框架也调用这个方法来指定它默认的hostname verifier；而如果我传个null，那么它会抛个异常给我。所以，代码层面没有很方便的解决方案。\n无奈，最后决定，先临时换了个没开启SNI的域名，暂时解决掉问题，然后升级Java到1.8u181，一劳永逸。\n\n终于，我也有机会理直气壮地喊出“这是Java的bug”了！（笑\n[^1]: Extended server_name (SNI Extension) not sent with jdk1.8.0 but send with jdk1.7.0[^2]: Custom HostnameVerifier disables SNI support on client in Java 8\n","categories":["学知识"],"tags":["Java 8","bug","SNI"]},{"title":"搭建ELK日志平台 - 安装Logstash","url":"/install-elk-part-logstash.html","content":"上回书说道，我们已经安装好了Elastic Search。那么这次，我们继续安装Logstash。\n\n\n安装Logstash为了安装方便，本次依旧选择使用RPM包安装。\nsudo rpm -ivh logstash-6.2.2.rpm\n\n安装结束后，运行Logstash以检查安装是否成功。使用如下命令启动Logstash，并配置输入源为基本输入(stdin)，以及输出到基本输出(stdout)：\n# 因为使用RPM方式安装，导致/usr/share/logstash/data仅root才可写入，所以需要使用sudo环境sudo /usr/share/logstash/bin/logstash -e &#x27;input&#123; stdin&#123;&#125; &#125; output&#123; stdout&#123;&#125; &#125;&#x27;\n\n在日志滚动停止后，随意输入一些字符串，比如”hello world”，并回车，检查输出：\nhello world2018-02-26T07:18:07.904Z localhost.localdomain hello world\n\n可见Logstash成功从stdin读取到了输入，并打印到了stdout，证实安装成功。\n配置LogstashLogstash系统配置编辑/etc/logstash/logstash.yml，修改Logstash系统级配置。\n# 配置节点名，若未配置则默认取本机主机名作为节点名node.name: elk-logstash-node-0\n\n其他配置项略，如有需要请参考Logstash Reference。\n日志输入输出配置这里我们配置让Logstash接收Cent OS的系统日志。\ninput &#123;  file &#123;    path =&gt; &quot;/var/log/messages*&quot;    type =&gt; &quot;syslog&quot;  &#125;&#125;# Filter not needed. Commented out.#filter &#123;##&#125;output &#123;  elasticsearch &#123;    hosts =&gt; &quot;localhost:9200&quot;  &#125;&#125;\n\n另外，本例中还需要配置Logstash以root权限运行以读取系统日志(messages文件默认权限为600)，实际使用时需要按照实际需求配置。\n编辑/etc/systemd/system/logstash.service，修改user和group为root\n[Unit]Description=logstash[Service]Type=simpleUser=rootGroup=root# Load env vars from /etc/default/ and /etc/sysconfig/ if they exist.# Prefixing the path with &#x27;-&#x27; makes it try to load, but if the file doesn&#x27;t# exist, it continues onward.EnvironmentFile=-/etc/default/logstashEnvironmentFile=-/etc/sysconfig/logstashExecStart=/usr/share/logstash/bin/logstash &quot;--path.settings&quot; &quot;/etc/logstash&quot;Restart=alwaysWorkingDirectory=/Nice=19LimitNOFILE=16384[Install]WantedBy=multi-user.target\n\n然后使systemd重新加载配置文件并重新启动Logstash\nsudo systemctl daemon-reloadsudo systemctl restart logstash\n\n使Logstash开机自启动由于RPM包安装时已经放好了自启动的配置文件，我们只需要在systemd中激活它就可以了。\nsudo systemctl enable logstashsudo systemctl start logstash\n\n参考文档Logstash Reference\n","categories":["瞎折腾"],"tags":["ELK","Logstash"]},{"title":"在Java bean中实现setter的链式调用","url":"/java-bean-chaining-setters.html","content":"在经年累月的开发中，你是不是已经对动辄数行的.setXXX()方法厌烦了呢？在这篇博文中，我将介绍一个小技巧，可以稍稍让你的代码变得美观一些。\n\n\n前言通常来说，一个POJO大致上会是这样的\npublic class Student &#123;    private String name;    private Integer age;    private Integer score;    public String getName() &#123;        return this.name;    &#125;    public void setName(String name) &#123;        this.name = name;    &#125;    public Integer getAge() &#123;        return this.age;    &#125;    public void setAge(Integer age) &#123;        this.age = age;    &#125;    public Integer getScore() &#123;        return this.score;    &#125;    public void setScore(Integer score) &#123;        this.score = score;    &#125;&#125;\n\n然后，在赋值的时候，会是这样的\nStudent james = new Student();james.setName(&quot;James&quot;);james.setAge(22);james.setScore(80);\n\n上面这三行set方法，本来没干什么事，却要写3行代码六十多个字，是不是看起来很难受，写起来很费劲？\n那么，这时候就可以通过链式调用setter来显著减少代码行数。\nSetter的链式调用首先，要实现链式调用setter，就得稍微修改一下setter方法。放心，改动量是很小的，对业务是没有影响的，操作也是一劳永逸的。\n具体做法呢，就是让setter返回这个POJO类型，而不是void，然后将this返回，比如这样：\npublic Student setName(String name) &#123;    this.name = name;    return this;&#125;public Student setAge(Integer age) &#123;    this.age = age;    return this;&#125;public Student setScore(Integer score) &#123;    this.score = score;    return this;&#125;\n\n然后，在创建对象的时候，就可以这样：\nStudent student = new Student().setName(&quot;james&quot;).setAge(22).setScore(80);\n\n怎么样？是不是突然变得很短很简洁？写起来是不是很方便？\n但是，要一个个改那么多setter，也挺麻烦的，特别是IDE里面还不一定能自动化，有没有更好的办法？\n答案是，有！\n是啥呢？\nLombok！\n用Lombok实现setter的链式调用Lombok是一个自动生成模版代码的工具，像构造方法啊、getter和setter啊，都可以用一个或多个注解来代替，然后在编译期由Lombok替你写这部分的代码。如果你还没用过，那么请务必学习和试用一下，它可以为你省下那些浪费在模版代码上的时间。\n在引入了Lombok之后，你就可以把上文的Student类重构成这样子了：\n@Data@Accessors(chain = true)public class Student &#123;    private String name;    private Integer age;    private Integer score;&#125;\n\n是的，你不再需要去写getter和setter了，更不需要一个个的去修改来让它支持链式调用了。只需要两个注解，就可以完成上面的一大片工作。\n第一行的@Data注解，是一系列注解的集合，用来生成一系列的模版代码。由于这不是本文重点，所以就不再展开说了。\n第二行的@Accessors注解，就是这里的重点了，它用来设定Lombok应该怎样生成setter和getter代码。而其中的chain属性，就是打开链式setter的开关，将它打开之后，就可以生成支持链式调用的setter方法。\n然后，你就可以像上文一样，去享受链式调用了。\n","categories":["小技巧"],"tags":["Java","代码技巧"]},{"title":"快速理解Java中的守护线程","url":"/java-daemon-thread.html","content":"面试的时候被问到“了解不了解守护线程”，可本菜别提了解了，听都没听说过。所以面试之后赶紧上网冲浪学习起来。\n\n\n什么是守护线程首先我们要知道，Java中存在两种线程，用户线程和守护线程。\n用户线程就是我们平时最常用到的那种线程，它属于高优先级线程，JVM在退出前会等待所有用户线程完成。\n守护线程则是低优先级线程，它的作用是为用户线程提供服务，不会阻止JVM退出（不能100%保证，不良的代码仍会阻止JVM退出）。\n\n即Thread#setDaemon(boolean)方法的JavaDoc提到的The Java Virtual Machine exits when the only threads running are all daemon threads.\n\n守护线程能干什么常见的场景有垃圾回收、释放未使用的对象的内存、从缓存中释放不需要的条目等等。\n怎么创建守护线程调用Thread#setDaemon(boolean)方法就可以让一个线程变成守护线程，如：\nimport java.util.concurrent.TimeUnit;public class Main &#123;    public static void main(String[] args) throws InterruptedException &#123;        Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123;            System.out.println(&quot;Main thread exited.&quot;);        &#125;));        Thread thread = new Thread(() -&gt; &#123;            try &#123;                while (true) &#123;                    TimeUnit.SECONDS.sleep(1);                    System.out.println(&quot;Thread is running...&quot;);                &#125;            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125; finally &#123;                System.out.println(&quot;Thread exited.&quot;);            &#125;        &#125;);        // 一定要在Thread#start()前调用，否则会抛IllegalThreadStateException异常        thread.setDaemon(true);        thread.start();        TimeUnit.SECONDS.sleep(5);        System.out.println(&quot;Main thread exiting.....&quot;);    &#125;&#125;\n\n将上面代码运行后，可以在控制台看到如下输出：\nThread is running...Thread is running...Thread is running...Thread is running...Main thread exiting.....Main thread exited.\n\n可见，虽然线程中执行的是一个死循环，但是JVM在结束时，不会理会thread还在运行，直接就退出了。同样还可以看到的是，finally块并没有执行，所以在守护线程中不要使用任何需要关闭的资源，比如打开文件，因为守护线程退出的时候，它没有任何机会来关闭文件，就会导致数据丢失（我感觉就像kill -9掉一个正在内存中处理文件内容的进程，SIGKILL信号不会给这个进程任何善后的时间）。\n","categories":["学知识"],"tags":["Java","多线程","守护线程","面试","daemon thread"]},{"title":"Java的垃圾回收算法","url":"/java-gc-algorithms.html","content":"在JVM运行时的数据区域中了解到了JVM的内存模型，那么既然使用了内存，就要考虑如何回收内存。与C语言不同，Java不需要开发人员人工回收内存，而是交给Java的垃圾回收机制来完成。\n\n\n哪些内存需要回收在Java中，GC的对象是堆和方法区。栈中的栈帧随着方法的调用和退出，会自行完成压栈和出栈操作，每个栈帧所需的内存空间也是在类结构确定下来时就已知的，所以不怎么需要考虑内存的回收问题。但是堆和方法区则不一样，这部分的空间是动态分配和回收的，同时也只有在运行时才可得知要生成哪些对象以及需要多少空间。\n判断对象是否可以被回收通常有两种算法：引用计数法和可达性分析法。\n引用计数法引用计数法会给每个对象添加一个引用计数器，每当有一个地方引用它时，计数器就会加一；反之，每当一个引用失效时，计数器就会减一。任何时候，如果引用计数为0，则说明这个对象可以被回收。\n\n但是，这个算法有一个问题，那就是无法处理循环引用，即这样：\n\n此时，对象1，对象2，对象3都是不可达状态，理论上这三个对象都应该被回收，但是因为它们三个形成循环引用，引用计数器不为零，导致GC不会回收它们的空间。所以实际上，JVM并没有采用这种判断方法。\n可达性分析法(根搜索算法)可达性分析法的原理是，从根对象(GC Root)开始向下搜索，搜索走过的路径称为“引用链”，对象与引用链可以形成一个图，当任一个对象没有到根对象的引用链，即在这个图中该对象是不可达的，那么就判定这个对象是可以被回收的。\nJava语言使用如下几种GC Root对象：\n\n\n虚拟机栈(栈帧中的本地变量表)中引用的对象\n方法区中静态属性引用的对象\n方法区中常量引用的对象\n本地方法栈中JNI引用的对象\n\n\n还是用上面这个循环引用作为示例：\n\n此时虽然对象1、对象2、对象3互相之间存在引用，但是从根对象开始无法找到到达它们的路径，即它们三个都是不可达的，也就是可以被回收的。\n如何进行回收在根搜索算法的基础上，现代虚拟机中实现了三种算法：标记-清除算法，复制算法，标记-整理算法。\n标记-清除算法标记-清除算法把垃圾回收过程分成标记和清除两个阶段。在标记阶段，通过根节点标记所有可达的对象，也就是说，未被标记的对象都是不可达的对象。然后在清除阶段回收所有未被标记的对象。详细来说的话，就是当堆中的有效内存空间被耗尽时，就会停止整个程序(stop the world)，然后逐步开始标记和清除工作。\n标记的过程，实际上是遍历所有的GC Roots，并标记所有可达的对象。而清除的过程，则是遍历堆中所有的对象，并清除没有被标记的对象。\n在回收过程中一定要停止程序运行的原因，是为了避免在标记完成而尚未开始清除时，有新的可达的对象被创建出来。一旦出现这种情况，因为新创建的对象没有被标记，所以在清除阶段这个对象又会被清除。如果停止了程序的运行，那么在清除过程中，对象的状态不会发生变化，也就不会发生前面说的这种问题。\n这个算法尽管可以有效的回收内存，但是也有两个比较大的缺点：\n\n遍历所有对象的效率比较低，导致程序停止运行的时间比较长\n这种方法清理出的内存空间是不连续的，会造成空闲空间碎片化，并会影响数组分配空间。同时为了得知哪些空间是可用的，JVM还需要额外维护内存闲置空间的信息。\n\n复制算法复制算法的思想，是将原有的内存空间分成两部分，每次只使用其中一部分。在垃圾回收时，会从正在使用的部分中，将标记的对象复制到另一块内存中，然后清除正在使用的内存块，并交换两块内存的角色，来完成空间的回收。\n该算法比标记-清除算法的效率高，但是该算法不适合活动对象较多的场合，比如老年代空间。此外，该算法会造成一定程度的内存空间浪费，因为总是有一片内存空间是被闲置的。为了节省空间，考虑到新生代空间中的对象存活时间大多不会很长，所以虚拟机可以选择不将内存对半分，而是将内存分割成一块比较大的Eden空间和两块比较小的Survivor空间(From Survivor和To Survivor)，每次同时使用Eden和其中一个Survivor。比如HotSpot虚拟机默认为Eden分配80%的空间，为两个Survivor各分配10%的空间。\nEden区，如其名字“伊甸园”一般，对象在被创建时，首先会放在这个区域；Survivor区，也如其名字“幸存者区”一样，存放的是每次垃圾回收后被保留下来的对象。\n在每次垃圾回收时，Eden区中不能被回收的对象，和From Survivor区中不能被回收的对象，都将被复制到To Survivor区中，然后回收Eden区和From Survivor区的空间，并且幸存下来的对象的age属性会加一，最后From Survivor和To Survivor两者的角色对调。如果发生Survivor空间不足以存放所有活动对象时，则会使用老年代来进行分配担保，大的对象会跳过Survivor区直接进入老年代。\n标记-整理算法因为复制算法在活动对象较多时，会发生很多的复制操作，导致算法效率比较低，而老年代的特点就是活动的对象比较多。“标记-整理”算法就是为了应对这一情况而诞生的。\n标记-整理算法把垃圾回收过程分成标记和整理两个阶段。标记阶段的做法与“标记-清除”算法一样，遍历所有的GC Roots并标记出活动的对象；而在整理阶段，所有的活动对象都会向内存空间的一端移动(比如全部从内存空间的其实位置开始排列)，然后将边界以外的内存直接清理。\n该算法的另一个优点是，因为该算法不会分割内存空间，而且每次回收后对象占用的空间肯定小于回收前所占用的空间，所以不再需要额外的空间进行分配担保。\n分代收集算法分代收集算法实际上就是根据不同内存空间的特性，一般是将堆分为新生代和老年代，并根据其各自的特点，在新生代使用复制算法回收，在老年代使用标记-整理算法回收。\n","categories":["学知识"],"tags":["Java","垃圾回收","GC"]},{"title":"记一个Caddy和GitHub自定义域名的坑","url":"/caddy-and-github-custom-domain.html","content":"最近心血来潮，想给这个博客在GitHub上面的页面绑个自定义域名，结果无意中发现了一个坑。\n\n\n前情提要如关于页面所见，这个博客是同时放在GitHub Pages和我的服务器上面的。我的服务器上面呢，是用Caddy的git插件监听了一个WebHook来实现同步更新的。\n在我绑定Custom domain之前，Caddy的自动更新一直在默默正常工作着。但就在我绑了Custom domain之后，我发现，Caddy没能成功拉取最新版本的仓库。\n追踪线索首先使用排除法，肯定不是GitHub的问题。那就看一下Caddy的日志里面有没有什么线索吧。\nAug 09 14:44:42 vps caddy[4516]: 2018/08/09 14:44:42 Received pull notification for the tracking branch, updating...Aug 09 14:44:43 vps caddy[4516]: From https://github.com/boris1993/boris1993.github.ioAug 09 14:44:43 vps caddy[4516]:  * branch            master     -&gt; FETCH_HEADAug 09 14:44:43 vps caddy[4516]:  + 3d5ecea...204143b master     -&gt; origin/master  (forced update)Aug 09 14:44:43 vps caddy[4516]: *** Please tell me who you are.Aug 09 14:44:43 vps caddy[4516]: RunAug 09 14:44:43 vps caddy[4516]:   git config --global user.email &quot;you@example.com&quot;Aug 09 14:44:43 vps caddy[4516]:   git config --global user.name &quot;Your Name&quot;Aug 09 14:44:43 vps caddy[4516]: to set your account&#x27;s default identity.Aug 09 14:44:43 vps caddy[4516]: Omit --global to set the identity only in this repository.Aug 09 14:44:43 vps caddy[4516]: fatal: unable to auto-detect email address (got &#x27;www-data@vps.(none)&#x27;)Aug 09 14:44:43 vps caddy[4516]: 2018/08/09 14:44:43 exit status 128\n\n鞥？啥时候git pull也要提供用户名和邮箱了？\n随手往上面翻了翻，看见了点更有意思的东西：\nAug 10 16:45:46 vps caddy[11022]: 2018/08/10 16:45:46 Received pull notification for the tracking branch, updating...Aug 10 16:45:47 vps caddy[11022]: From https://github.com/boris1993/boris1993.github.ioAug 10 16:45:47 vps caddy[11022]:  * branch            master     -&gt; FETCH_HEADAug 10 16:45:47 vps caddy[11022]:    3a305c6..b57b257  master     -&gt; origin/masterAug 10 16:45:47 vps caddy[11022]: Updating 3a305c6..b57b257Aug 10 16:45:47 vps caddy[11022]: Fast-forwardAug 10 16:45:47 vps caddy[11022]:  CNAME | 1 +Aug 10 16:45:47 vps caddy[11022]:  1 file changed, 1 insertion(+)Aug 10 16:45:47 vps caddy[11022]:  create mode 100644 CNAMEAug 10 16:45:47 vps caddy[11022]: 2018/08/10 16:45:47 https://github.com/boris1993/boris1993.github.io.git pulled.\n\n新增了个叫CNAME的文件？这是啥玩意？得，看看里面写了啥。\n$ cat CNAMEblog2.boris1993.tk$\n\n这……不是我刚绑的那个自定义域名么……原来是这么实现的……\n好吧，这样一来，问题就清楚了。\n结案其实这个问题，是这么回事：\n在配置了自定义域名之后，GitHub会往仓库里放一个名为CNAME的文件，而我在用hexo提交的时候，我本地完全没有关于这个文件的任何记录，导致远端仓库的CNAME文件又丢了，而在Caddy更新的时候，怀疑Caddy在进行merge操作，merge操作需要用户提供用户名和邮箱，但是运行Caddy的www-data用户下没有这两个配置，于是就导致了上面的错误。\n至于解决方案嘛，要么就往博客的源码里面放一个名为CNAME文件并且保证内容正确，要么就干脆不配置自定义域名了。\n","categories":["瞎折腾"],"tags":["Caddy","GitHub","Custom domain"]},{"title":"在Java中如何实现在多个线程全部完成后再执行后续的代码","url":"/java-how-to-wait-for-threads-before-continuing.html","content":"面试的时候被问到，如何在5个线程全部执行完毕之后，再继续执行后续的代码。迫于对多线程的东西了解不多，只答出一个CountDownLatch，还大概答出一个用Future的思路。回来痛定思痛，请出了万能的ChatGPT，学到了其他的几种方法。\n\n\nCountDownLatch在 Java 中可以使用CountDownLatch来实现线程间的协调和等待。CountDownLatch是一个同步工具类，它允许一个或多个线程等待一组事件发生。\n在这个问题中，我们可以创建一个初始值为5的CountDownLatch，每个线程完成时调用countDown()方法将计数器减一，主线程调用await()方法等待计数器归零后再执行后续代码。\n下面是一个示例代码：\nimport java.util.concurrent.CountDownLatch;public class Main &#123;    public static void main(String[] args) throws InterruptedException &#123;        int numThreads = 5;        CountDownLatch latch = new CountDownLatch(numThreads);                // 创建 5 个线程        for (int i = 0; i &lt; numThreads; i++) &#123;            Thread thread = new MyThread(latch);            thread.start();        &#125;                // 等待所有线程执行完毕        // P.S.：我当时想不起来可以await了，就答了个用循环检查CountDownLatch......        latch.await();                // 执行后续代码        System.out.println(&quot;All threads have finished executing.&quot;);    &#125;    private static class MyThread extends Thread &#123;        private final CountDownLatch latch;        public MyThread(CountDownLatch latch) &#123;            this.latch = latch;        &#125;        @Override        public void run() &#123;            try &#123;                // 线程执行一些操作                // ...                System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; has finished executing.&quot;);            &#125; finally &#123;                // 线程执行完毕后调用 countDown() 方法                latch.countDown();            &#125;        &#125;    &#125;&#125;\n\n在上面的示例代码中，MyThread类表示线程的实现。在run()方法中，线程执行一些操作，然后调用countDown()方法通知CountDownLatch计数器减一。在主线程中，我们创建5个线程并启动它们，然后调用await()方法等待所有线程执行完毕。最后，当所有线程执行完毕时，主线程输出一条消息，并继续执行后续代码。\n使用join()方法如果不使用CountDownLatch，可以使用Java的线程join()方法来等待所有线程执行完毕。join()方法可以使得一个线程在另一个线程结束后再执行。具体来说，可以在主线程中依次调用每个线程的join()方法，让主线程等待每个线程执行完毕后再继续执行后续代码。\n下面是一个使用join()方法实现的示例代码：\nimport java.util.ArrayList;import java.util.List;public class Test &#123;    public static void main(String[] args) throws InterruptedException &#123;        List&lt;MyThread&gt; threads = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 5; i++) &#123;            MyThread thread = new MyThread();            thread.start();            threads.add(thread);        &#125;        for (MyThread thread : threads) &#123;            // join()方法的JavaDoc写的清晰明了：            // Waits for this thread to die.            thread.join();        &#125;        System.out.println(&quot;All threads have finished executing.&quot;);    &#125;    private static class MyThread extends Thread &#123;        @Override        public void run() &#123;            System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; has finished executing.&quot;);        &#125;    &#125;&#125;\n\n在上面的示例代码中，MyThread类表示线程的实现。在主线程中，我们创建5个线程并启动它们，然后依次调用每个线程的join()方法等待线程执行完毕。最后，当所有线程执行完毕时，主线程输出一条消息，并继续执行后续代码。\n需要注意的是，join()方法会阻塞当前线程，直到被等待的线程执行完毕。因此，在使用join()方法时要小心，以避免出现死锁等问题。\nFuture使用Future类也可以实现等待多个线程执行完毕。Future是Java提供的一种异步计算的机制，可以在一个线程中调用另一个线程并等待其执行结果。具体来说，可以使用 ExecutorService的invokeAll()方法启动多个线程，将返回的Future对象保存到一个列表中，然后调用每个Future对象的get()方法等待线程执行完毕。\n下面是一个使用Future类实现的示例代码：\nimport java.util.ArrayList;import java.util.List;import java.util.concurrent.*;public class Test &#123;    public static void main(String[] args) throws InterruptedException, ExecutionException &#123;        ExecutorService executor = Executors.newFixedThreadPool(5);        List&lt;Callable&lt;Void&gt;&gt; tasks = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 5; i++) &#123;            Callable&lt;Void&gt; task = new MyThread();            tasks.add(task);        &#125;        List&lt;Future&lt;Void&gt;&gt; futures = executor.invokeAll(tasks);        for (Future&lt;Void&gt; future : futures) &#123;            // Waits if necessary for the computation to complete,            // and then retrieves its result.            future.get();        &#125;        // 执行后续代码        System.out.println(&quot;All threads have finished executing.&quot;);        // 关闭线程池        executor.shutdown();    &#125;    private static class MyThread implements Callable&lt;Void&gt; &#123;        @Override        public Void call() &#123;            System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; has finished executing.&quot;);            return null;        &#125;    &#125;&#125;\n\n在上面的示例代码中，MyTask类表示线程的实现。在主线程中，我们使用ExecutorService的invokeAll()方法启动多个线程，并将返回的Future对象保存到一个列表中。然后，我们依次调用每个Future对象的get()方法等待线程执行完毕。最后，当所有线程执行完毕时，主线程输出一条消息，并继续执行后续代码。\n需要注意的是，在使用Future类时要小心，以避免出现线程池满载等问题。在实际应用中，可以根据需要调整线程池大小或使用其他调度机制来处理大量的并发任务。\nwait()和notifyAll()可以使用Java的wait()和notifyAll()方法来实现等待多个线程执行完毕。具体来说，可以在主线程中创建一个共享的计数器变量，每个线程在执行完毕后将计数器减一。当计数器为0时，说明所有线程执行完毕，可以调用notifyAll()方法唤醒主线程继续执行后续代码。\n下面是一个使用wait()和notifyAll()方法实现的示例代码：\nimport java.util.ArrayList;import java.util.List;import java.util.concurrent.*;public class Test &#123;    public static void main(String[] args) throws InterruptedException &#123;        final CountDownLatch countDownLatch = new CountDownLatch(5);        for (int i = 0; i &lt; 5; i++) &#123;            new MyThread(countDownLatch).start();        &#125;        synchronized (countDownLatch) &#123;            while (countDownLatch.getCount() &gt; 0) &#123;                countDownLatch.wait();            &#125;        &#125;        System.out.println(&quot;All threads have finished executing.&quot;);    &#125;    private static class MyThread extends Thread &#123;        private final CountDownLatch countDownLatch;        public MyThread(CountDownLatch countDownLatch) &#123;            this.countDownLatch = countDownLatch;        &#125;        @Override        public void run() &#123;            try &#123;                System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; has finished executing.&quot;);            &#125; finally &#123;                synchronized (countDownLatch) &#123;                    countDownLatch.countDown();                    countDownLatch.notifyAll();                &#125;            &#125;        &#125;    &#125;&#125;\n\n在上面的示例代码中，MyThread 类表示线程的实现。在主线程中，我们创建 5 个线程并启动它们，然后使用一个共享的计数器变量 countDownLatch 记录线程执行的状态。当每个线程执行完毕时，将计数器减一，并调用 notifyAll() 方法唤醒主线程。在主线程中，我们使用 wait() 方法等待所有线程执行完毕，直到计数器为 0。\nP.S.：感觉这个实现有种莫名其妙的别扭感……\nCompletionService使用CompletionService可以比较方便地实现等待所有线程执行完毕的功能。CompletionService是Java提供的一个接口，它可以将任务提交给线程池执行，并在任务执行完毕后立即返回结果，从而实现异步执行和结果收集的功能。\n具体来说，可以创建一个ExecutorService对象作为线程池，然后将任务提交给CompletionService执行。在提交任务时，可以使用submit()方法返回一个Future对象，用于后续获取任务执行的结果。使用CompletionService的take()方法可以等待任意一个任务执行完毕并返回结果，从而避免了使用join()方法等待所有线程执行完毕的阻塞等待。\n下面是一个使用CompletionService实现的示例代码：\nimport java.util.concurrent.*;public class Test &#123;    public static void main(String[] args) throws InterruptedException &#123;        ExecutorService executor = Executors.newFixedThreadPool(5);        CompletionService&lt;Void&gt; completionService = new ExecutorCompletionService&lt;&gt;(executor);        for (int i = 0; i &lt; 5; i++) &#123;            completionService.submit(new MyTask());        &#125;        for (int i = 0; i &lt; 5; i++) &#123;            // Retrieves and removes the Future representing the next completed task,            // waiting if none are yet present.            completionService.take();            System.out.println(&quot;Thread &quot; + i + &quot; has finished executing&quot;);        &#125;        executor.shutdown();        System.out.println(&quot;All threads have finished executing.&quot;);    &#125;    private static class MyTask implements Callable&lt;Void&gt; &#123;        @Override        public Void call() &#123;            System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; has finished executing.&quot;);            return null;        &#125;    &#125;&#125;\n\n线程的getState()方法除了CountDownLatch和Future类，还有其他实现方法。其中一个比较简单的方法是使用 Java 的线程状态（Thread.State）来判断所有线程是否执行完毕。\n具体来说，可以将所有线程保存到一个列表中，然后在主线程中依次调用每个线程的getState()方法，检查线程状态是否为Terminated。如果所有线程都已经执行完毕，则可以继续执行后续代码。\n下面是一个使用线程状态实现的示例代码：\nimport java.util.ArrayList;import java.util.List;public class Test &#123;    public static void main(String[] args) throws InterruptedException &#123;        List&lt;MyThread&gt; threads = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 5; i++) &#123;            MyThread thread = new MyThread();            thread.start();            threads.add(thread);        &#125;        boolean allThreadsFinished = false;        while (!allThreadsFinished) &#123;            allThreadsFinished = true;            for (MyThread thread : threads) &#123;                if (thread.getState() != Thread.State.TERMINATED) &#123;                    allThreadsFinished = false;                    break;                &#125;            &#125;            Thread.sleep(100);        &#125;        System.out.println(&quot;All threads have finished executing.&quot;);    &#125;    private static class MyThread extends Thread &#123;        @Override        public void run() &#123;            System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; has finished executing.&quot;);        &#125;    &#125;&#125;\n\n在上面的示例代码中，MyThread类表示线程的实现。在主线程中，我们创建5个线程并启动它们，然后循环检查每个线程的状态，直到所有线程都执行完毕。在每次循环中，我们先将allThreadsFinished标志设为true，然后依次检查每个线程的状态。如果有任何一个线程的状态不是Terminated，则将allThreadsFinished标志设为false，并跳出循环。等待一段时间后重新检查线程状态，直到所有线程都执行完毕。\n需要注意的是，使用线程状态进行等待需要定期检查所有线程的状态，因此会占用一定的CPU资源。在实际应用中，可以根据需要调整等待的时间间隔以及检查的次数，以平衡等待时间和CPU资源的消耗。\nCyclicBarrier使用CyclicBarrier也可以比较方便地实现等待所有线程执行完毕的功能。CyclicBarrier是Java提供的一个同步辅助类，它可以让一组线程等待彼此达到某个共同的屏障点。\n具体来说，可以创建一个CyclicBarrier对象，并指定需要等待的线程数量。每个线程在执行完自己的任务后，调用CyclicBarrier的await()方法，表示已经到达了屏障点。当所有线程都到达了屏障点后，CyclicBarrier就会释放所有线程，从而实现等待所有线程执行完毕的功能。\n下面是一个使用CyclicBarrier实现的示例代码：\nimport java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;public class Test &#123;    public static void main(String[] args) throws InterruptedException &#123;        // 创建 CyclicBarrier 对象，等待 5 个线程        final CyclicBarrier cyclicBarrier =                new CyclicBarrier(                        5,                        () -&gt; &#123;                            // 所有线程到达屏障点时执行的操作                            System.out.println(&quot;All threads have finished executing.&quot;);                        &#125;);        for (int i = 0; i &lt; 5; i++) &#123;            new Thread(new MyTask(cyclicBarrier)).start();        &#125;    &#125;    private static class MyTask implements Runnable &#123;        private final CyclicBarrier cyclicBarrier;        public MyTask(CyclicBarrier cyclicBarrier) &#123;            this.cyclicBarrier = cyclicBarrier;        &#125;        @Override        public void run() &#123;            // 等待所有线程执行完毕            try &#123;                System.out.println(&quot;Thread &quot; + Thread.currentThread().getId() + &quot; has finished executing.&quot;);                cyclicBarrier.await();            &#125; catch (InterruptedException | BrokenBarrierException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n在上面的示例代码中，MyTask类表示线程的实现。在主线程中，我们创建一个CyclicBarrier对象，并指定需要等待的线程数量为5。每个线程在执行完自己的任务后，调用CyclicBarrier的await()方法，表示已经到达了屏障点。当所有线程都到达了屏障点后，CyclicBarrier就会执行屏障操作，这里是输出All threads have finished executing.。\n需要注意的是，如果其中一个线程在等待过程中被中断或者抛出异常，那么CyclicBarrier就会被破坏，所有线程都会被唤醒并抛出BrokenBarrierException异常。因此，在实现时需要捕获InterruptedException和BrokenBarrierException异常。\n","categories":["学知识"],"tags":["Java","多线程","面试","CountDownLatch","Future"]},{"title":"Java的内部类和private修饰符","url":"/java-inner-class-and-private-identifier.html","content":"一个private属性只能被它所在的类访问，这件事地球人都知道。但是，你有没有想过，这条规则有没有可能在某种情况下，会变得不成立？\n本文将通过一个小例子，来演示怎么让private修饰符“失效”，以及它为什么会“失效”。\n\n\n示例代码废话不多说，先写一段代码：\npublic class OuterClass &#123;    private String outerClassName = &quot;outerClass&quot;;    public class InnerClass &#123;        public void printOuterClassName() &#123;            System.out.println(outerClassName);        &#125;    &#125;    public static void main(String[] args) &#123;        OuterClass outerClass = new OuterClass();        OuterClass.InnerClass innerClass = outerClass.new InnerClass();        innerClass.printOuterClassName();    &#125;&#125;\n\n上面的代码是不是感觉有一丝异样？为什么在内部类里，能直接访问到外部类的private属性？难道private修饰符真的“失效”了？\n别急，待我们把这个class反编译了，从字节码层面来看看它到底有什么猫腻。毕竟，字节码可不会骗人。\n反编译外部类$ javap -c OuterClass.classCompiled from &quot;OuterClass.java&quot;public class com.boris1993.OuterClass &#123;  public com.boris1993.OuterClass();    Code:       0: aload_0       1: invokespecial #2                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V       4: aload_0       5: ldc           #3                  // String outerClass       7: putfield      #1                  // Field outerClassName:Ljava/lang/String;      10: return  public static void main(java.lang.String[]);    Code:       0: new           #4                  // class com/boris1993/OuterClass       3: dup       4: invokespecial #5                  // Method &quot;&lt;init&gt;&quot;:()V       7: astore_1       8: new           #6                  // class com/boris1993/OuterClass$InnerClass      11: dup      12: aload_1      13: dup      14: invokevirtual #7                  // Method java/lang/Object.getClass:()Ljava/lang/Class;      17: pop      18: invokespecial #8                  // Method com/boris1993/OuterClass$InnerClass.&quot;&lt;init&gt;&quot;:(Lcom/boris1993/OuterClass;)V      21: astore_2      22: aload_2      23: invokevirtual #9                  // Method com/boris1993/OuterClass$InnerClass.printOuterClassName:()V      26: return  static java.lang.String access$000(com.boris1993.OuterClass);    Code:       0: aload_0       1: getfield      #1                  // Field outerClassName:Ljava/lang/String;       4: areturn&#125;\n\n有没有发现，78行出现了一个我们没有写过的方法access$000？而且从注释来看，它接受一个OuterClass类型的参数，而且返回的正是外部类的outerClassName的值。\n既然我们没定义这个方法，那就是编译器偷偷的给咱整了点活。至于为啥编译器要这么干，结合上面这个例子，也不难猜出来：这就是给内部类访问它的private属性用的。\n反编译内部类但是咱不能光猜啊，咱还得有证据。证据哪来？当然是内部类的字节码。\n$ javap -c OuterClass$InnerClass.classCompiled from &quot;OuterClass.java&quot;public class com.boris1993.OuterClass$InnerClass &#123;  final com.boris1993.OuterClass this$0;  public com.boris1993.OuterClass$InnerClass(com.boris1993.OuterClass);    Code:       0: aload_0       1: aload_1       2: putfield      #1                  // Field this$0:Lcom/boris1993/OuterClass;       5: aload_0       6: invokespecial #2                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V       9: return  public void printOuterClassName();    Code:       0: getstatic     #3                  // Field java/lang/System.out:Ljava/io/PrintStream;       3: aload_0       4: getfield      #1                  // Field this$0:Lcom/boris1993/OuterClass;       7: invokestatic  #4                  // Method com/boris1993/OuterClass.access$000:(Lcom/boris1993/OuterClass;)Ljava/lang/String;      10: invokevirtual #5                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V      13: return&#125;\n\n嗯，果然没错，在第20行这一条指令里，它调用了上面我们看到的那个access$000()方法。\n","categories":["学知识"],"tags":["Java","内部类"]},{"title":"Java自定义注解","url":"/java-customized-annotations.html","content":"注解是Java 5引入的一个新特性，它提供了一个用来将信息和元数据与程序元素相关联的能力，其作用如同一个修饰符，本身并不包含任何程序逻辑。\n本文将介绍如何创建和使用自定义的注解。\n\n\n元注解Java自带了四个作用于注解上的注解，即元注解，分别是：\n\n@Documented，用于注明该注解是否包含于JavaDoc中\n@Retention，用于注明这个注解将保留到什么时候\n@Target，用于注明这个注解将作用于哪些元素上\n@Inherit，用于注明该注解是否会被子类继承\n\n@Retention@Retention元注解定义了这个注解的生命周期，即这个注解将保留到什么时候。注解的生命周期有这三种：\n\nRetentionPolicy.SOURCE：仅在源码中保留，在编译期就会被丢弃。比如@Override和@SuppressWarnings就属于这类注解\nRetentionPolicy.CLASS：注解将会被写入到字节码中，但是在运行时会被丢弃。这个是默认的生命周期。\nRetentionPolicy.RUNTIME：该注解将保留至运行时。这意味着在运行时可以通过反射机制读取到注解的信息。\n\n@Target@Target元注解指定了该注解将可用于哪些元素上。可用的参数有如下几种：\n\nElementType.ANNOTATION_TYPE，用于描述注解。@Target(ElementType.ANNOTATION_TYPE)标注的注解将成为一个元注解。\nElementType.CONSTRUCTOR，用于描述构造方法\nElementType.FIELD，用于描述成员变量、对象、属性（包括enum实例）\nElementType.LOCAL_VARIABLE，用于描述局部变量\nElementType.METHOD，用于描述方法\nElementType.PACKAGE，用于描述包\nElementType.PARAMETER，用于描述参数\nElementType.TYPE，用于描述类、接口（包括注解）、enum生命声明\n\nJava 8中又新增了两个参数：\n\nElementType.TYPE_PARAMETER，可以用在Type的声明前\nElementType.TYPE_USE，可以用在使用Type的地方\n\n编写自定义注解及相关方法自定义注解的类型为@interface，注解中可以包含方法，方法名将作为注解的属性。\n注解中的方法不可以有参数，也不可以抛出异常，同时方法只能返回原始类型、String、Class、enums、注解类型，以及上述类型的数组。方法的默认值不可以是null。\n下面将通过一个示例演示如何编写和使用自定义注解相关的方法。\n示例将分别创建两个名为@JsonSerializable和@JsonElement的注解，以及一个名为JsonUtils的工具类。\n@JsonSerializable标记一个类可以被序列化成JSON，@JsonElement标记一个成员变量将会被包含在这个JSON中；JsonUtils工具类包含将对象序列化为JSON的方法。\n@JsonSerializable/** * 标记一个类可以被序列化成JSON * * 因为这个注解要在运行时通过反射获取，所以retention为runtime * * 因为这个注解作用于一个类，所以target为type */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface JsonSerializable &#123;&#125;\n\n@JsonElement/** * 标记一个成员变量将会被包含在这个JSON中 * * 因为这个注解要在运行时通过反射获取，所以retention为runtime * * 因为这个注解作用于成员变量，所以target为field */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)public @interface JsonElement &#123;    /**     * 指定该成员变量在JSON中的key值     */    public String key() default &quot;&quot;;&#125;\n\nJsonUtilspublic class JsonUtils &#123;    /**     * 将对象序列化为JSON     *     * @param object 要序列化的对象，需要有&#123;@link JsonSerializable&#125;注解     * @return 序列化后的JSON字符串，如果不可序列化则是null     * @throws InvocationTargetException     * @throws IllegalAccessException     * @throws NoSuchMethodException     */    public static String serializeToJson(Object object) throws InvocationTargetException, IllegalAccessException, NoSuchMethodException &#123;        // 检查对象是否可以被序列化        if (!isSerializable(object)) &#123;            return null;        &#125;        // 取得对象所属的类        Class clazz = object.getClass();        // 取得类中的所有方法        Method[] methods = clazz.getMethods();        // 取得所有方法的方法名，后面用于搜索        List&lt;String&gt; methodNames = Arrays.stream(methods).map(Method::getName).collect(Collectors.toList());        // 取得类中所有成员变量，包括public、protected、private、和默认访问权限的        Field[] fields = clazz.getDeclaredFields();        // 创建一个空的HashMap，用于存放要序列化的属性的名字和值        Map&lt;String, String&gt; elements = new HashMap&lt;&gt;(fields.length);        // 遍历所有成员变量        for (Field field : fields) &#123;            // 如果有JsonElement注解            if (field.isAnnotationPresent(JsonElement.class)) &#123;                // 取得变量名                String fieldName = field.getName();                // 拼接其对应getter方法名                // 不直接使用setAccessible()方法是因为我不喜欢这么干，这会破坏封装性                String getterName = &quot;get&quot; + fieldName.substring(0, 1).toUpperCase() + fieldName.substring(1);                // 检查这个变量是否有getter方法                if (methodNames.contains(getterName)) &#123;                    // 如果有getter方法，则根据方法名取得对应的方法实例                    Method method = clazz.getMethod(getterName);                    // 取得JsonElement注解中设定的key值                    String keyName = field.getAnnotation(JsonElement.class).key();                    // 如果key的值为空字符串，则使用属性名作为JSON中的key名                    // 否则取指定的key名                    // 并调用变量对应的getter方法取得变量的值                    // 最后放入HashMap中                    elements.put(&quot;&quot;.equals(keyName) ? field.getName() : keyName, String.valueOf(method.invoke(object)));                &#125;            &#125;        &#125;        // 遍历HashMap，构造JSON内容        String jsonBody = elements.entrySet()                .stream()                // 取得每个元素的key名和值，拼接成 \\t&quot;key&quot;:&quot;value&quot; 的形式                .map(entry -&gt; &quot;\\t\\&quot;&quot; + entry.getKey() + &quot;\\&quot;:\\&quot;&quot; + entry.getValue() + &quot;\\&quot;&quot;)                // 每行元素间插入分隔符，逗号分隔每行数据，\\n实现换行                .collect(Collectors.joining(&quot;,\\n&quot;));        // 最后拼接JSON首尾的大括号        return &quot;&#123;\\n&quot; + jsonBody + &quot;\\n&#125;&quot;;    &#125;    /**     * 检查对象是否可被序列化成JSON     * @param object 将被序列化的对象     * @return 是否可被序列化     */    private static boolean isSerializable(Object object) &#123;        // null不可被序列化        if (object == null) &#123;            return false;        &#125;        Class clazz = object.getClass();        // 如果有JsonSerializable注解，即可被序列化        return clazz.isAnnotationPresent(JsonSerializable.class);    &#125;&#125;\n\n使用自定义注解首先创建一个BookModel类：\n/** * 书籍信息 */@JsonSerializablepublic class BookModel &#123;    /**     * 书名     */    @JsonElement(key = &quot;bookname&quot;) // 在JSON中将bookName重命名为bookname    private String bookName;    /**     * 分类     */    @JsonElement    private String category;    /**     * 价格     */    @JsonElement    private int price;    @Override    public String toString() &#123;        return &quot;BookModel&#123;&quot; +                &quot;bookName=&#x27;&quot; + bookName + &#x27;\\&#x27;&#x27; +                &quot;, category=&#x27;&quot; + category + &#x27;\\&#x27;&#x27; +                &quot;, price=&quot; + price +                &#x27;&#125;&#x27;;    &#125;    /**     * getter，setter和构造方法略     */&#125;\n\n接下来在main方法里构造对象，并将其序列化成JSON：\npublic static void main(String[] args) &#123;    BookModel book = new BookModel(&quot;Head First Java&quot;, &quot;Java&quot;, 55);    try &#123;        System.out.println(JsonUtils.serializeToJson(book));    &#125; catch (InvocationTargetException | IllegalAccessException | NoSuchMethodException e) &#123;        e.printStackTrace();    &#125;&#125;\n\n序列化后的结果将是这样：\n&#123;    &quot;bookname&quot;:&quot;Head First Java&quot;,    &quot;category&quot;:&quot;Java&quot;,    &quot;price&quot;:&quot;55&quot;&#125;","categories":["学知识"],"tags":["Java","注解","Annotations"]},{"title":"简谈JVM中的几种引用","url":"/java-jvm-reference.html","content":"在Java的垃圾回收算法一文中，我们知道JVM是根据一个对象有没有被引用来判断要不要对其进行垃圾回收的。但是，如果我们为了提升垃圾回收的效率，想要再把垃圾回收的条件更细化一些，比如只在内存非常紧张的时候才回收某些对象，那么光靠一个粗略的“引用”，就显得心有余而力不足了。所以，在JDK 1.2版本之后，Java扩充了引用的概念，将其扩充成了强引用，软引用，弱引用，虚引用四个更细化的概念。\n本文将参考《深入理解Java虚拟机(第3版)》中再谈引用一节，简述一下这四种引用的概念，以及被引用的对象何时会被垃圾回收器回收。\n\n\n强引用(Strongly reference)强引用是最传统的“引用”的定义，指在代码中普遍存在的引用赋值，比如Object obj = new Object()。不论在任何情况下，只要强引用关系存在，那么垃圾回收器就永远不会回收掉被引用的对象。\n软引用(Soft reference)软引用是用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常之前，会把这些对象纳入回收范围进行第二次回收，如果在回收之后仍没有足够的内存，才会抛出内存溢出异常。在JDK 1.2版本之后提供了SoftReference类来实现软引用。\n软引用对象可以用在类似缓存的场景中，比如在一个图片编辑器中，应用可以将用户打开的文件读入一个软引用对象。\n如果要创建一个软引用对象，那么可以使用如下代码：\nSoftReference&lt;String&gt; stringSoftReference = new SoftReference&lt;&gt;(&quot;string&quot;);\n\n弱引用(Weak reference)弱引用也是用来被描述非必须的对象，但它的强度比软引用要更弱一些。被弱引用关联的对象只能生存到下一次垃圾回收发生时。当垃圾回收器开始工作后，无论当前剩余内存是否足够，被弱引用关联的对象都会被回收掉。在JDK 1.2版本之后提供了WeakReference类来实现弱引用。\n弱引用常见于一些集合类中，尤其在哈希表中。因为哈希表的接口允许用户使用任何Java对象作为key来使用，而当一个键值被存入哈希表后，哈希表本身就有了对这些键和值的引用。如果这些引用是强引用，那么只要这个哈希表对象存活，哈希表关联的键和值也就永远不会被回收。如果这个哈希表同时又包含了大量的对象，那么就可能会长时间占用服务器的大量内存。这类问题的解决办法就是使用弱引用来关联这些对象，比如使用WeakHashMap。\n如果要创建一个弱引用对象，那么可以使用如下代码：\nWeakReference&lt;String&gt; stringWeakReference = new WeakReference&lt;&gt;(&quot;string&quot;);\n\n虚引用(Phantom reference)虚引用也叫“幽灵引用”或“幻影引用”，是最弱的一种引用。一个对象是否有虚引用存在，完全不会对其生存时间构成影响，同时用户也无法通过一个虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被回收时收到一个系统通知。在JDK 1.2版本之后提供了PhantomReference类来实现虚引用。\n[^1]: Java 强软弱虚引用介绍及使用场景[^2]: java中四种引用类型\n","categories":["学知识"],"tags":["Java","JVM"]},{"title":"JVM运行时的数据区域","url":"/java-jvm-runtime-data-areas.html","content":"JVM在运行时，会按照程序执行的需要来创建一系列的运行时数据区域。有的区域只会随JVM起停而被创建和销毁，有的区域则会独立分配给各个线程，并随线程的起停而创建和销毁。这些运行时区域，按照功能和性质不同，会分成如下几部分：\n\n线程专有\npc(program counter)寄存器\nJVM栈\n本地方法栈\n\n\n线程间共享\n堆\n方法区\n运行时常量池\n\n\n\n\n\n\n\npc(program counter)寄存器JVM允许同时运行多个线程，每个线程都有它自己的PC寄存器。在任意时刻，每个JVM线程都在执行一个方法中的某条语句，而这个正在被执行的方法，就叫做这个线程的“当前方法”。\n如果当前方法不是一个本地(native)方法，那么PC寄存器的内容是当前正在执行的指令的地址；如果当前方法是本地方法，那么PC寄存器的值则是空(undefined)的。\nJVM栈每个JVM都会在其启动时创建自己私有的JVM栈，栈之中存储的是栈帧，用于存储局部变量和方法调用信息。\n规范中允许栈的深度可以是固定的，也可以根据要求动态的扩展和收缩。如果是固定深度的栈，那么每个栈的深度会在其创建时按照需要独立指定。\n当请求创建的栈大于所允许的深度，那么JVM会抛出StackOverflowError异常；当程序试图扩大一个可以动态伸缩的栈，或者试图为新的线程创建一个栈，但是可用内存不足以完成这个操作时，那么JVM会抛出OutOfMemoryError异常。\n本地方法栈本地方法栈与JVM栈类似，保存了本地方法的调用信息。\n本地方法栈的空间可以是固定的，也可以是动态伸缩的。\n当程序申请了大于所允许的本地方法栈空间，那么JVM会抛出StackOverflowError异常；如果程序申请扩展一个可以动态伸缩的本地方法栈，或者试图创建一个栈，但是可用内存不足以满足要求时，JVM会抛出OutOfMemoryError异常。\n堆在JVM启动时，会创建一个共享于所有线程的堆空间，其中存放着所有的对象，和被分配好空间的数组。用于存放对象的空间由一个自动化的存储空间管理机制，即垃圾回收机制(garbage collector)，来进行管理。堆空间可以是固定大小的，也可以是按需伸缩的。\n如果程序试图申请扩大堆空间，但是存储管理机制无法满足需求时，JVM会抛出OutOfMemory异常。\n在堆中，JVM又根据作用不同，将内存空间分为如下几部分：\n新生代(New generation)新生代保留的是生命周期短，并且很快就会被回收掉的对象。其中的空间又随着“复制算法”这一垃圾回收算法而被分为Eden Space和Survivor Space。具体可以参考Java的垃圾回收算法这篇博文。\n老年代(Tenured generation)在多次垃圾回收后仍然存活的对象，将会被放到老年代空间中。因此可以认为，老年代中的对象的生命周期都是比较长的。\n方法区方法区(method area)是一个共享于所有JVM线程的空间，创建于JVM启动时，其中主要存放的是类的元数据，包括类的类型信息、常量池、方法数据、方法的代码等，这些数据主要来源于class文件。方法区逻辑上属于堆的一部分，但是为了与堆区分开来，方法区通常又叫非堆。\n类型信息包括类的完整名称、父类的完整名称、类型修饰符(private&#x2F;protected&#x2F;public)，和类型的直接接口类表。\n方法的数据包括方法的名称、返回类型、参数、方法的修饰符、字节码、操作数栈和方法栈帧的局部变量区大小，和异常表。\n方法区的大小可以是固定的，也可以是按需伸缩的，但是根据虚拟机实现的不同，垃圾回收机制可能不会回收或压缩方法区的空间。\n如果方法区的可用内存无法满足一次申请空间的请求，那么JVM会抛出OutOfMemoryError异常。\n永久代和Metaspace在HotSpot VM中，永久代和Metaspace就是方法区的具体实现。在Java 8之前，方法区是以永久代的形式存在的；而从Java 8之后，永久代就被Metaspace取而代之了。\n在Java 1.7和之前版本中，永久代是一块独立于堆的内存空间，在物理内存上与堆是连续的。同时，在Java 1.7中，一部分原属于永久代的内容也在逐步被移动到其他位置，比如符号引用被移动到了本地内存(native memory)中，字符串常量池和类的静态变量则被移动到了堆中。\n从Java 8开始，永久代被Metaspace取而代之。Metaspace的内存空间不再与堆连续，而是存在于本地内存中。\n运行时常量池运行时常量池对应class文件中的constant_pool表。\n运行时常量池中包含了数值常量和属性的引用。每个运行时常量池的空间都会在类或接口被创建时生成，并且从方法区中分配空间。在创建运行时方法区时，如果申请的空间大于方法区可提供的空间，那么JVM会抛出OutOfMemoryError异常。\n参考文档[^1]: 《The Java Virtual Machine Specification (Java SE 8 Edition)》 - 2.5 Run-Time Data Areas[^2]: 面试官，Java8 JVM内存结构变了，永久代到元空间[^3]: 方法区 - JVM 运行时的数据区域[^4]: 方法区（永久区、元空间） - 深入理解JAVA虚拟机（内存模型+GC算法+JVM调优）[^5]: Java8内存模型—永久代(PermGen)和元空间(Metaspace)\n","categories":["学知识"],"tags":["Java","JVM","内存模型"]},{"title":"JVM的栈帧","url":"/java-jvm-stack-frame.html","content":"栈帧是支持JVM进行方法调用和执行的数据结构，它是JVM 运行时的数据区域的栈元素，其中包含了方法的局部变量表、操作数栈、动态链接方法，和返回地址等信息。\n\n\n局部变量表和操作数栈的空间，在编译期就已经可以确定下来，并会随着方法表的code属性一并提供给JVM，所以每个栈帧的空间不会受运行时数据的影响，而仅取决于JVM的实现。\n每当一个方法被调用都会生成一个栈帧，并在方法执行完毕后被销毁，并且对于每个活动的线程，只有栈顶的栈帧是活动的，这个栈帧被称为“活动栈帧”，与其相关联的方法被称为“活动方法”，以及与其相关联的类被称为“活动类”。\n局部变量表每个栈帧中都会有一个被称为“局部变量表”的数组，其中保存着方法的局部变量。局部变量表的大小在编译期就已经确定下来，并保存在class文件的code区。各个变量可通过数组下标的方式被定位到，对于需要占用两个元素的数据类型，比如long和double，其对应的下标使用较小的那个值。\n在方法执行时，JVM使用局部变量表完成参数值到参数列表的传递过程的。如果调用的是类方法，那么参数会从局部变量表第0位开始向后排列。如果调用的是实例方法(非static方法)，则局部变量表第0位默认用于传递方法所属对象的实例的引用，在方法中使用this关键字可以访问到这个隐含的参数，其余的参数则从第1位开始向后排列；在参数表分配完毕后，方法体内部定义的变量会按照其顺序和作用域分配剩余的位置。\n操作数栈每个栈帧中都有一个被称为“操作数栈”的栈。操作数栈的最大深度也是在编译期就可以确定下来，并保存在class文件的code区。\n在栈帧创建初期，其中的操作数栈是空的。JVM提供了一系列的指令，用于将值压入操作数栈，同时也有指令来从操作数栈中取出值并进行计算，并将计算结果压入操作数栈。比如iadd指令会从操作数栈中取出最顶部的两个int数值，将其相加，然后将结果压入操作数栈。\n压入操作数栈的元素的类型必须与指令的要求严格匹配，比如使用iadd指令将一个float和一个double相加是不允许的，这一点不仅在编译期会被严格确定，在类校验阶段也会进行检查。\n动态链接每个栈帧都包含一个指向运行时常量池的引用，用来支持方法调用过程中的动态链接。\n字节码中的方法调用指令会以常量池中指向方法的符号引用作为参数。这些符号引用一部分会在类加载阶段或者第一次使用时转化成直接引用，这种称为静态解析；另一部分将在每一次运行期间转化为直接引用，这种称为动态链接。\n返回地址当一个方法开始执行后，只有两种方式可以退出这个方法：执行方法返回的指令，和遇到未处理的异常。\n执行方法返回的指令称为“正常方法调用出口(Normal Method Invocation Completion)”，在这种情况下，如果方法有返回值，那么返回值将会被传递到上方的调用者。此时，当前栈帧将被用来恢复调用者的状态，包括调用者的本地变量表和操作数栈，并会修改pc寄存器的值来跳过方法调用指令。\n当方法执行期间遇到了异常，并没有找到对应的异常处理器时，导致的方法返回称为“异常方法调用出口(Abrupt Method Invocation Completion)”，在这种情况下将不会有值被传回上方调用者。\n附加信息虚拟机规范允许具体的JVM实现增加一些规范中没有的信息到栈帧中，比如调试信息等，这些信息的内容将取决于JVM的具体实现。\n","categories":["学知识"],"tags":["Java","JVM"]},{"title":"Java多线程概念","url":"/java-multi-threading-concepts.html","content":"本文记录一些Java多线程相关的概念性的知识。\n\n\n线程的状态\n新建(New)线程已被创建，但是尚未启动\n可运行(Runnable)此线程在JVM中正在运行\n阻塞(Blocked)此线程正在等待获取一个监视锁(monitor lock)，需要其他线程显式唤醒\n等待(Waiting)此线程正在无限期等待另一个线程完成某些工作\n\n\n\n进入方法\n退出方法\n\n\n\nObject#wait()\nObject#notify()或Object#notifyAll()\n\n\nThread#join()\n被调用的线程执行完毕\n\n\n限期等待(Timed waiting)此线程正在有限期等待另一个线程完成某些工作\n\n\n\n进入方法\n退出方法\n\n\n\nThread.sleep()\n设定的休眠时间结束\n\n\nObject#wait(long timeout)\n时间结束 &#x2F; Object#notify() &#x2F; Object#notifyAll()\n\n\nThread#join(long millis)\n时间结束 &#x2F; 被调用的线程执行完毕\n\n\n终止(Terminated)线程结束\n使用线程见Java 如何创建和运行多线程\n互斥同步synchronized同步一个代码块只作用于同一个对象，如多个Thread使用同一个Runnable时。一个线程若要使用此方法，则必须获得obj对象的锁。\npublic void something() &#123;    synchronized (obj) &#123;        // do something    &#125;&#125;\n\n同步一个方法// 只作用于同一个对象。// 一个线程若要使用此方法，则必须获得该方法所在对象的锁public void synchronized something() &#123;    // do something&#125;\n\n// 作用于整个类// 一个线程若要使用此方法，则必须获得该方法所在类的锁public void static synchronized aStaticMethod() &#123;    // do something&#125;\n\n同步一个类作用于整个类，即使两个线程使用同一个类的不同对象，也会进行同步。一个线程若要使用此方法，则必须获得该类的锁。\npublic void something() &#123;    synchronized (SynchronizationExample.class) &#123;        // do something    &#125;&#125;\n\nReentrantLockReentrantLock是java.util.concurrent包中的锁\npublic class LockDemo implements Runnable &#123;    private Lock lock = new ReentrantLock();    @Override    public void run() &#123;        try &#123;            lock.lock();            for (int i = 0; i &lt; 10; i++) &#123;                System.out.println(i + &quot; &quot;);            &#125;        &#125; finally &#123;            lock.unlock();        &#125;    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) &#123;        LockDemo lockDemo = new LockDemo();        ExecutorService executorService = Executors.newCachedThreadPool();        executorService.execute(lockDemo);        executorService.execute(lockDemo);    &#125;&#125;\n\n线程协作Thread#join()在A线程中调用B线程的join()方法，会将当前线程挂起，直到目标线程结束。\npublic class MyRunnable implements Runnable &#123;    private AtomicInteger ticketCount = new AtomicInteger(5);    @Override    public void run() &#123;        System.out.println(Thread.currentThread().getName() + &quot; started.&quot;);        while (true) &#123;            try &#123;                Thread.sleep(500);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            if (ticketCount.get() &gt; 0) &#123;                System.out.println(Thread.currentThread().getName() + &quot; has &quot; + ticketCount.getAndDecrement() + &quot; tickets&quot;);            &#125; else &#123;                break;            &#125;        &#125;        System.out.println(Thread.currentThread().getName() + &quot; stopped.&quot;);    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) throws InterruptedException &#123;        Thread t1 = new Thread(new MyRunnable());        Thread t2 = new Thread(new MyRunnable());        Thread t3 = new Thread(new MyRunnable());        t1.start();        // 主线程等待 t1 执行完毕        t1.join();        t2.start();        t3.start();    &#125;&#125;\n\n执行结果如下：\nThread-0 started.Thread-0 has 5 ticketsThread-0 has 4 ticketsThread-0 has 3 ticketsThread-0 has 2 ticketsThread-0 has 1 ticketsThread-0 stopped.Thread-1 started.Thread-2 started.Thread-1 has 5 ticketsThread-2 has 5 ticketsThread-2 has 4 ticketsThread-1 has 4 ticketsThread-2 has 3 ticketsThread-1 has 3 ticketsThread-2 has 2 ticketsThread-1 has 2 ticketsThread-1 has 1 ticketsThread-2 has 1 ticketsThread-1 stopped.Thread-2 stopped.\n\nwait()，notify()，和notifyAll()wait()将当前线程变为等待状态，notify()和notifyAll()将等待状态的线程唤醒。wait()方法必须在有锁(即synchronized)的代码块中执行。\n当有多个线程处于等待状态时，notify()会任意选择一个线程来唤醒，选择的方式由JVM的实现来决定；而notifyAll()则会唤醒所有等待中的线程。\n因为线程唤醒后，程序将会从wait()的下一条语句中开始执行，所以wait()方法应当总在while循环中调用，通过循环条件控制线程是否继续等待。\npublic class WaitNotifyDemo &#123;    public synchronized void before() &#123;        System.out.println(&quot;before&quot;);        notifyAll();    &#125;    public synchronized void after() &#123;        try &#123;            wait();        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        System.out.println(&quot;after&quot;);    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) throws InterruptedException &#123;        ExecutorService executorService = Executors.newCachedThreadPool();        WaitNotifyDemo waitNotifyDemo = new WaitNotifyDemo();        // 调用after()后，遇到wait()进入等待状态        executorService.execute(waitNotifyDemo::after);        // 调用before()后，遇到`notifyAll()`，唤醒所有线程        executorService.execute(waitNotifyDemo::before);    &#125;&#125;\n\n执行结果：\nbeforeafter","categories":["学知识"],"tags":["Java","多线程"]},{"title":"Java如何创建和运行多线程","url":"/java-multi-threading-how-to.html","content":"本文通过一个简单的示例，介绍一下在Java中如何创建和运行多线程，以及我在学习过程中遇到的问题。包括：\n\n如何实现多线程\n如何在线程间共享资源\n共享资源时可能出现的问题\n\n\n\n多线程的实现方法多线程有三种实现方式：\n\n继承Thread类，并实现其run()方法；\n实现Runnable接口，并实现其run()方法；\n和实现Callable接口，并实现其run()方法。\n\n通常来说，我们会通过实现Runnable接口来实现多线程，因为继承Thread类可能会有多继承的问题，而实现接口则没有这方面的影响。\n下面示例会创建一个MyThread的类来实现，然后在main()中运行。\n继承Thread类public class MyThread extends Thread &#123;    private int ticketCount = 5;    private String threadName;    public MyThread(String threadName) &#123;        this.threadName = threadName;    &#125;    @Override    public void run() &#123;        while (ticketCount &gt; 0) &#123;            System.out.println(threadName + &quot; has &quot; + ticketCount-- + &quot; tickets&quot;);        &#125;    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) &#123;        new MyThread(&quot;thread1&quot;).start();        new MyThread(&quot;thread2&quot;).start();        new MyThread(&quot;thread3&quot;).start();    &#125;&#125;\n\n实现Runnable接口public class MyThread implements Runnable &#123;    private int ticketCount = 5;    private String threadName;    public MyThread(String threadName) &#123;        this.threadName = threadName;    &#125;    @Override    public void run() &#123;        while (ticketCount &gt; 0) &#123;            System.out.println(threadName + &quot; has &quot; + ticketCount-- + &quot; tickets&quot;);        &#125;    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) &#123;        new Thread(new MyThread(&quot;thread1&quot;)).start();        new Thread(new MyThread(&quot;thread2&quot;)).start();        new Thread(new MyThread(&quot;thread3&quot;)).start();    &#125;&#125;\n\n实现Callable接口** TODO: 这东西看起来好像有点复杂，在这里先占个坑，改日单开一篇记录学习过程 **\n执行start()方法与执行run()方法的区别实际上，唯一合法的运行多线程的方式，是调用start()方法，但是为什么不能调用run()方法呢？\n因为start()方法会开辟一个新的线程，并且在新的线程中调用目标的run()方法。但是直接调用run()则不会创建新的线程，而是像调用其他任何一个方法那样，他将会在当前线程中执行。\n这么说可能有些生涩，那么还是通过上面的例子来帮助理解。\n在调用了start()方法后，程序的输出是这样子的，注意观察每行输出是由哪个线程写出来的：\nthread1 has 5 ticketsthread3 has 5 ticketsthread2 has 5 ticketsthread3 has 4 ticketsthread1 has 4 ticketsthread3 has 3 ticketsthread2 has 4 ticketsthread3 has 2 ticketsthread1 has 3 ticketsthread3 has 1 ticketsthread2 has 3 ticketsthread1 has 2 ticketsthread2 has 2 ticketsthread1 has 1 ticketsthread2 has 1 tickets\n\n可见输出是乱序的。然而调用run()方法之后，输出变成了这样：\nthread1 has 5 ticketsthread1 has 4 ticketsthread1 has 3 ticketsthread1 has 2 ticketsthread1 has 1 ticketsthread2 has 5 ticketsthread2 has 4 ticketsthread2 has 3 ticketsthread2 has 2 ticketsthread2 has 1 ticketsthread3 has 5 ticketsthread3 has 4 ticketsthread3 has 3 ticketsthread3 has 2 ticketsthread3 has 1 tickets\n\n看起来像是三个线程按照创建的顺序依次执行，但实际上只是先后调用了它们三个的run()方法而已，并没有新的线程被创建出来。\n多线程共享资源上文中卖票这个例子，都是开了三个线程，各卖各的票，但是实际上它们应该是从同一组票池中卖票。接下来，就把例子修改一下，让这三个线程共享资源。\npublic class MyThread implements Runnable &#123;    private int ticketCount = 20;    private String threadName;    public MyThread(String threadName) &#123;        this.threadName = threadName;    &#125;    @Override    public void run() &#123;        while (ticketCount &gt; 0) &#123;            // Thread.currentThread().getName() 打印出正在执行的线程的名字            System.out.println(Thread.currentThread().getName() + &quot; has &quot; + ticketCount-- + &quot; tickets&quot;);        &#125;    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) &#123;        MyThread myThread = new MyThread(&quot;MyThread&quot;);        Thread t1 = new Thread(myThread);        Thread t2 = new Thread(myThread);        Thread t3 = new Thread(myThread);        t1.start();        t2.start();        t3.start();    &#125;&#125;\n\n为什么用Runnable而不用ThreadThread(Runnable target)的JavaDoc中，target参数的描述是这么写的：\n\nthe object whose run method is invoked when this thread is started\n\n以及Thread#run()是这样写的：\npublic void run() &#123;    if (target != null) &#123;        target.run();    &#125;&#125;\n\n同时run()的JavaDoc有如下描述：\n\nIf this thread was constructed using a separate Runnable run object, then that Runnable object’s run method is called.\n\n说明，在将一个Runnable对象赋给一个或多个Thread后，这些Thread调用的都是这一个Runnable对象的run()方法，所操作的数据也是这一个Runnable对象里面的数据。\n依旧用例子说话。\n在上一节的代码的t1.start()这一行打个断点，看看这三个线程的信息。\n根据上面的JavaDoc，这里特别关注线程的target属性。\n\n可见，这三个Thread都使用了MyThread@534这个对象。也就是说，这三个线程都调用了MyThread@534的run()方法，并且在操作MyThread@534这个对象的成员变量。\n然后，换成继承Thread的形式：\npublic class MyThread extends Thread &#123;    private int ticketCount = 20;    private String threadName;    public MyThread(String threadName) &#123;        this.threadName = threadName;    &#125;    @Override    public void run() &#123;        while (ticketCount &gt; 0) &#123;            System.out.println(Thread.currentThread().getName() + &quot; has &quot; + ticketCount-- + &quot; tickets&quot;);        &#125;    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) &#123;        MyThread t1 = new MyThread(&quot;MyThread1&quot;);        MyThread t2 = new MyThread(&quot;MyThread2&quot;);        MyThread t3 = new MyThread(&quot;MyThread3&quot;);        t1.start();        t2.start();        t3.start();    &#125;&#125;\n\n同样，在t1.start()上打断点，得到结果如下：\n\n可以发现，这三个Thread不止没有target，甚至它们的成员变量都是各自有一份，何谈线程之间共享。\n多线程的同步问题将多线程共享资源这一节的代码执行，得到了这样的输出：\nThread-0 has 20 ticketsThread-2 has 19 ticketsThread-1 has 19 ticketsThread-2 has 17 ticketsThread-0 has 18 ticketsThread-2 has 15 ticketsThread-1 has 16 ticketsThread-2 has 13 ticketsThread-0 has 14 ticketsThread-2 has 11 ticketsThread-1 has 12 ticketsThread-0 has 10 ticketsThread-2 has 9 ticketsThread-0 has 7 ticketsThread-1 has 8 ticketsThread-0 has 5 ticketsThread-2 has 6 ticketsThread-0 has 3 ticketsThread-1 has 4 ticketsThread-0 has 1 ticketsThread-2 has 2 tickets\n\n鞥？第二行和第三行好像不太对劲？线程1和线程2把同一张票重复卖了两次？果然出现了线程的同步问题了。\n发生这个问题的原因是，Java中的自增、自减不是线程安全的。一个自增自减操作，实际上包含了三步：\n\n获取变量当前的值\n为该值加1或减1\n写回新值\n\n那么要解决这个问题，就需要加锁，来保证“读-算-写”这个操作具有原子性，或者使用AtomicInteger类提供的原子操作。\n使用synchronized关键字加锁尝试使用synchronized关键字给run()方法加锁，代码修改如下：\npublic class MyRunnable implements Runnable &#123;    private int ticketCount = 20;    @Override    public synchronized void run() &#123;        System.out.println(Thread.currentThread().getName() + &quot; started.&quot;);        while (true) &#123;            try &#123;                Thread.sleep(500);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            if (ticketCount &gt; 0) &#123;                System.out.println(Thread.currentThread().getName() + &quot; has &quot; + ticketCount-- + &quot; tickets&quot;);            &#125; else &#123;                break;            &#125;        &#125;        System.out.println(Thread.currentThread().getName() + &quot; stopped.&quot;);    &#125;&#125;\n\n运行后得到如下结果：\nThread-0 started.Thread-0 has 20 ticketsThread-0 has 19 ticketsThread-0 has 18 ticketsThread-0 has 17 ticketsThread-0 has 16 ticketsThread-0 has 15 ticketsThread-0 has 14 ticketsThread-0 has 13 ticketsThread-0 has 12 ticketsThread-0 has 11 ticketsThread-0 has 10 ticketsThread-0 has 9 ticketsThread-0 has 8 ticketsThread-0 has 7 ticketsThread-0 has 6 ticketsThread-0 has 5 ticketsThread-0 has 4 ticketsThread-0 has 3 ticketsThread-0 has 2 ticketsThread-0 has 1 ticketsThread-0 stopped.Thread-2 started.Thread-2 stopped.Thread-1 started.Thread-1 stopped.\n\n可见run()方法被Thread-0上锁，被上锁的方法在释放锁前只能被一个线程所访问，Thread-1和Thread-2都在Thread-0执行结束并释放锁后才开始运行，并且也都进行了一次对run()的上锁-释放过程。\n如果只对ticketCount--操作上锁呢？\npublic class MyRunnable implements Runnable &#123;    private int ticketCount = 20;    @Override    public void run() &#123;        System.out.println(Thread.currentThread().getName() + &quot; started.&quot;);        while (true) &#123;            try &#123;                Thread.sleep(500);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            // 拿到了这里，而不是对run方法上锁            synchronized (this) &#123;                if (ticketCount &gt; 0) &#123;                    System.out.println(Thread.currentThread().getName() + &quot; has &quot; + ticketCount-- + &quot; tickets&quot;);                &#125; else &#123;                    break;                &#125;            &#125;        &#125;        System.out.println(Thread.currentThread().getName() + &quot; stopped.&quot;);    &#125;&#125;\n\n执行之后得到了这样的结果：\nThread-0 started.Thread-2 started.Thread-1 started.Thread-0 has 20 ticketsThread-2 has 19 ticketsThread-1 has 18 ticketsThread-0 has 17 ticketsThread-1 has 16 ticketsThread-2 has 15 ticketsThread-0 has 14 ticketsThread-1 has 13 ticketsThread-2 has 12 ticketsThread-1 has 11 ticketsThread-0 has 10 ticketsThread-2 has 9 ticketsThread-1 has 8 ticketsThread-0 has 7 ticketsThread-2 has 6 ticketsThread-2 has 5 ticketsThread-0 has 4 ticketsThread-1 has 3 ticketsThread-2 has 2 ticketsThread-1 has 1 ticketsThread-0 stopped.Thread-2 stopped.Thread-1 stopped.\n\n三个线程在结束休眠后开始竞争锁，得到锁的线程操作了ticketCount，然后释放了锁。\n原子操作这次尝试将ticketCount换成AtomicInteger类型，并且使用AtomicInteger#getAndDecrement()方法进行原子的自减计算，修改后的代码如下：\npublic class MyRunnable implements Runnable &#123;    private AtomicInteger ticketCount = new AtomicInteger(20);    @Override    public void run() &#123;        System.out.println(Thread.currentThread().getName() + &quot; started.&quot;);        while (true) &#123;            try &#123;                Thread.sleep(500);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            if (ticketCount.get() &gt; 0) &#123;                System.out.println(Thread.currentThread().getName() + &quot; has &quot; + ticketCount.getAndDecrement() + &quot; tickets&quot;);            &#125; else &#123;                break;            &#125;        &#125;        System.out.println(Thread.currentThread().getName() + &quot; stopped.&quot;);    &#125;&#125;\n\nmain()方法内容依旧不变，运行之后出现了这样的结果：\nThread-1 started.Thread-2 started.Thread-0 started.Thread-1 has 19 ticketsThread-0 has 18 ticketsThread-2 has 20 ticketsThread-1 has 17 ticketsThread-0 has 16 ticketsThread-2 has 15 ticketsThread-1 has 13 ticketsThread-2 has 12 ticketsThread-0 has 14 ticketsThread-0 has 11 ticketsThread-2 has 10 ticketsThread-1 has 9 ticketsThread-1 has 8 ticketsThread-0 has 6 ticketsThread-2 has 7 ticketsThread-2 has 4 ticketsThread-0 has 3 ticketsThread-1 has 5 ticketsThread-2 has 2 ticketsThread-0 has 1 ticketsThread-1 stopped.Thread-2 stopped.Thread-0 stopped.\n\n虽然没有了脏读，但是线程的执行顺序也无法保证，如果要求线程定序执行，这样就不行了。\n","categories":["学知识"],"tags":["Java","多线程"]},{"title":"Java源码阅读 - ArrayList","url":"/java-read-src-arraylist.html","content":"做技术，不能只知其然而不知其所以然。在知道了工具的原理之后，才能更高效的使用这个工具。在程序的世界里，源码里面没有秘密，看懂了源码，也就看懂了原理。\n这次就来阅读一下ArrayList的源码。\n\n\nArrayList的特性ArrayList有如下几个特性：\n\n底层是一个动态扩容的数组\n它允许存放多个null元素\n允许存放多个重复的元素\n元素在List中的顺序由添加顺序决定\n不是线程安全的\n\n类的声明public class ArrayList&lt;E&gt;    extends AbstractList&lt;E&gt;    implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; ... &#125;\n\n上面代码声明了一个叫ArrayList的泛型类，继承了AbstractList，并实现了List，RandomAccess，Cloneable，Serializable接口。\nAbstractList抽象类提供了一个“骨架”级别的List接口的实现，用来减少实现一个支持随机存储的List的工作量。\nRandomAccess中没有声明任何方法，是一个标记接口(marker interface)，表明了这个类支持快速(通常是O(1)时间复杂度)的随机存取。在遍历一个集合前，可以用instanceof判断这个集合是否实现了RandomAccess，来选择合适的遍历方法。\nCloneable也是一个标记接口，表明了这个类允许使用Object.clone()命令进行属性到属性的复制。\nSerializable也是一个标记接口，表明在这个类上启用Java的序列化功能。\n如何存储数据/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */transient Object[] elementData; // non-private to simplify nested class access/** * The size of the ArrayList (the number of elements it contains). * * @serial */private int size;\n\nelementData数组用来实际存放数据，ArrayList的空间(capacity)对应这个数组的长度(size)。ArrayList实现了自己的序列化(ArrayList#writeObject())和反序列化(ArrayList#readObject())方法，所以加上transient关键字来使elementData不参与Java自带的序列化和反序列化过程。\nsize成员变量记录当前ArrayList中元素的数量。\n构造方法ArrayList有三个构造方法\n\n使用默认大小的ArrayList()\n指定最初大小的ArrayList(int initialCapacity)\n根据一个给定集合来初始化的ArrayList(Collection&lt;? extends E&gt; c)\n\n使用默认大小类中首先指定了默认的大小\n/** * Default initial capacity. */private static final int DEFAULT_CAPACITY = 10;\n\n但是，在它下面，还有这么一个东西：\n/** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;\n\n在最初被构造时，elementData会先指向DEFAULTCAPACITY_EMPTY_ELEMENTDATA，而不是直接创建一个容量为10的数组。\n/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123;    this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;\n\n这样做的好处在于可以更合理的利用空间。试想一下，如果某个场景中需要创建5个ArrayList备用，如果直接就分配好空间的话，那么就会消耗掉至少50个元素所需要的空间。所以Java选择先将elementData指向一个空数组，在向ArrayList中添加数据时，再去创建合适大小的数组。\n指定最初大小/** * Constructs an empty list with the specified initial capacity. * * @param  initialCapacity  the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity *         is negative */public ArrayList(int initialCapacity) &#123;    if (initialCapacity &gt; 0) &#123;        this.elementData = new Object[initialCapacity];    &#125; else if (initialCapacity == 0) &#123;        this.elementData = EMPTY_ELEMENTDATA;    &#125; else &#123;        throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+                                           initialCapacity);    &#125;&#125;\n\n当指定的大小是一个正整数时，Java会创建好对应大小的数组，并将elementData指向这个数组；如果指定的大小为零，那么Java也会将elementData指向一个共享的空数组EMPTY_ELEMENTDATA，注意这个空数组与上文提到的不是同一个；如果指定的大小为负数，则抛出一个异常。\n那么为什么要专门把EMPTY_ELEMENTDATA和DEFAULTCAPACITY_EMPTY_ELEMENTDATA区分出来呢？DEFAULTCAPACITY_EMPTY_ELEMENTDATA的JavaDoc是这么说的：\n\nWe distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when first element is added.我们将它与EMPTY_ELEMENTDATA区分开来，是方便在添加第一个元素时计算要扩张多少空间。\n\n根据给定的集合初始化/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection&#x27;s * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public ArrayList(Collection&lt;? extends E&gt; c) &#123;    elementData = c.toArray();    if ((size = elementData.length) != 0) &#123;        // c.toArray might (incorrectly) not return Object[] (see 6260652)        if (elementData.getClass() != Object[].class)            elementData = Arrays.copyOf(elementData, size, Object[].class);    &#125; else &#123;        // replace with empty array.        this.elementData = EMPTY_ELEMENTDATA;    &#125;&#125;\n\n程序首先试图调用给定集合的Collection#toArray()方法，将集合转换成一个Object[]数组。\n当数组中有元素时，检查elementData的数据类型是否为Object[]类型，如果不是则使用Arrays.copyOf()方法重新复制元素到一个Object[]对象中；而当数组中没有元素时，则重新使elementData指向EMPTY_ELEMENTDATA。\n添加元素当添加元素时，首先会调用ensureCapacityInternal()方法，来保证空间足够。保证有足够空间后，就会向elementData[size]处放置被添加的元素，并且使size加一。\n/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123;    ensureCapacityInternal(size + 1);  // Increments modCount!!    elementData[size++] = e;    return true;&#125;\n\n扩容ensureCapacityInternal()方法用于确保在添加元素时有足够的空间。如果空间不足，则会调用grow()方法扩容。\ngrow()方法会将elementData扩张为当前的1.5倍空间，并使用Arrays.copyOf()方法将元素放入新的数组。\n/** * 确保空间 */private void ensureCapacityInternal(int minCapacity) &#123;    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;/** * 计算扩容目标 */private static int calculateCapacity(Object[] elementData, int minCapacity) &#123;    if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;        return Math.max(DEFAULT_CAPACITY, minCapacity);    &#125;    return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123;    modCount++;    // overflow-conscious code    // 检查目标容量是否大于当前已有容量    if (minCapacity - elementData.length &gt; 0)        grow(minCapacity);&#125;/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * 增加容量，以确保至少可以容纳minCapacity所指定个数的元素 * * @param minCapacity the desired minimum capacity 目标最小容量 */private void grow(int minCapacity) &#123;    // overflow-conscious code    int oldCapacity = elementData.length;    // newCapacity = olcCapacity + (oldCapacity / 2)    int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);    if (newCapacity - minCapacity &lt; 0)        newCapacity = minCapacity;    if (newCapacity - MAX_ARRAY_SIZE &gt; 0)        newCapacity = hugeCapacity(minCapacity);    // minCapacity is usually close to size, so this is a win:    elementData = Arrays.copyOf(elementData, newCapacity);&#125;\n\n删除元素ArrayList提供了两种方式来删除一个元素：根据元素位置(index)删除，和匹配元素删除。\n根据位置删除根据位置删除时，首先会检查给定的位置是否越界。如果没有越界，就会先取出被删除的元素，用来向调用方返回。\n删除元素的方法是将index+1后面的元素重新放在index起始的位置上。可以看出，删除操作的消耗是比较高的。\n在重新排列元素后，数组中最后一个元素将与倒数第二个元素重复。所以还需要将最后一个元素置为null，并将size减一。\n/** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). * * @param index the index of the element to be removed * @return the element that was removed from the list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123;    rangeCheck(index);    modCount++;    E oldValue = elementData(index);    // 计算要移动的元素数量    int numMoved = size - index - 1;    if (numMoved &gt; 0)        System.arraycopy(            // 源            elementData,            // 源位置            index+1,            // 目标            elementData,            // 目标位置            index,            // 要复制的个数            numMoved);    elementData[--size] = null; // clear to let GC do its work    return oldValue;&#125;\n\n匹配元素删除如果向remove()方法提供了一个对象，那么ArrayList会遍历elementData，并会删除第一个与给定对象匹配的元素。\n/** * Removes the first occurrence of the specified element from this list, * if it is present.  If the list does not contain the element, it is * unchanged.  More formally, removes the element with the lowest index * &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt; * (if such an element exists).  Returns &lt;tt&gt;true&lt;/tt&gt; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &lt;tt&gt;true&lt;/tt&gt; if this list contained the specified element */public boolean remove(Object o) &#123;    if (o == null) &#123;        for (int index = 0; index &lt; size; index++)            if (elementData[index] == null) &#123;                fastRemove(index);                return true;            &#125;    &#125; else &#123;        for (int index = 0; index &lt; size; index++)            if (o.equals(elementData[index])) &#123;                fastRemove(index);                return true;            &#125;    &#125;    return false;&#125;/* * Private remove method that skips bounds checking and does not * return the value removed. */private void fastRemove(int index) &#123;    modCount++;    int numMoved = size - index - 1;    if (numMoved &gt; 0)        System.arraycopy(elementData, index+1, elementData, index,                         numMoved);    elementData[--size] = null; // clear to let GC do its work&#125;\n\n缩减容量ArrayList#trimToSize()方法可以将ArrayList的容量缩减至当前元素个数。这个操作需要通过Arrays.copyOf()方法进行，所以成本也是比较高的。\n/** * Trims the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance to be the * list&#x27;s current size.  An application can use this operation to minimize * the storage of an &lt;tt&gt;ArrayList&lt;/tt&gt; instance. */public void trimToSize() &#123;    modCount++;    if (size &lt; elementData.length) &#123;        elementData = (size == 0)          ? EMPTY_ELEMENTDATA          : Arrays.copyOf(elementData, size);    &#125;&#125;\n\nFail fast在会改变elementData大小的方法中，经常会看到类似modCount++这样的操作。那么这个操作的目的是什么呢？\n首先来看看modCount成员变量的JavaDoc是怎么说的。\n/** * The number of times this list has been &lt;i&gt;structurally modified&lt;/i&gt;. * Structural modifications are those that change the size of the * list, or otherwise perturb it in such a fashion that iterations in * progress may yield incorrect results. * * &lt;p&gt;This field is used by the iterator and list iterator implementation * returned by the &#123;@code iterator&#125; and &#123;@code listIterator&#125; methods. * If the value of this field changes unexpectedly, the iterator (or list * iterator) will throw a &#123;@code ConcurrentModificationException&#125; in * response to the &#123;@code next&#125;, &#123;@code remove&#125;, &#123;@code previous&#125;, * &#123;@code set&#125; or &#123;@code add&#125; operations.  This provides * &lt;i&gt;fail-fast&lt;/i&gt; behavior, rather than non-deterministic behavior in * the face of concurrent modification during iteration. * * &lt;p&gt;&lt;b&gt;Use of this field by subclasses is optional.&lt;/b&gt; If a subclass * wishes to provide fail-fast iterators (and list iterators), then it * merely has to increment this field in its &#123;@code add(int, E)&#125; and * &#123;@code remove(int)&#125; methods (and any other methods that it overrides * that result in structural modifications to the list).  A single call to * &#123;@code add(int, E)&#125; or &#123;@code remove(int)&#125; must add no more than * one to this field, or the iterators (and list iterators) will throw * bogus &#123;@code ConcurrentModificationExceptions&#125;.  If an implementation * does not wish to provide fail-fast iterators, this field may be * ignored. */protected transient int modCount = 0;\n\n也就是说，modCount记录了一个List的结构被修改的次数，并且提到了如果在迭代过程中修改了List的结构，那么可能会导致得到错误的结果。\n在迭代或者序列化的过程中，程序会检查modCount的值是否被修改过，如果被修改，就会抛出ConcurrentModificationException异常。比如ArrayList.Itr#next()方法：\n@SuppressWarnings(&quot;unchecked&quot;)public E next() &#123;    checkForComodification();    int i = cursor;    if (i &gt;= size)        throw new NoSuchElementException();    Object[] elementData = ArrayList.this.elementData;    if (i &gt;= elementData.length)        throw new ConcurrentModificationException();    cursor = i + 1;    return (E) elementData[lastRet = i];&#125;final void checkForComodification() &#123;    if (modCount != expectedModCount)        throw new ConcurrentModificationException();&#125;\n\n序列化与反序列化如上文所说，ArrayList实现了自己的序列化与反序列化方法，所以elementData使用transient修饰。\n在序列化时，程序并不是直接序列化elementData这个数组，而是只取出数组中有效的元素(包括null元素)，并逐个序列化每个元素的对象。\n/** * Save the state of the &lt;tt&gt;ArrayList&lt;/tt&gt; instance to a stream (that * is, serialize it). * * @serialData The length of the array backing the &lt;tt&gt;ArrayList&lt;/tt&gt; *             instance is emitted (int), followed by all of its elements *             (each an &lt;tt&gt;Object&lt;/tt&gt;) in the proper order. */private void writeObject(java.io.ObjectOutputStream s)    throws java.io.IOException&#123;    // Write out element count, and any hidden stuff    int expectedModCount = modCount;    s.defaultWriteObject();    // Write out size as capacity for behavioural compatibility with clone()    s.writeInt(size);    // Write out all elements in the proper order.    for (int i=0; i&lt;size; i++) &#123;        s.writeObject(elementData[i]);    &#125;    if (modCount != expectedModCount) &#123;        throw new ConcurrentModificationException();    &#125;&#125;\n\n在反序列化时，首先会使elementData指向EMPTY_ELEMENTDATA，只在有元素会被反序列化时，才会为elementData扩容并逐个反序列化对应的对象。\n/** * Reconstitute the &lt;tt&gt;ArrayList&lt;/tt&gt; instance from a stream (that is, * deserialize it). */private void readObject(java.io.ObjectInputStream s)    throws java.io.IOException, ClassNotFoundException &#123;    elementData = EMPTY_ELEMENTDATA;    // Read in size, and any hidden stuff    s.defaultReadObject();    // Read in capacity    s.readInt(); // ignored    if (size &gt; 0) &#123;        // be like clone(), allocate array based upon size not capacity        int capacity = calculateCapacity(elementData, size);        SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity);        ensureCapacityInternal(size);        Object[] a = elementData;        // Read in all elements in the proper order.        for (int i=0; i&lt;size; i++) &#123;            a[i] = s.readObject();        &#125;    &#125;&#125;\n\n迭代迭代器是用于遍历各个集合的标准方法，ArrayList也不例外，它提供了通过iterator()方法获得的Iterator的迭代器，以及通过listIterator()方法获得的ListItr迭代器。\nIterator迭代器迭代器Iterator接口提供了四个方法：\npublic interface Iterator&lt;E&gt; &#123;    // 检查是否还有下一个元素    boolean hasNext();    // 获取下一个元素    E next();    // 删除一个当前的元素    void remove();    // 使用Lambda表达式进行遍历    forEachRemaining(Consumer&lt;? super E&gt; action);&#125;\n\n在调用ArrayList#iterator()方法后，我们可以得到一个Itr内部类的实例。\n/** * An optimized version of AbstractList.Itr */private class Itr implements Iterator&lt;E&gt; &#123;    // 下一个将被返回的元素的下标    int cursor;    // 上一个被返回的元素的下标; -1 代表尚没有元素被返回    int lastRet = -1;    // 在开始迭代时这个`ArrayList`的`modCount`    // 在迭代操作时，迭代器将反复检查`expectedModCount`与当前`modCount`是否相同，一旦不同就会触发`fail fast`机制。    int expectedModCount = modCount;    Itr() &#123;&#125;    public boolean hasNext() &#123;        // 如果当前迭代位置的下标等于集合的长度        // 则说明迭代已经结束        return cursor != size;    &#125;    @SuppressWarnings(&quot;unchecked&quot;)    public E next() &#123;        // 检查集合的结构是否在迭代过程中被修改过        checkForComodification();        // 取得当前迭代位置的下标        int i = cursor;        // 如果下标超出了集合的长度        // 则抛出异常        if (i &gt;= size)            throw new NoSuchElementException();        // 取得该集合的所有元素        Object[] elementData = ArrayList.this.elementData;        // 如果当前位置超出了集合的长度        // 则说明集合的结构发生了变化        if (i &gt;= elementData.length)            throw new ConcurrentModificationException();        // 将游标向前移动一个位置        cursor = i + 1;        // 将上次返回的下标指向当前位置，并返回该位置的元素        return (E) elementData[lastRet = i];    &#125;    public void remove() &#123;        // 因为该方法删除的是lastRet指向的元素        // 而在未调用next方法前，lastRet是-1        // 所以不允许这样操作        if (lastRet &lt; 0)            throw new IllegalStateException();        // 检查集合的结构是否在迭代过程中被修改过        checkForComodification();        try &#123;            // 调用集合的remove方法删除元素            ArrayList.this.remove(lastRet);            // 因为集合少了一个元素，所以将游标向前移动一个位置            cursor = lastRet;            // 而上次被返回的元素已经没了，            // 所以lastRet指向-1            lastRet = -1;            // 因为ArrayList#remove修改了modCount            // 所以这里同步expectedModCount，确保后续的迭代过程中不会触发fail fast机制            // 所以使用迭代器删除元素是安全的            expectedModCount = modCount;        &#125; catch (IndexOutOfBoundsException ex) &#123;            // 如果在删除操作时发生了数组越界            // 则说明集合的结构被改变了            throw new ConcurrentModificationException();        &#125;    &#125;    @Override    @SuppressWarnings(&quot;unchecked&quot;)    public void forEachRemaining(Consumer&lt;? super E&gt; consumer) &#123;        Objects.requireNonNull(consumer);        final int size = ArrayList.this.size;        int i = cursor;        if (i &gt;= size) &#123;            return;        &#125;        final Object[] elementData = ArrayList.this.elementData;        if (i &gt;= elementData.length) &#123;            throw new ConcurrentModificationException();        &#125;        while (i != size &amp;&amp; modCount == expectedModCount) &#123;            consumer.accept((E) elementData[i++]);        &#125;        // update once at end of iteration to reduce heap write traffic        cursor = i;        lastRet = i - 1;        checkForComodification();    &#125;    final void checkForComodification() &#123;        if (modCount != expectedModCount)            throw new ConcurrentModificationException();    &#125;&#125;\n\nListItr迭代器ArrayList提供了两个方法用来获得一个ListItr迭代器，其区别是有无指定下标。\n/** * Returns a list iterator over the elements in this list (in proper * sequence), starting at the specified position in the list. * The specified index indicates the first element that would be * returned by an initial call to &#123;@link ListIterator#next next&#125;. * An initial call to &#123;@link ListIterator#previous previous&#125; would * return the element with the specified index minus one. * * &lt;p&gt;The returned list iterator is &lt;a href=&quot;#fail-fast&quot;&gt;&lt;i&gt;fail-fast&lt;/i&gt;&lt;/a&gt;. * * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public ListIterator&lt;E&gt; listIterator(int index) &#123;    if (index &lt; 0 || index &gt; size)        throw new IndexOutOfBoundsException(&quot;Index: &quot;+index);    return new ListItr(index);&#125;/** * Returns a list iterator over the elements in this list (in proper * sequence). * * &lt;p&gt;The returned list iterator is &lt;a href=&quot;#fail-fast&quot;&gt;&lt;i&gt;fail-fast&lt;/i&gt;&lt;/a&gt;. * * @see #listIterator(int) */public ListIterator&lt;E&gt; listIterator() &#123;    return new ListItr(0);&#125;\n\nListItr继承了前面提到的Itr类，也就是说它拥有Itr类的所有方法。同时它实现了ListIterator接口。\nListIterator接口继承了Iterator接口。除了Iterator接口中定义的方法外，ListIterator还另外声明了数个方法。\npublic interface ListIterator&lt;E&gt; extends Iterator&lt;E&gt; &#123;    // 在Iterator中存在的方法这里就略过了    // 检查是否有前一个元素    boolean hasPrevious();    // 向前迭代一个元素    E previous();    // 获取next()方法将要迭代到的元素的下标    int nextIndex();    // 获取previous()方法将要迭代到的元素的下标    int previousIndex();    // 把通过next()或者previous()得到的元素修改为传入的数据    void set(E e);    // 将一个新的元素插入到上一个被迭代的元素和下一个被迭代的元素之间    void add(E e);&#125;\n\nArrayList中具体的ListItr的实现是这样子的：\n/** * An optimized version of AbstractList.ListItr */private class ListItr extends Itr implements ListIterator&lt;E&gt; &#123;    // 在构造时直接将cursor指向指定的下标    ListItr(int index) &#123;        super();        cursor = index;    &#125;    // 因为数组首个元素的下标是0，而cursor不会是负数    // 所以当cursor不为0时，前面就有元素可供迭代    public boolean hasPrevious() &#123;        return cursor != 0;    &#125;    // cursor指向下一个被迭代的元素的下标    public int nextIndex() &#123;        return cursor;    &#125;    // cursor减一就得到了前一个元素的下标    public int previousIndex() &#123;        return cursor - 1;    &#125;    // 向前迭代一个元素    @SuppressWarnings(&quot;unchecked&quot;)    public E previous() &#123;        // 检查集合的结构有没有被修改过        checkForComodification();        // 取得前一个元素的下标        int i = cursor - 1;        // 如果前一个元素的下标是负数        // 则抛出NoSuchElementException        if (i &lt; 0)            throw new NoSuchElementException();        // 取到集合的所有元素        Object[] elementData = ArrayList.this.elementData;        // 如果此时下标超出了所有元素的长度        // 则认定集合的结构被外部改变过了        if (i &gt;= elementData.length)            throw new ConcurrentModificationException();        // 将cursor指向前一个元素        cursor = i;        // 取得前一个元素，并重新设定lastRet的值        return (E) elementData[lastRet = i];    &#125;    // 将上一个迭代到的元素修改为指定的数据    public void set(E e) &#123;        // 尚无元素被迭代到的时候        // lastRet是-1        // 此时没有元素可以被修改        if (lastRet &lt; 0)            throw new IllegalStateException();        // 检查集合的结构有没有被修改过        checkForComodification();        try &#123;            // 尝试使用`ArrayList#set`方法修改上一个被迭代位置的值            ArrayList.this.set(lastRet, e);        &#125; catch (IndexOutOfBoundsException ex) &#123;            // 如果发生下标越界，则认定集合的结构被外部修改过            throw new ConcurrentModificationException();        &#125;    &#125;    // 在上一个元素与下一个元素之间插入数据    public void add(E e) &#123;        // 检查集合的结构有没有被修改过        checkForComodification();        try &#123;            // 取得当前迭代的位置            int i = cursor;            // 尝试在当前位置插入一个元素            ArrayList.this.add(i, e);            // 将cursor后移一个位置            cursor = i + 1;            lastRet = -1;            // 因为修改了集合结构后，modCount会改变            // 所以需要同步expectedModCount的值            expectedModCount = modCount;        &#125; catch (IndexOutOfBoundsException ex) &#123;            // 如果在操作过程中发生了下标越界            // 则认定集合的结构被外部修改了            throw new ConcurrentModificationException();        &#125;    &#125;&#125;\n","categories":["学知识"],"tags":["Java","ArrayList"]},{"title":"Java线程池","url":"/java-multi-threading-thread-pool.html","content":"就像数据库连接可以使用连接池管理一样，Java中的线程也可以使用线程池来管理。本文介绍在Java中如何使用线程池，以及有哪些线程池。\n\n\n为什么需要线程池每个线程的创建和销毁，都会消耗一定的系统资源，尤其在高并发的系统中，频繁创建和销毁线程会造成大量的资源浪费。\n那么，为了避免频繁的创建和销毁线程，就可以在系统启动时，预先创建好一定数量的线程，并将其交由线程调度器管理，这就是线程池。\n怎么使用线程池依旧是用一个示例来演示。\npublic class MyRunnable implements Runnable &#123;    private int ticketCount = 20;    @Override    public void run() &#123;        System.out.println(Thread.currentThread().getName() + &quot; started.&quot;);        while (ticketCount &gt; 0) &#123;            try &#123;                Thread.sleep(500);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            synchronized (gthis) &#123;                if (ticketCount &gt; 0) &#123;                    System.out.println(Thread.currentThread().getName() + &quot; has &quot; + ticketCount-- + &quot; tickets&quot;);                &#125; else &#123;                    break;                &#125;            &#125;        &#125;        System.out.println(Thread.currentThread().getName() + &quot; stopped.&quot;);    &#125;&#125;\n\npublic class Main &#123;    public static void main(String[] args) &#123;        // 创建一个线程池        ExecutorService executorService = Executors.newCachedThreadPool();        System.out.println(&quot;Thread pool created&quot;);        MyRunnable myRunnable = new MyRunnable();        System.out.println(&quot;Assigning jobs to thread pool&quot;);        // 向线程池提交任务        executorService.exeute(myRunnable);        executorService.execute(myRunnable);        // 在所有线程都完成工作后，线程池会继续等待新的工作任务        // 所以如果需要程序在完成后退出，需要显式关闭线程池        executorService.shutdown();        while (!executorService.isTerminated()) &#123; &#125;        System.out.println(&quot;Thread pool is down&quot;);    &#125;&#125;\n\n运行后得到如下结果：\nThread pool createdAssigning jobs to thread poolpool-1-thread-2 started.pool-1-thread-1 started.pool-1-thread-1 has 20 ticketspool-1-thread-2 has 19 ticketspool-1-thread-1 has 18 ticketspool-1-thread-2 has 17 ticketspool-1-thread-1 has 16 ticketspool-1-thread-2 has 15 ticketspool-1-thread-1 has 14 ticketspool-1-thread-2 has 13 ticketspool-1-thread-2 has 12 ticketspool-1-thread-1 has 11 ticketspool-1-thread-1 has 10 ticketspool-1-thread-2 has 9 ticketspool-1-thread-1 has 8 ticketspool-1-thread-2 has 7 ticketspool-1-thread-2 has 6 ticketspool-1-thread-1 has 5 ticketspool-1-thread-2 has 4 ticketspool-1-thread-1 has 3 ticketspool-1-thread-2 has 2 ticketspool-1-thread-1 has 1 ticketspool-1-thread-1 stopped.pool-1-thread-2 stopped.Thread pool is down\n\n几种线程池的简介Single thread pool是用Executors.newSingleThreadExecutor()创建。\n该线程池仅有一个线程，并拥有一个无边界的队列。提交到队列中的任务将会按顺序执行。如果当前线程在执行过程中出现错误而被终止，那么线程池会创建一个新的线程并继续执行队列中后续的任务。\n与使用Executors.newFixedThreadPool(1)创建的线程池不同的是，我们不能为single thread pool分配更多的线程数。\nFixed thread pool使用Executors.newFixedThreadPool(int nThreads)创建。\n该线程池维护着固定数量的线程(nThreads个)，在任何时间只允许最多nThreads个线程执行任务，多出来的任务将会在队列中等待，直到有空闲的线程出现。如果其中一个线程在执行过程中因为错误而异常退出，则线程池会立刻创建一个新的线程并执行后续的任务。\n该线程池在显式关闭(ExecutorService#shutdown)前将一直存在。\nWork stealing pool使用Executors.newWorkStealingPool(int parallelism)或Executors.newWorkStealingPool()创建。\n该线程池无法保证各个被提交的任务将会以何种顺序执行。\nnewWorkStealingPool(int parallelism)该方法将根据给定的“并行量(parallelism)”，来创建一个包含足够数量线程的线程池，并会使用多个队列来减少线程与队列的争抢。\n“并行量”的值对应于最多允许参与执行任务的线程数量。但实际存在的线程数可能会动态的增减。\nExecutors.newWorkStealingPool()将所有的“可用的处理器”的数目作为“并行量”来创建线程池。\n可用的处理器数量使用Runtime.getRuntime().availableProcessors()获取，其值等同于CPU中逻辑处理器的数量。\nCached thread pool使用Executors.newCachedThreadPool()创建。\n当接收到新的任务后，线程池会根据有无可用线程，来决定使用线程池中的空闲线程，或者在线程池中创建新的线程。\n如果线程池中有线程空置超过60秒，则该线程就会被终止并从线程池中移除。\nScheduled thread pool使用Executors.newScheduledThreadPool(int corePoolSize)创建。corePoolSize为线程池中保持的线程数。\n该线程池可以指定一个延迟，或指定一个周期，并按照这个计划执行任务。\n线程池是如何工作的那么，在调用ExecutorService#execute(Runnable)之后，线程池究竟做了些什么呢？\n在ThreadPoolExecutor#execute(Runnable)方法中，有这样一段注释：\n\nProceed in 3 steps:\n\nIf fewer than corePoolSize threads are running, try tostart a new thread with the given command as its firsttask.  The call to addWorker atomically checks runState andworkerCount, and so prevents false alarms that would addthreads when it shouldn’t, by returning false.\nIf a task can be successfully queued, then we still needto double-check whether we should have added a thread(because existing ones died since last checking) or thatthe pool shut down since entry into this method. So werecheck state and if necessary roll back the enqueuing ifstopped, or start a new thread if there are none.\nIf we cannot queue task, then we try to add a newthread.  If it fails, we know we are shut down or saturatedand so reject the task.\n\n\n也就是说：\n\n\nflowchart TD\ncheck_running_threads_number{检查正在运行的线程数是否小于corePoolSize};\nstart_new_thread(&quot;调用addWorker尝试开启一个新的线程，并将这个runnable作为第一个task交给这个线程&quot;);\ntry_enqueue_the_task{&quot;尝试将任务加入队列，并重新调用addWorker再次检查能否加入新的worker&quot;};\nreject_the_task(&quot;此时线程池已经饱和，或者正在被关闭，所以拒绝掉这个task&quot;);\n\n开始 --&gt; check_running_threads_number\ncheck_running_threads_number -- 是 --&gt; start_new_thread\nstart_new_thread -- 成功 --&gt; 结束\nstart_new_thread -- 失败 --&gt; try_enqueue_the_task\ncheck_running_threads_number -- 否 --&gt; try_enqueue_the_task\ntry_enqueue_the_task -- 成功 --&gt; 结束\ntry_enqueue_the_task -- 失败 --&gt; reject_the_task\nreject_the_task --&gt; 结束\n\n\n\n这里要注意一个比较容易引起误解的点，就是在core pool size &lt; 当前正在运行的线程数量 &lt; max pool size时，如果有新的任务进来，那么这个任务首先会被放入队列。仅当队列满了的时候，线程池才会为这个新来的任务创建新的线程。\n参考文章Thread pools and work queuesCore pool size vs maximum pool size in ThreadPoolExecutor - StackOverflow\n","categories":["学知识"],"tags":["Java","多线程"]},{"title":"Java源码阅读 - HashMap","url":"/java-read-src-hashmap.html","content":"做技术，不能只知其然而不知其所以然。在知道了工具的原理之后，才能更高效的使用这个工具。在程序的世界里，源码里面没有秘密，看懂了源码，也就看懂了原理。\n这次就来阅读一下HashMap的源码。\n\n\nHashMap的特性HashMap有如下的特性：\n\nHashMap是根据键值对来存储数据的，多个数据之间的键不能重复。在键重复时，旧的数据将会被覆盖\nHashMap中各个数据实际存放的位置与hashCode()方法的结果有关，但不是由其结果直接决定\nHashMap只允许一个键是null(因为存储多个键是null的数据就违反了第一条特性)，但是允许多个值是null的数据\nHashMap中数据存储的位置是不确定的，并且可能会因为扩容而改变，所以它的遍历顺序是不确定的\nHashMap不是线程安全的，如果需要线程安全性则可以使用ConcurrentHashMap\n\n类的声明public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt;    implements Map&lt;K,V&gt;, Cloneable, Serializable\n\n上面代码声明了一个名为HashMap的泛型类，它继承了AbstractMap，并实现了Map，Cloneable，Serializable接口。\nAbstractMap是一个抽象类，它是一个骨架级的Map实现，来减少实现一个Map所需的工作量。\nMap接口顾名思义，它定义了要实现一个Map时必须实现的方法。\n一些关键的常量和概念在深入了解HashMap前，有一些关键的概念我们需要知道：\n\n哈希桶(bucket&#x2F;bin)：一个数组元素中存放的链表，就是一个哈希桶\n哈希表：即存放了各个哈希桶的数组\n树化阈值：当一个桶的大小超过了树化阈值之后才会将其变成红黑树\n非树化阈值：当一个已经变成红黑树的桶中节点数量小于该值时，这个红黑树会被变回链表\n最小树化容量：在选择是否将一个链表变成红黑树时，除了会考虑链表长度外，还会考虑哈希表的长度。仅当哈希表长度超过最小树化容量，且某个链表长度超过树化阈值时，这个链表才会被变成红黑树\n\n与之对应的有这几个常量值：\n// 树化阈值static final int TREEIFY_THRESHOLD = 8;// 非树化阈值static final int UNTREEIFY_THRESHOLD = 6;// 最小树化容量static final int MIN_TREEIFY_CAPACITY = 64;\n\n此外HashMap还针对哈希表的扩容定义了一系列的常量和变量：\n// 默认初始容量。为了保证添加和查找的高效性，HashMap的容量总是2的幂次static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16// 最大容量static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 默认载荷因子。// 载荷因子是哈希表在其容量自动增加之前被允许获得的最大数量的度量，决定了哈希表何时扩容static final float DEFAULT_LOAD_FACTOR = 0.75f;// 扩容阈值。它的值等于哈希表容量乘以载荷因子int threshold;// 实际的载荷因子final float loadFactor;\n\n如何存储数据HashMap存储数据的方式有两种，而这两种方式也正是Java 1.7和Java 8的分界线，因为Java 8对于HashMap进行了底层上的改动。\nJava 1.7之前因为HashMap是依靠hashCode()方法的结果来决定元素存储的位置的，而再完美的哈希函数也无法避免哈希碰撞的出现，所以HashMap选择采用拉链法(也叫链地址法)来存储数据。\n链地址法是一种结合了数组和链表的存储方式，在每个数组元素中存储的都是一个链表，这些链表被称为桶(bucket/bin)。\n为了直观的展示，这里借用一下参考文章1[^1]中的一幅图：\n\n我们都知道，一个数组元素只能保存一个数据，但是多个数据经过哈希运算后可能得到相同的哈希值，所以HashMap会将哈希值相同的数据存放在相同数组位置中的一个链表中。而在取出元素时，HashMap首先会根据哈希值找到数组中的位置，然后遍历其中的链表来找到数据。\nJava 8之后在一个HashMap存储越来越多的数据之后，数据之间发生哈希碰撞的可能性也会越来越大，导致每个数组中的链表也会越来越长，而因为遍历链表操作的时间复杂度是O(n)，所以链表越长，遍历的效率就越差。所以在Java 8中，当数组长度大于MIN_TREEIFY_CAPACITY，且某个链表长度大于TREEIFY_THRESHOLD时，这个链表将会被转换成红黑树。\n这里依旧借用参考文章1[^1]中的一幅图：\n\n数据的存储单元HashMap中定义了一个Node&lt;K,V&gt;型的数组table用于存储数据：\n// 这个就是哈希表transient Node&lt;K,V&gt;[] table;\n\n分别针对树化前和树化后的数据，HashMap定义了不同的内部类作为其数据的存储单元。\n树化前HashMap中定义了一个内部类Node，作为链表中各个元素的存储单元。\n/** * Basic hash bin node, used for most entries.  (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;    // 这个节点的哈希，即数组的索引位置    final int hash;    // 这个节点所属的key    final K key;    // 这个节点的值    V value;    // 链表中的下一个元素    Node&lt;K,V&gt; next;    Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123;        this.hash = hash;        this.key = key;        this.value = value;        this.next = next;    &#125;    public final K getKey()        &#123; return key; &#125;    public final V getValue()      &#123; return value; &#125;    public final String toString() &#123; return key + &quot;=&quot; + value; &#125;    // 节点的哈希值通过将key的哈希和value的哈希异或得到    public final int hashCode() &#123;        return Objects.hashCode(key) ^ Objects.hashCode(value);    &#125;    // 替换这个节点的数据    public final V setValue(V newValue) &#123;        V oldValue = value;        value = newValue;        return oldValue;    &#125;    public final boolean equals(Object o) &#123;        if (o == this)            return true;        if (o instanceof Map.Entry) &#123;            Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o;            if (Objects.equals(key, e.getKey()) &amp;&amp;                Objects.equals(value, e.getValue()))                return true;        &#125;        return false;    &#125;&#125;\n\n树化后针对树化后的红黑树，HashMap定义了一个内部类TreeNode作为树中各个元素的存储单元。但是这个类的代码太长了，放在这里不太合适，后面我再单独开一篇博文专门给它。\n构造方法HashMap提供了四个构造方法，我们下面一个一个来看：\n可以指定容量和载荷因子的构造方法/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param  initialCapacity the initial capacity * @param  loadFactor      the load factor * @throws IllegalArgumentException if the initial capacity is negative *         or the load factor is nonpositive */public HashMap(int initialCapacity, float loadFactor) &#123;    // 初始容量不能小于0    if (initialCapacity &lt; 0)        throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; +                                            initialCapacity);    // 初始容量不能大于最大允许容量    if (initialCapacity &gt; MAXIMUM_CAPACITY)        initialCapacity = MAXIMUM_CAPACITY;    // 载荷因子必须大于等于0，且不能为无穷大(比如0.0f/0.0f)    if (loadFactor &lt;= 0 || Float.isNaN(loadFactor))        throw new IllegalArgumentException(&quot;Illegal load factor: &quot; +                                            loadFactor);    this.loadFactor = loadFactor;    this.threshold = tableSizeFor(initialCapacity);&#125;// 根据期望容量返回一个大于等于cap的扩容阈值，并且保证扩容阈值一定是2的幂次static final int tableSizeFor(int cap) &#123;    int n = cap - 1;    n |= n &gt;&gt;&gt; 1;    n |= n &gt;&gt;&gt; 2;    n |= n &gt;&gt;&gt; 4;    n |= n &gt;&gt;&gt; 8;    n |= n &gt;&gt;&gt; 16;    return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125;\n\n虽然上面说扩容阈值 = 哈希表容量 * 加载因子，但是有没有发现，上面的构造方法里面其实并没有初始化table？实际上，table在第一次添加数据时才会被初始化，具体的操作我们放到后面再说。\n可以指定容量的构造方法/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and the default load factor (0.75). * * @param  initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */public HashMap(int initialCapacity) &#123;    this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;\n\n这个构造方法就是把默认载荷因子和给定的初始容量传给上面说的那个构造方法，这里就不重复解释了。\n无参构造方法/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */public HashMap() &#123;    // 使用默认的载荷因子    this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;\n\n新增数据我们知道，HashMap既可以一次只新增一条数据，也可以一次新增多个数据。我们先看它是怎么新增单条数据的。\n新增单条数据public V put(K key, V value) &#123;    return putVal(hash(key), key, value, false, true);&#125;\n\n乍一看好像很简单的样子，一句话轻飘飘的完成了新增数据的任务。但是要展开看的话，信息量可就很大了。\n我们从里面到外面一个一个的看。\n计算新元素的哈希值在上面提到的putVal方法中，第一个参数是这个数据的哈希值。那么这个哈希值是怎么计算出来的呢？在java 8中，hash方法是这么实现的：\nstatic final int hash(Object key) &#123;    int h;    // 将key的hashCode与其无符号右移16位之后得到的值做一次异或运算    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;\n\n上面这段代码会对key的hashCode做一个扰动计算，来得到这个key在HashMap中的哈希值。这个扰动计算的目的就是为了降低发生哈希碰撞的可能性。\n向HashMap中增加数据在计算完key的哈希值后，putVal方法会开始向HashMap中添加数据。\n// hash就是key的哈希// key就是key值// value就是被添加的数据// onlyIfAbsent如果是true则不替换数据// evict如果是false，则说明是在初始化状态final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123;    // 将指向哈希表    Node&lt;K,V&gt;[] tab;    // 在计算完本次要操作的哈希表下标后    // p将指向哈希表的这个下标中的数据    Node&lt;K,V&gt; p;    // n将是哈希表的长度    // 在计算完本次要操作的哈希表下标后，i将是这个下标值    int n, i;    // 先检查哈希表是不是还没有被初始化过，或者哈希表长度为0    if ((tab = table) == null || (n = tab.length) == 0)        // 对哈希表进行首次扩容，即初始化        n = (tab = resize()).length;    // 经过i = (n - 1) &amp; hash这步运算得到本次插入的位置，即哈希表的数组下标    // 如果这个位置尚没有元素，说明没有发生哈希碰撞    // 那么就直接将插入的数据放在这个位置    if ((p = tab[i = (n - 1) &amp; hash]) == null)        tab[i] = newNode(hash, key, value, null);    // 如果这个位置已经有元素存在了，那就说明发生了哈希碰撞    else &#123;        Node&lt;K,V&gt; e; K k;        // 如果哈希相同，且key值相同，则覆盖这个元素        if (p.hash == hash &amp;&amp;            ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))            e = p;        // 如果哈希表的这个位置已经被变成红黑树了        // 那么就要调用红黑树版本的putVal，即putTreeVal来完成插入操作        else if (p instanceof TreeNode)            e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);        // 在哈希相同，key却不同的时候        else &#123;            // 循环整个单链表，同时使用binCount记录单链表的节点数量            for (int binCount = 0; ; ++binCount) &#123;                // 在单链表尾部拼接本次插入的数据                if ((e = p.next) == null) &#123;                    p.next = newNode(hash, key, value, null);                    // 如果单链表的节点数量大于等于树化阈值时，就将这个单链表进行树化                    if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st                        treeifyBin(tab, hash);                    break;                &#125;                // 如果在遍历过程中发现有数据的key与本次操作的key相同                // 此时e指向要被替换value的节点，并结束遍历                if (e.hash == hash &amp;&amp;                    ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                    break;                p = e;            &#125;        &#125;        // 如果e != null，说明需要替换e所指节点的数据        if (e != null) &#123; // existing mapping for key            V oldValue = e.value;            if (!onlyIfAbsent || oldValue == null)                e.value = value;            // 这个方法在HashMap中是空实现            // 但是LinkedHashMap中会有实现            afterNodeAccess(e);            return oldValue;        &#125;    &#125;    // fail-fast机制    ++modCount;    // 如果哈希表长度大于扩容阈值，则对哈希表扩容    if (++size &gt; threshold)        resize();    // 这个方法在HashMap中也是空的    afterNodeInsertion(evict);    return null;&#125;\n\n上面代码可能看起来比较费劲，这里借用美团博客的一张图来展示put方法的执行流程：\n\n新增多条数据public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123;    putMapEntries(m, true);&#125;\n\n依旧是调用了另一个方法实现的添加数据。那么继续深入进去看看。\nfinal void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123;    // 传入的map的长度    int s = m.size();    // 如果传入的map有数据，才进行后面的运算    if (s &gt; 0) &#123;        // 如果哈希表尚未初始化，则先计算扩容阈值        if (table == null) &#123; // pre-size            float ft = ((float)s / loadFactor) + 1.0F;            int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ?                        (int)ft : MAXIMUM_CAPACITY);            if (t &gt; threshold)                threshold = tableSizeFor(t);        &#125;        // 如果哈希表已经初始化完成，但是传入的map的大小超过了扩容阈值        // 那么就将哈希表扩容        else if (s &gt; threshold)            resize();        // 遍历传入的map，然后逐个调用putVal方法增加元素        for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123;            K key = e.getKey();            V value = e.getValue();            putVal(hash(key), key, value, false, evict);        &#125;    &#125;&#125;\n\nHashMap扩容上面多次提到了HashMap的扩容操作，这里我们就详细看看它是怎么扩容的。\nfinal Node&lt;K,V&gt;[] resize() &#123;    // oldTab指向扩容前的哈希表    Node&lt;K,V&gt;[] oldTab = table;    // oldCap是扩容前的哈希表容量，如果哈希表尚未被初始化，那么容量就是0    int oldCap = (oldTab == null) ? 0 : oldTab.length;    // oldThr是扩容前的扩容阈值    int oldThr = threshold;    // newCap为扩容后的容量，newThr是扩容后的扩容阈值    int newCap, newThr = 0;    if (oldCap &gt; 0) &#123;        // 如果扩容前的哈希表容量已经是最大容量        if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;            // 那么就将扩容阈值设为Integer.MAX_VALUE            threshold = Integer.MAX_VALUE;            // 并停止扩容            return oldTab;        &#125;        // 新容量是旧容量的2倍，且新的扩容阈值也是旧扩容阈值的2倍        else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp;                    oldCap &gt;= DEFAULT_INITIAL_CAPACITY)            newThr = oldThr &lt;&lt; 1; // double threshold    &#125;    // 如果旧的扩容阈值大于0    // 而同时旧的容量等于0    // 那么说明已经使用带参数的构造方法设定了载荷因子和初始容量    // 而且这次是首次扩容    // 此时初始容量不等于期望容量，且大于期望容量    else if (oldThr &gt; 0) // initial capacity was placed in threshold        // 设定新的容量等于旧的扩容阈值        newCap = oldThr;    // 如果旧的扩容阈值是0    // 而同时旧的容量等于0    // 那么说明这个HashMap是用默认构造方法初始化的    // 而且这次是首次扩容    else &#123;               // zero initial threshold signifies using defaults        // 那么新的容量就等于默认初始容量        newCap = DEFAULT_INITIAL_CAPACITY;        // 新的扩容阈值等于(默认载荷因子 * 默认初始容量)        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);    &#125;    // 如果新的扩容阈值是0    // 对应当前table为空，但是有阈值的情况    // 那么就计算新的扩容阈值    if (newThr == 0) &#123;        float ft = (float)newCap * loadFactor;        newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ?                    (int)ft : Integer.MAX_VALUE);    &#125;    // 更新HashMap的扩容阈值    threshold = newThr;    // 使用扩容后的容量创建一个新的哈希表    @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;)    Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];    // 将HashMap的哈希表指向新的哈希表    table = newTab;    // 如果旧的哈希表不为null    // 则进行重新插入操作    if (oldTab != null) &#123;        // 遍历旧的哈希表        for (int j = 0; j &lt; oldCap; ++j) &#123;            // 存储旧的哈希表对应位置中链表的头节点            Node&lt;K,V&gt; e;            // 如果这个位置有数据            if ((e = oldTab[j]) != null) &#123;                // 释放掉旧的链表中的空间                oldTab[j] = null;                // 如果这个链表中只有一个节点                if (e.next == null)                    // 则使用新哈希表的长度计算下标，并插入                    newTab[e.hash &amp; (newCap - 1)] = e;                // 如果这个节点是个红黑树                // 那么需要选择是拆分这个红黑树，或者将其非树化                else if (e instanceof TreeNode)                    ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap);                // 如果这个链表有数个节点                else &#123; // preserve order                    // 因为扩容后的容量是扩容前容量的2倍                    // 所以原链表上的节点，既有可能会被放在它原来的位置上(低位)                    // 也有可能会被放到扩容后新增加的位置上(高位 = 低位 + 旧的容量)                    // 低位链表的头节点和尾节点                    Node&lt;K,V&gt; loHead = null, loTail = null;                    // 高位链表的头节点和尾节点                    Node&lt;K,V&gt; hiHead = null, hiTail = null;                    // 存放原来链表中的节点                    Node&lt;K,V&gt; next;                    // 遍历旧的链表                    do &#123;                        next = e.next;                        // 利用哈希值和旧的容量进行与运算                        // 如果结果等于0，那么就拼接到低位链表的末尾                        if ((e.hash &amp; oldCap) == 0) &#123;                            if (loTail == null)                                loHead = e;                            else                                loTail.next = e;                            loTail = e;                        &#125;                        // 否则拼接到高位链表的末尾                        else &#123;                            if (hiTail == null)                                hiHead = e;                            else                                hiTail.next = e;                            hiTail = e;                        &#125;                    &#125; while ((e = next) != null);                    // 如果低位链表非空，则将链表放到原位置                    if (loTail != null) &#123;                        loTail.next = null;                        newTab[j] = loHead;                    &#125;                    // 如果高位链表非空，则将链表放到新增加的位置                    if (hiTail != null) &#123;                        hiTail.next = null;                        newTab[j + oldCap] = hiHead;                    &#125;                &#125;            &#125;        &#125;    &#125;    return newTab;&#125;\n\n查询数据public V get(Object key) &#123;    Node&lt;K,V&gt; e;    return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;public V getOrDefault(Object key, V defaultValue) &#123;    Node&lt;K,V&gt; e;    return (e = getNode(hash(key), key)) == null ? defaultValue : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123;    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k;    // 检查哈希表是不是空    // 以及哈希表中对应下标的第一个元素是不是null，即这个位置是否有节点    if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;        (first = tab[(n - 1) &amp; hash]) != null) &#123;        // 因为第一个节点无所谓是链表节点还是红黑树节点        // 所以先用一个简单的办法检查第一个节点是不是要找的节点        if (first.hash == hash &amp;&amp; // always check first node            ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))            // 如果是，就直接返回            return first;        // 如果第一个节点不是要找的节点        // 而且它后面有别的节点        if ((e = first.next) != null) &#123;            // 如果后面的节点是红黑树，那么调用红黑树的方法取寻找            if (first instanceof TreeNode)                return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);            // 否则遍历这个链表，寻找符合要求的节点并返回            do &#123;                if (e.hash == hash &amp;&amp;                    ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                    return e;            &#125; while ((e = e.next) != null);        &#125;    &#125;    return null;&#125;\n\n删除数据// 匹配key来删除public V remove(Object key) &#123;    Node&lt;K,V&gt; e;    return (e = removeNode(hash(key), key, null, false, true)) == null ?        null : e.value;&#125;// 同时匹配key和value来删除public boolean remove(Object key, Object value) &#123;    return removeNode(hash(key), key, value, true, true) != null;&#125;final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123;    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index;    // 检查哈希表是不是空    // 以及哈希表中对应下标的第一个元素是不是null，即这个位置是否有节点    if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;        (p = tab[index = (n - 1) &amp; hash]) != null) &#123;        Node&lt;K,V&gt; node = null, e; K k; V v;        // 先检查链表中第一个元素是否匹配        // 如果匹配就直接取出来        if (p.hash == hash &amp;&amp;            ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))            node = p;        // 否则继续向后遍历        else if ((e = p.next) != null) &#123;            if (p instanceof TreeNode)                // 如果后续的节点是红黑树，那么使用红黑树的方法寻找匹配的节点                node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key);            else &#123;                // 否则遍历链表，根据hash和key寻找节点                do &#123;                    if (e.hash == hash &amp;&amp;                        ((k = e.key) == key ||                            (key != null &amp;&amp; key.equals(k)))) &#123;                        node = e;                        break;                    &#125;                    p = e;                &#125; while ((e = e.next) != null);            &#125;        &#125;        // 如果取到了节点，则开始删除        // (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))构成了一个判断链条        if (node != null &amp;&amp; (!matchValue || (v = node.value) == value ||                                (value != null &amp;&amp; value.equals(v)))) &#123;            if (node instanceof TreeNode)                ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable);            else if (node == p)                tab[index] = node.next;            else                p.next = node.next;            ++modCount;            --size;            afterNodeRemoval(node);            return node;        &#125;    &#125;    return null;&#125;\n\n迭代HashMapHashMap提供了多种迭代的方式，比如迭代EntrySet，或者迭代KeySet。\n迭代KeySet在迭代KeySet的时候，我们可以逐个得到HashMap中的key，然后根据key来进行操作。\n// 返回一个KeySet实例public Set&lt;K&gt; keySet() &#123;    Set&lt;K&gt; ks = keySet;    if (ks == null) &#123;        ks = new KeySet();        keySet = ks;    &#125;    return ks;&#125;final class KeySet extends AbstractSet&lt;K&gt; &#123;    // 得到KeySet的长度，也是HashMap的长度    public final int size()                 &#123; return size; &#125;    // 可以清空这个HashMap    public final void clear()               &#123; HashMap.this.clear(); &#125;    // 得到一个KeyIterator迭代器    public final Iterator&lt;K&gt; iterator()     &#123; return new KeyIterator(); &#125;    // 检查是否包含某个key    public final boolean contains(Object o) &#123; return containsKey(o); &#125;    // 根据key删除某个节点    public final boolean remove(Object key) &#123;        return removeNode(hash(key), key, null, false, true) != null;    &#125;    public final Spliterator&lt;K&gt; spliterator() &#123;        return new KeySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0);    &#125;    public final void forEach(Consumer&lt;? super K&gt; action) &#123;        Node&lt;K,V&gt;[] tab;        if (action == null)            throw new NullPointerException();        if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123;            int mc = modCount;            for (int i = 0; i &lt; tab.length; ++i) &#123;                for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next)                    action.accept(e.key);            &#125;            if (modCount != mc)                throw new ConcurrentModificationException();        &#125;    &#125;&#125;final class KeyIterator extends HashIterator implements Iterator&lt;K&gt; &#123;    // 得到下一个节点的key    public final K next() &#123; return nextNode().key; &#125;&#125;\n\n迭代EntrySet在迭代EntrySet的时候，我们可以同时得到一个节点的key和value。\n// 返回一个EntrySet实例public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123;    Set&lt;Map.Entry&lt;K,V&gt;&gt; es;    return (es = entrySet) == null ? (entrySet = new EntrySet()) : es;&#125;final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123;    // 得到EntrySet的大小，即HashMap的大小    public final int size()                 &#123; return size; &#125;    // 可以清空这个HashMap    public final void clear()               &#123; HashMap.this.clear(); &#125;    // 得到一个EntryIterator迭代器    public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123;        return new EntryIterator();    &#125;    // 检查EntrySet中是否包含某个Entry    public final boolean contains(Object o) &#123;        if (!(o instanceof Map.Entry))            return false;        Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;) o;        Object key = e.getKey();        Node&lt;K,V&gt; candidate = getNode(hash(key), key);        return candidate != null &amp;&amp; candidate.equals(e);    &#125;    // 根据传入的Entry在HashMap中匹配并删除对应的节点    public final boolean remove(Object o) &#123;        if (o instanceof Map.Entry) &#123;            Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;) o;            Object key = e.getKey();            Object value = e.getValue();            return removeNode(hash(key), key, value, true, true) != null;        &#125;        return false;    &#125;    public final Spliterator&lt;Map.Entry&lt;K,V&gt;&gt; spliterator() &#123;        return new EntrySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0);    &#125;    public final void forEach(Consumer&lt;? super Map.Entry&lt;K,V&gt;&gt; action) &#123;        Node&lt;K,V&gt;[] tab;        if (action == null)            throw new NullPointerException();        if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123;            int mc = modCount;            for (int i = 0; i &lt; tab.length; ++i) &#123;                for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next)                    action.accept(e);            &#125;            if (modCount != mc)                throw new ConcurrentModificationException();        &#125;    &#125;&#125;final class EntryIterator extends HashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123;    // 得到下一个节点的Entry    public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125;&#125;\n\nHashIterator为什么上面看到KeyIterator和EntryIterator就停止了呢？因为它们两个都是继承于HashIterator，这里我们集中看一下。\nabstract class HashIterator &#123;    Node&lt;K,V&gt; next;        // 下一个要返回的Entry    Node&lt;K,V&gt; current;     // 当前的Entry    int expectedModCount;  // 结合modCount实现fast-fail机制    int index;             // 当前哈希表的下标    HashIterator() &#123;        // 取迭代时的modCount        expectedModCount = modCount;        // 指向当前的哈希表        Node&lt;K,V&gt;[] t = table;        current = next = null;        index = 0;        // 从哈希表中第一个不为空的位置获取第一个Entry        if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry            do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null);        &#125;    &#125;    // 返回是否还有节点可供迭代    public final boolean hasNext() &#123;        return next != null;    &#125;    // 获取下一个节点    final Node&lt;K,V&gt; nextNode() &#123;        Node&lt;K,V&gt;[] t;        Node&lt;K,V&gt; e = next;        if (modCount != expectedModCount)            throw new ConcurrentModificationException();        if (e == null)            throw new NoSuchElementException();        // 如果当前链表被遍历完了，那么就寻找下一个不是null的链表头        if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123;            do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null);        &#125;        return e;    &#125;    // 删除当前被迭代的节点    public final void remove() &#123;        Node&lt;K,V&gt; p = current;        if (p == null)            throw new IllegalStateException();        if (modCount != expectedModCount)            throw new ConcurrentModificationException();        current = null;        K key = p.key;        // 删除节点        removeNode(hash(key), key, null, false, false);        // 同步新的modCount        expectedModCount = modCount;    &#125;&#125;\n\n从上面迭代时的算法可以看到，迭代器总是先遍历当前的链表或者红黑树，然后再去遍历哈希表，也就是说，它采用的是深度优先的算法。\n[^1]: 搞懂 Java HashMap 源码[^2]: Java 8系列之重新认识HashMap[^3]: 集合番@HashMap一文通（1.7版）[^4]: HashMap 源码详细分析(JDK1.8)[^5]: Java 集合深入理解（16）：HashMap 主要特点和关键方法源码解读\n","categories":["学知识"],"tags":["Java","HashMap"]},{"title":"生产者与消费者问题在Java中的实现","url":"/java-multi-threading-producer-consumer-problem.html","content":"生产者与消费者问题(Producer-consumer problem)，也叫有限缓冲问题(Bounded-buffer problem)，是一个经典的多线程同步问题案例。\n该问题中有两个线程共享一个固定大小的缓冲区，一个线程作为生产者，负责向缓冲区中放入数据；另一个线程作为消费者，负责从缓冲区中取出数据。该问题的重点在于，要保证当缓冲区满时，生产者不能继续向其中放入数据，而当缓冲区空时，消费者也不能从缓冲区中取出数据。\n那么要保证以上两点，需要在缓冲区空时休眠消费者线程，并当缓冲区有数据之后唤醒消费者线程；以及当缓冲区满时休眠生产者线程，在缓冲区有空闲空间后唤醒生产者线程，或者直接在缓冲区满时放弃未存入缓冲区的数据。\n这个问题的难点在于可能会产生死锁。当陷入死锁时，生产者和消费者都会处于休眠状态，并等待对方唤醒自己。\n\n\n使用同步锁实现产品类产品类代表将要被生产和消费的产品。\n/** * 生产者-消费者问题 -- 产品类 */public class Product &#123;    private int productId;    public Product(int productId) &#123;        this.productId = productId;    &#125;    @Override    public String toString() &#123;        return &quot;Product&#123;&quot; +                &quot;productId=&quot; + productId +                &#x27;&#125;&#x27;;    &#125;    public int getProductId() &#123;        return productId;    &#125;    public void setProductId(int productId) &#123;        this.productId = productId;    &#125;&#125;\n\n仓库类仓库类用来构造一个存放产品的数组，并带有存取数组的方法(pop&#x2F;push)，本质上是一个栈。\n/** * 生产者-消费者问题 -- 仓库类(缓冲区) */public class Warehouse &#123;    private Product[] products;    /**     * 栈顶指针     */    private int top = 0;    public Warehouse() &#123;        products = new Product[10];    &#125;    public Warehouse(int capacity) &#123;        products = new Product[capacity];    &#125;    /**     * 生产产品     *     * @param product 产品     */    public synchronized void push(Product product) &#123;        // 如果仓库已满        if (top == products.length) &#123;            try &#123;                System.out.println(&quot;Warehouse full.&quot;);                // 将生产者线程置于等待态                wait();            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Warehouse full but failed to wait. Reason:&quot;);                System.out.println(e.getMessage());            &#125;        &#125;        products[top++] = product;        // 现在仓库已有产品        // 可以唤醒消费者线程        notifyAll();    &#125;    /**     * 消费产品     *     * @return 取出的产品     */    public synchronized Product pop() &#123;        Product product = null;        // 如果仓库空        while (products[0] == null) &#123;            System.out.println(&quot;Warehouse empty&quot;);            // 唤醒生产者线程            notifyAll();            try &#123;                // 将消费者线程置于等待态                wait();            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Warehouse empty but failed to wait. Reason:&quot;);                System.out.println(e.getMessage());            &#125;        &#125;        product = products[--top];        products[top] = null;        // 仓库非满，可以唤醒生产者线程        notifyAll();        return product;    &#125;&#125;\n\n生产者public class Producer implements Runnable &#123;    private String producerName;    private Warehouse warehouse;    private Random random = new Random();    public Producer(String producerName, Warehouse warehouse) &#123;        this.producerName = producerName;        this.warehouse = warehouse;    &#125;    @Override    public void run() &#123;        produce();    &#125;    /**     * 生产产品并存入仓库     */    private void produce() &#123;        int i = 0;        while (true) &#123;            Product product = new Product(i++);            warehouse.push(product);            System.out.println(&quot;[PRODUCED] Product &quot; + product.getProductId());            try &#123;                Thread.sleep(random.nextInt(20) * 100);            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Exception occurred in producer thread. Reason: &quot; + e.getMessage());            &#125;        &#125;    &#125;&#125;\n\n消费者public class Consumer implements Runnable &#123;    private String consumerName;    private Warehouse warehouse;    private Random random = new Random();    public Consumer(String consumerName, Warehouse warehouse) &#123;        this.consumerName = consumerName;        this.warehouse = warehouse;    &#125;    @Override    public void run() &#123;        consume();    &#125;    private void consume() &#123;        while (true) &#123;            Product product = warehouse.pop();            System.out.println(&quot;[CONSUMED] Product &quot; + product.getProductId());            try &#123;                Thread.sleep(random.nextInt(20) * 100);            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Exception occurred in consumer thread. Reason: &quot; + e.getMessage());            &#125;        &#125;    &#125;&#125;\n\nMain类public class Main &#123;    public static void main(String[] args) &#123;        Warehouse warehouse = new Warehouse();        Producer producer = new Producer(&quot;producer&quot;, warehouse);        Consumer consumer = new Consumer(&quot;consumer&quot;, warehouse);        ExecutorService executorService = Executors.newCachedThreadPool();        executorService.execute(consumer);        executorService.execute(producer);        executorService.shutdown();        while (!executorService.isTerminated()) &#123;        &#125;        System.out.println(&quot;Thread pool is down&quot;);    &#125;&#125;\n\n运行结果Warehouse empty[PRODUCED] Product 0[CONSUMED] Product 0[PRODUCED] Product 1[CONSUMED] Product 1[PRODUCED] Product 2[PRODUCED] Product 3[PRODUCED] Product 4[PRODUCED] Product 5[CONSUMED] Product 5[PRODUCED] Product 6[CONSUMED] Product 6[CONSUMED] Product 4[CONSUMED] Product 3[CONSUMED] Product 2[PRODUCED] Product 7[CONSUMED] Product 7[PRODUCED] Product 8[CONSUMED] Product 8Warehouse empty[PRODUCED] Product 9[CONSUMED] Product 9Warehouse empty\n\n使用阻塞队列实现相比较于队列，阻塞队列(Blocking queue)可以在队列空时阻塞取值操作，并在队列满时阻塞存入操作。\n实际上根据调用不同的方法，可以实现在队列空&#x2F;满时抛出异常、返回特殊值、阻塞操作、带超时的阻塞操作，具体请参考BlockingQueue文档\n产品类和仓库类产品类实现同上，仓库使用阻塞队列(ArrayBlockingQueue)实现。\n生产者public class Producer implements Runnable &#123;    private String producerName;    private BlockingQueue&lt;Product&gt; warehouse;    private Random random = new Random();    public Producer(String producerName, BlockingQueue&lt;Product&gt; warehouse) &#123;        this.producerName = producerName;        this.warehouse = warehouse;    &#125;    @Override    public void run() &#123;        produce();    &#125;    /**     * 生产产品并存入仓库     */    private void produce() &#123;        int i = 0;        while (true) &#123;            Product product = new Product(i++);            try &#123;                warehouse.put(product);            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Exception occurred when putting product in producer. Reason: &quot; + e.getMessage());            &#125;            System.out.println(&quot;[PRODUCED] Product &quot; + product.getProductId());            try &#123;                Thread.sleep(random.nextInt(20) * 100);            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Exception occurred in producer thread. Reason: &quot; + e.getMessage());            &#125;        &#125;    &#125;&#125;\n\n消费者public class Consumer implements Runnable &#123;    private String consumerName;    private BlockingQueue&lt;Product&gt; warehouse;    private Random random = new Random();    public Consumer(String consumerName, BlockingQueue&lt;Product&gt; warehouse) &#123;        this.consumerName = consumerName;        this.warehouse = warehouse;    &#125;    @Override    public void run() &#123;        consume();    &#125;    private void consume() &#123;        while (true) &#123;            Product product = null;            try &#123;                product = warehouse.take();            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Exception occurred when taking product in consumer. Reason: &quot; + e.getMessage());            &#125;            System.out.println(&quot;[CONSUMED] Product &quot; + product.getProductId());            try &#123;                Thread.sleep(random.nextInt(20) * 100);            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;Exception occurred in consumer thread. Reason: &quot; + e.getMessage());            &#125;        &#125;    &#125;&#125;\n\n运行结果[PRODUCED] Product 0[CONSUMED] Product 0[PRODUCED] Product 1[CONSUMED] Product 1[PRODUCED] Product 2[CONSUMED] Product 2[PRODUCED] Product 3[CONSUMED] Product 3[PRODUCED] Product 4[CONSUMED] Product 4[PRODUCED] Product 5[PRODUCED] Product 6[CONSUMED] Product 5[PRODUCED] Product 7[PRODUCED] Product 8[CONSUMED] Product 6[PRODUCED] Product 9","categories":["学知识"],"tags":["Java","多线程"]},{"title":"Java源码阅读 - LinkedList","url":"/java-read-src-linkedlist.html","content":"做技术，不能只知其然而不知其所以然。在知道了工具的原理之后，才能更高效的使用这个工具。在程序的世界里，源码里面没有秘密，看懂了源码，也就看懂了原理。\n这次就来阅读一下LinkedList的源码。\n\n\nLinkedList的特性LinkedList有如下几个特性：\n\n底层的数据结构是双向链表\n存储的数据允许为null\n允许存放重复的数据\n元素在List中的顺序由添加顺序决定\n不是线程安全的\n\n类的声明public class LinkedList&lt;E&gt;    extends AbstractSequentialList&lt;E&gt;    implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable\n\n上面代码声明了一个名为LinkedList的泛型类，继承了AbstractSequentialList，并实现了List，Deque，Cloneable，Serializable接口。\nAbstractSequentialList抽象类提供了一个“骨架”级别的List实现，用来减少实现一个支持顺序读写的List的工作量。\nDeque接口约定了要实现一个双向队列(Double Ended Queue)所必须要实现的方法。\nCloneable是一个标记接口，表明了这个类允许使用Object.clone()命令进行属性到属性的复制。\nSerializable也是一个标记接口，表明在这个类上启用Java的序列化功能。\n如何存储数据transient int size = 0;/**    * Pointer to first node.    * Invariant: (first == null &amp;&amp; last == null) ||    *            (first.prev == null &amp;&amp; first.item != null)    */transient Node&lt;E&gt; first;/**    * Pointer to last node.    * Invariant: (first == null &amp;&amp; last == null) ||    *            (last.next == null &amp;&amp; last.item != null)    */transient Node&lt;E&gt; last;\n\nfirst和last是两个Node对象，分别指向了链表中的第一个节点和最后一个节点。size保存了这个链表中元素的个数。\nNode类是LinkedList类中的一个内部类，它定义了一个元素实际上是如何被存储的。\nprivate static class Node&lt;E&gt; &#123;    E item;    Node&lt;E&gt; next;    Node&lt;E&gt; prev;    Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123;        this.item = element;        this.next = next;        this.prev = prev;    &#125;&#125;\n\nitem是实际存储的数据，next和prev则分别指向了下一个元素和上一个元素。\n构造方法LinkedList有两个构造方法，分别用来初始化一个空的链表，和从一个给定的集合中取出元素来初始化一个链表。\n无参构造方法/**    * Constructs an empty list.    */public LinkedList() &#123;&#125;\n\n无参的构造方法实际上什么都没有做，返回的LinkedList对象中，size为默认值0，first和last的值都是null。\n从集合初始化的构造方法/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection&#x27;s * iterator. * * @param  c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public LinkedList(Collection&lt;? extends E&gt; c) &#123;    this();    addAll(c);&#125;\n\n这个构造方法首先创建了一个空的LinkedList，然后调用了addAll方法将集合中的数据放到这个链表中。\n插入数据在LinkedList中插入数据有三种方式：在头部增加节点、在尾部增加节点，和在某个元素间插入节点。\n在头部增加节点要在链表头部增加节点，可以使用addFirst(E)方法。\n/** * Inserts the specified element at the beginning of this list. * * @param e the element to add */public void addFirst(E e) &#123;    linkFirst(e);&#125;\n\n该方法又调用了一个private方法linkFirst(E)实现在头部插入数据。\n/** * Links e as first element. */private void linkFirst(E e) &#123;    final Node&lt;E&gt; f = first;    final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f);    first = newNode;    if (f == null)        last = newNode;    else        f.prev = newNode;    size++;    modCount++;&#125;\n\nlinkFirst(E)方法首先取出当前的头部元素first，然后构造了一个新的Node对象，新对象的prev值为null，代表它是一个头部元素，next值为原来的first，它存储的数据则是这次插入的数据。然后它将链表的first设为这次新增加的元素。\n根据链表的特性可以知道，如果一个链表不是空的，那么它的first必定非空；反之，如果它的first为null，那么这个链表一定为空。所以根据这个规则，它会判断在插入元素前，这个链表是不是空的，如果是空的，那么新元素就同时作为链表的尾last；如果不是空的，那么就让原来的first的prev指向新插入的元素。这样操作之后，新元素与原first元素之间就出现了一个双向的引用，即完成了一个小的双向链表。\n最后使链表的size加一，就完成了一次新增头元素的操作。\n在尾部增加节点在尾部增加节点，可以使用add(E)方法或addLast(E)方法。\n/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #add&#125;. * * @param e the element to add */public void addLast(E e) &#123;    linkLast(e);&#125;/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #addLast&#125;. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123;    linkLast(e);    return true;&#125;\n\n可以看出，两个方法都是通过一个private方法linkLast(E)实现的。\n/** * Links e as last element. */void linkLast(E e) &#123;    final Node&lt;E&gt; l = last;    final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null);    last = newNode;    if (l == null)        first = newNode;    else        l.next = newNode;    size++;    modCount++;&#125;\n\n在尾部插入数据的操作与在头部插入数据的操作类似，依旧是构造一个新的节点，使原来的last节点指向新节点，然后根据原链表是否为空执行后续操作。在这里就不多赘述了。\n在中间增加节点要在链表中间插入数据，可以使用add(int, E)方法。\n/** * Inserts the specified element at the specified position in this list. * Shifts the element currently at that position (if any) and any * subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123;    checkPositionIndex(index);    if (index == size)        linkLast(element);    else        linkBefore(element, node(index));&#125;\n\n这个方法所做的操作，简单来说就是，将新的元素放到指定位置，并将原来处于这个位置的元素及其所有后续元素全部后移一个位置。\n首先它调用了checkPositionIndex(int)方法，我们看看它干了什么。\nprivate void checkPositionIndex(int index) &#123;    if (!isPositionIndex(index))        throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;/** * Tells if the argument is the index of a valid position for an * iterator or an add operation. */private boolean isPositionIndex(int index) &#123;    return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;/** * Constructs an IndexOutOfBoundsException detail message. * Of the many possible refactorings of the error handling code, * this &quot;outlining&quot; performs best with both server and client VMs. */private String outOfBoundsMsg(int index) &#123;    return &quot;Index: &quot;+index+&quot;, Size: &quot;+size;&#125;\n\n这个方法检查了用户输入的插入位置是不是一个合法的位置，规则就是插入位置必须大于等于0且小于等于最大位置。\n通过检查之后，它继续判断插入的位置是不是链表的末尾，如果是末尾的话，就直接调用linkLast(E)在链表尾部新增一个元素，否则它会先取出现在位于插入位置的节点，然后调用linkBefore(E, Node)在链表中间插入元素。\n/** * Returns the (non-null) Node at the specified element index. */Node&lt;E&gt; node(int index) &#123;    // assert isElementIndex(index);    if (index &lt; (size &gt;&gt; 1)) &#123;        Node&lt;E&gt; x = first;        for (int i = 0; i &lt; index; i++)            x = x.next;        return x;    &#125; else &#123;        Node&lt;E&gt; x = last;        for (int i = size - 1; i &gt; index; i--)            x = x.prev;        return x;    &#125;&#125;/** * Inserts element e before non-null Node succ. */void linkBefore(E e, Node&lt;E&gt; succ) &#123;    // assert succ != null;    final Node&lt;E&gt; pred = succ.prev;    final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ);    succ.prev = newNode;    if (pred == null)        first = newNode;    else        pred.next = newNode;    size++;    modCount++;&#125;\n\nnode(int)方法巧妙的利用了二分法，根据元素所在的位置来决定是从链表头部还是从尾部开始查找节点。\nlinkBefore(E, Node)方法进行的操作，就像我们在书中学习到的一样，先让新节点建立起到左右两个节点的连接，然后让右边的节点连接到新插入的节点，最后更新链表的大小。\n用集合批量增加节点之前我们在LinkedList的构造方法中看到了一个addAll(Collection)方法，现在就来看看它干了什么。\n/** * Appends all of the elements in the specified collection to the end of * this list, in the order that they are returned by the specified * collection&#x27;s iterator.  The behavior of this operation is undefined if * the specified collection is modified while the operation is in * progress.  (Note that this will occur if the specified collection is * this list, and it&#x27;s nonempty.) * * @param c collection containing elements to be added to this list * @return &#123;@code true&#125; if this list changed as a result of the call * @throws NullPointerException if the specified collection is null */public boolean addAll(Collection&lt;? extends E&gt; c) &#123;    return addAll(size, c);&#125;/** * Inserts all of the elements in the specified collection into this * list, starting at the specified position.  Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices).  The new elements will appear * in the list in the order that they are returned by the * specified collection&#x27;s iterator. * * @param index index at which to insert the first element *              from the specified collection * @param c collection containing elements to be added to this list * @return &#123;@code true&#125; if this list changed as a result of the call * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; * @throws NullPointerException if the specified collection is null */public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123;    checkPositionIndex(index);    Object[] a = c.toArray();    int numNew = a.length;    if (numNew == 0)        return false;    Node&lt;E&gt; pred, succ;    if (index == size) &#123;        succ = null;        pred = last;    &#125; else &#123;        succ = node(index);        pred = succ.prev;    &#125;    for (Object o : a) &#123;        @SuppressWarnings(&quot;unchecked&quot;) E e = (E) o;        Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null);        if (pred == null)            first = newNode;        else            pred.next = newNode;        pred = newNode;    &#125;    if (succ == null) &#123;        last = pred;    &#125; else &#123;        pred.next = succ;        succ.prev = pred;    &#125;    size += numNew;    modCount++;    return true;&#125;\n\n这里的重头戏是addAll(int, Collection)方法，它实现了在链表中间批量插入节点的功能。addAll(Collection)实际上就是调用它在链表末尾批量插入节点。\n首先addAll方法会检查插入的位置是否合法，如果不合法就会抛出IndexOutOfBoundsException异常。然后它将传入的集合转换成一个对象数组，并检查数组长度，如果长度是0，则说明链表内容未被改变，直接返回false。\n然后它会检查插入的位置，并且记录下插入位置的上一个节点和下一个节点。\n接下来这个方法开始遍历传入的集合，并将集合中的数据逐个插入到链表中。插入的逻辑与前面讲的类似，所以就不再赘述了。\n最后它会完成一系列收尾工作，包括设定链表尾部的节点，和更新链表的长度，然后返回true，代表链表成功被更新了。\n查询数据因为LinkedList也是一个双向队列，所以它既允许从两端开始获取数据，又可以根据下标从指定位置获取数据。\n取出头部的节点LinkedList提供了多个方法来允许用户从链表头部取出数据，分别有：\n/** * Returns the first element in this list. * * @return the first element in this list * @throws NoSuchElementException if this list is empty */public E getFirst() &#123;    final Node&lt;E&gt; f = first;    if (f == null)        throw new NoSuchElementException();    return f.item;&#125;/** * Retrieves, but does not remove, the head (first element) of this list. * * @return the head of this list, or &#123;@code null&#125; if this list is empty * @since 1.5 */public E peek() &#123;    final Node&lt;E&gt; f = first;    return (f == null) ? null : f.item;&#125;/** * Retrieves and removes the head (first element) of this list. * * @return the head of this list, or &#123;@code null&#125; if this list is empty * @since 1.5 */public E poll() &#123;    final Node&lt;E&gt; f = first;    return (f == null) ? null : unlinkFirst(f);&#125;\n\n从头部取出节点的操作都大同小异，区别只是在于取出空值之后是抛异常还是返回null，以及会不会同时删除头部元素。逻辑很简单，这里就不多赘述了。\n除了上面列出的几个方法外，还有pop()、pollFirst()等方法也提供了相同的功能，但是代码内容大同小异，所以也不放上来了，以免浪费篇幅。\n取出尾部的节点LinkedList同样提供了数个方法用于从尾部取出节点，它们的逻辑也基本相同，这里同上文一样，仅展示部分代码。\n/** * Returns the last element in this list. * * @return the last element in this list * @throws NoSuchElementException if this list is empty */public E getLast() &#123;    final Node&lt;E&gt; l = last;    if (l == null)        throw new NoSuchElementException();    return l.item;&#125;/** * Retrieves, but does not remove, the last element of this list, * or returns &#123;@code null&#125; if this list is empty. * * @return the last element of this list, or &#123;@code null&#125; *         if this list is empty * @since 1.6 */public E peekLast() &#123;    final Node&lt;E&gt; l = last;    return (l == null) ? null : l.item;&#125;/** * Retrieves and removes the last element of this list, * or returns &#123;@code null&#125; if this list is empty. * * @return the last element of this list, or &#123;@code null&#125; if *     this list is empty * @since 1.6 */public E pollLast() &#123;    final Node&lt;E&gt; l = last;    return (l == null) ? null : unlinkLast(l);&#125;\n\n取出中间的节点要从链表中的某个位置取出节点，可以使用get(int)方法。\n/** * Returns the element at the specified position in this list. * * @param index index of the element to return * @return the element at the specified position in this list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E get(int index) &#123;    checkElementIndex(index);    return node(index).item;&#125;\n\n它首先还是检查了传入的下标是否合法，如果合法就调用node(int)方法取得该节点，并返回其数据。node(int)方法在上面已经介绍过，这里就不重复介绍了。\n查询链表是否包含某个数据LinkedList提供了contains(Object)方法用来查询该链表是否包含某个数据。\n/** * Returns &#123;@code true&#125; if this list contains the specified element. * More formally, returns &#123;@code true&#125; if and only if this list contains * at least one element &#123;@code e&#125; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;e==null&amp;nbsp;:&amp;nbsp;o.equals(e))&lt;/tt&gt;. * * @param o element whose presence in this list is to be tested * @return &#123;@code true&#125; if this list contains the specified element */public boolean contains(Object o) &#123;    return indexOf(o) != -1;&#125;/** * Returns the index of the first occurrence of the specified element * in this list, or -1 if this list does not contain the element. * More formally, returns the lowest index &#123;@code i&#125; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;, * or -1 if there is no such index. * * @param o element to search for * @return the index of the first occurrence of the specified element in *         this list, or -1 if this list does not contain the element */public int indexOf(Object o) &#123;    int index = 0;    if (o == null) &#123;        for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123;            if (x.item == null)                return index;            index++;        &#125;    &#125; else &#123;        for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123;            if (o.equals(x.item))                return index;            index++;        &#125;    &#125;    return -1;&#125;\n\ncontains(Object)方法实际上是调用了indexOf(Object)方法，并检查其返回是否为-1，来判断这个值是否存在于该链表中。\nindexOf(Object)方法的逻辑就是，从链表的头部开始，逐个检查其节点的值是否为传入的值。如果链表为空则直接返回-1。\n修改数据LinkedList提供了一个set(int, E)的方法用于修改某个节点的数据。\n/** * Replaces the element at the specified position in this list with the * specified element. * * @param index index of the element to replace * @param element element to be stored at the specified position * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E set(int index, E element) &#123;    checkElementIndex(index);    Node&lt;E&gt; x = node(index);    E oldVal = x.item;    x.item = element;    return oldVal;&#125;\n\n该方法首先检查传入的下标是否合法，检查通过后，它会为指定位置的节点设定新的数据，并返回该节点原有的数据。\n删除数据LinkedList提供了多个方法来从链表中删除节点。\n/** * Removes and returns the first element from this list. * * @return the first element from this list * @throws NoSuchElementException if this list is empty */public E removeFirst() &#123;    final Node&lt;E&gt; f = first;    if (f == null)        throw new NoSuchElementException();    return unlinkFirst(f);&#125;/** * Removes and returns the last element from this list. * * @return the last element from this list * @throws NoSuchElementException if this list is empty */public E removeLast() &#123;    final Node&lt;E&gt; l = last;    if (l == null)        throw new NoSuchElementException();    return unlinkLast(l);&#125;\n\nremoveFirst()和removeLast()方法分别可以从链表的头部和尾部取出一个节点，并将其删除。如果链表是空的，则会抛出NoSuchElementException异常。\n/** * Removes the element at the specified position in this list.  Shifts any * subsequent elements to the left (subtracts one from their indices). * Returns the element that was removed from the list. * * @param index the index of the element to be removed * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123;    checkElementIndex(index);    return unlink(node(index));&#125;\n\nremove(int)方法可以用来取出并删除指定位置下的一个节点，同时所有处于其后方的节点都将向前移动一个位置。\n/** * Removes the first occurrence of the specified element from this list, * if it is present.  If this list does not contain the element, it is * unchanged.  More formally, removes the element with the lowest index * &#123;@code i&#125; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt; * (if such an element exists).  Returns &#123;@code true&#125; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &#123;@code true&#125; if this list contained the specified element */public boolean remove(Object o) &#123;    if (o == null) &#123;        for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123;            if (x.item == null) &#123;                unlink(x);                return true;            &#125;        &#125;    &#125; else &#123;        for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123;            if (o.equals(x.item)) &#123;                unlink(x);                return true;            &#125;        &#125;    &#125;    return false;&#125;/** * Removes the first occurrence of the specified element in this * list (when traversing the list from head to tail).  If the list * does not contain the element, it is unchanged. * * @param o element to be removed from this list, if present * @return &#123;@code true&#125; if the list contained the specified element * @since 1.6 */public boolean removeFirstOccurrence(Object o) &#123;    return remove(o);&#125;/** * Removes the last occurrence of the specified element in this * list (when traversing the list from head to tail).  If the list * does not contain the element, it is unchanged. * * @param o element to be removed from this list, if present * @return &#123;@code true&#125; if the list contained the specified element * @since 1.6 */public boolean removeLastOccurrence(Object o) &#123;    if (o == null) &#123;        for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123;            if (x.item == null) &#123;                unlink(x);                return true;            &#125;        &#125;    &#125; else &#123;        for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123;            if (o.equals(x.item)) &#123;                unlink(x);                return true;            &#125;        &#125;    &#125;    return false;&#125;\n\nremove(Object)方法和removeFirstOccurrence(Object)方法会从头部遍历整个链表，并检查各个节点是否与传入的参数匹配。一旦找到一个匹配的节点就将其删除，并结束操作。removeLastOccurrence(Object)方法则是从链表尾部开始查找匹配的节点，并删除第一个匹配到的节点。\n将LinkedList当作栈来操作上面说过，LinkedList可以被当成一个双向队列来操作。那么，如果我们把这个队列的底部“封死”，只操作头部，它是不是就变成了一个栈呢？没错，它是可以这样用的，而且也已经有方法来允许我们这样操作了。实际上，push和pop的操作，就是直接调用了addFirst(E)和removeFirst()方法来实现入栈和出栈操作的。\n/** * Pushes an element onto the stack represented by this list.  In other * words, inserts the element at the front of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #addFirst&#125;. * * @param e the element to push * @since 1.6 */public void push(E e) &#123;    addFirst(e);&#125;/** * Pops an element from the stack represented by this list.  In other * words, removes and returns the first element of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #removeFirst()&#125;. * * @return the element at the front of this list (which is the top *         of the stack represented by this list) * @throws NoSuchElementException if this list is empty * @since 1.6 */public E pop() &#123;    return removeFirst();&#125;\n\n迭代LinkedList提供了iterator()和listIterator(int)方法来获取迭代器。实际上这两个方法都将返回一个ListItr实例，区别在于iterator()是从链表头部开始迭代，而listIterator(int)方法则是从指定位置开始迭代。\nprivate class ListItr implements ListIterator&lt;E&gt; &#123;    // 上次迭代时返回的节点    private Node&lt;E&gt; lastReturned;    // 下一次迭代即将返回的节点    // 其实也是当前指向但仍未取值的节点    private Node&lt;E&gt; next;    // 下一个被迭代节点的位置    private int nextIndex;    private int expectedModCount = modCount;    ListItr(int index) &#123;        // 判断开始迭代的位置是否为链表末尾        // 如果不是末尾就取出开始迭代位置的节点，否则取出null        next = (index == size) ? null : node(index);        // 将nextIndex指向初始迭代位置        nextIndex = index;    &#125;    // 检查有无下一个节点可供迭代    public boolean hasNext() &#123;        // 如果下一个迭代位置的下标小于链表长度        // 就认为还有元素可供迭代        return nextIndex &lt; size;    &#125;    // 获取下一个节点    public E next() &#123;        // 检查链表的结构有没有被修改        checkForComodification();        // 如果已经没有节点可供迭代        // 则抛出NoSuchElementException        if (!hasNext())            throw new NoSuchElementException();        // 取出下一个被迭代的节点        lastReturned = next;        // next指针像下一个节点移动        next = next.next;        nextIndex++;        // 取出当前被迭代的节点的值        return lastReturned.item;    &#125;    // nextIndex的初始值为0    // 当它大于0时，就认定该位置的前面仍有节点可供迭代    public boolean hasPrevious() &#123;        return nextIndex &gt; 0;    &#125;    // 获取上一个节点    public E previous() &#123;        // 检查链表的结构有没有被修改        checkForComodification();        // 检查前面有无节点可供迭代        if (!hasPrevious())            throw new NoSuchElementException();        // 检查当前节点是否为null，如果是，就说明当前已经处于链表的末尾，那么就返回链表最后一个节点；        // 如果不是，就返回当前位置的上一个节点        // 然后设定当前位置和上一次返回位置为上一个节点        lastReturned = next = (next == null) ? last : next.prev;        nextIndex--;        return lastReturned.item;    &#125;    // 获取下一个被迭代节点的位置    public int nextIndex() &#123;        return nextIndex;    &#125;    // 获取上一次被迭代节点的位置    public int previousIndex() &#123;        return nextIndex - 1;    &#125;    // 删除上一次被迭代的节点    public void remove() &#123;        // 检查链表的结构有没有被修改        checkForComodification();        // 如果没有上一次被迭代的节点        // 则抛出IllegalStateException        if (lastReturned == null)            throw new IllegalStateException();        // 取出将被删除节点的下一个节点        Node&lt;E&gt; lastNext = lastReturned.next;        // 然后删掉它        unlink(lastReturned);        if (next == lastReturned)            next = lastNext;        else            nextIndex--;        // 重置上一次被迭代的位置        lastReturned = null;        // 因为unlink会使modCound加一        // 所以这里要同步把expectedModCount加一        expectedModCount++;    &#125;    // 修改上次迭代到的节点的值    public void set(E e) &#123;        if (lastReturned == null)            throw new IllegalStateException();        checkForComodification();        // 因为lastReturned是某个节点的引用        // 所以可以直接修改它的值        lastReturned.item = e;    &#125;    // 在下一个被迭代的节点前插入一个节点    public void add(E e) &#123;        checkForComodification();        lastReturned = null;        if (next == null)            // 如果已经迭代到链表的末尾，那么就在末尾新增一个节点            linkLast(e);        else            // 否则就在下个被迭代的节点前插入一个节点            linkBefore(e, next);        // 游标向后移一位        nextIndex++;        // 同步expectedModCount        expectedModCount++;    &#125;    public void forEachRemaining(Consumer&lt;? super E&gt; action) &#123;        Objects.requireNonNull(action);        while (modCount == expectedModCount &amp;&amp; nextIndex &lt; size) &#123;            action.accept(next.item);            lastReturned = next;            next = next.next;            nextIndex++;        &#125;        checkForComodification();    &#125;    final void checkForComodification() &#123;        if (modCount != expectedModCount)            throw new ConcurrentModificationException();    &#125;&#125;\n","categories":["学知识"],"tags":["Java","LinkedList"]},{"title":"Java源码阅读 - Timer","url":"/java-read-src-timer.html","content":"前段时间拿ConcurrentHashMap和Timer简单整了个带过期功能的缓存，那用都用了，顺便就看看Timer这玩意是咋实现的。\n\n\n代码示例这段代码的背景是，我们想要把已经验证通过的token缓存起来，下次遇到同样的token就不需要再浪费CPU做重复的校验，直接从缓存中返回解析好的token对象就行。而因为token会过期，所以要每秒检查一次缓存里面是否有token过期，并批量从缓存中删除。\n验证和缓存token那部份不是本文重点，就不写了。清除过期缓存的Timer就是这样实现的：\n// 定义一个Timerprivate final Timer purgeCacheTimer = new Timer();// 在构造方法里面注册一个定时任务purgeCacheTimer.scheduleAtFixedRate(new TimerTask() &#123;    @Override    public void run() &#123;        final List&lt;String&gt; expiredTokenList = new ArrayList&lt;&gt;();        for (final Map.Entry&lt;String, Object&gt; token : validatedTokenCache.entrySet()) &#123;            if (isTokenExpired(token.getValue())) &#123;                expiredTokenList.add(token.getKey());            &#125;        &#125;        expiredTokenList.forEach(validatedTokenCache::remove);    &#125;&#125;, 1000, 1000);\n\nTimer的核心数据结构点进Timer类就能在开头看到两个属性：queue和thread。\n/** * The timer task queue.  This data structure is shared with the timer * thread.  The timer produces tasks, via its various schedule calls, * and the timer thread consumes, executing timer tasks as appropriate, * and removing them from the queue when they&#x27;re obsolete. */private final TaskQueue queue = new TaskQueue();/** * The timer thread. */private final TimerThread thread = new TimerThread(queue);\n\nqueue所属的TaskQueue就是这个Timer用来存储任务的队列，里面是把一个数组处理成了一个优先队列。thread所属的TimerThread类则是Timer的核心，负责任务调度。\n任务是怎么添加进队列的先看看给Timer添加任务会发生什么。点进scheduleAtFixedRate可以看到这么一段代码：\npublic void scheduleAtFixedRate(TimerTask task, long delay, long period) &#123;    if (delay &lt; 0)        throw new IllegalArgumentException(&quot;Negative delay.&quot;);    if (period &lt;= 0)        throw new IllegalArgumentException(&quot;Non-positive period.&quot;);    sched(task, System.currentTimeMillis()+delay, period);&#125;\n\n跟着点进sched可以看到这么一段：\nprivate void sched(TimerTask task, long time, long period) &#123;    if (time &lt; 0)        throw new IllegalArgumentException(&quot;Illegal execution time.&quot;);    // Constrain value of period sufficiently to prevent numeric    // overflow while still being effectively infinitely large.    if (Math.abs(period) &gt; (Long.MAX_VALUE &gt;&gt; 1))        period &gt;&gt;= 1;    synchronized(queue) &#123;        if (!thread.newTasksMayBeScheduled)            throw new IllegalStateException(&quot;Timer already cancelled.&quot;);        synchronized(task.lock) &#123;            if (task.state != TimerTask.VIRGIN)                throw new IllegalStateException(                    &quot;Task already scheduled or cancelled&quot;);            task.nextExecutionTime = time;            task.period = period;            task.state = TimerTask.SCHEDULED;        &#125;        queue.add(task);        if (queue.getMin() == task)            queue.notify();    &#125;&#125;\n\n看下来其实就做了两件事：向queue里面增加一个TimerTask对象，然后检查队列里面即将执行的task，如果是刚添加的这个task那就唤醒任务。\n任务是怎么被调度的点进TimerThread首先可以看到它的run方法：\npublic void run() &#123;    try &#123;        mainLoop();    &#125; finally &#123;        // Someone killed this Thread, behave as if Timer cancelled        synchronized(queue) &#123;            newTasksMayBeScheduled = false;            queue.clear();  // Eliminate obsolete references        &#125;    &#125;&#125;\n\n因为TimerThread继承自Thread类，所以run方法就是它的入口点。在Timer的构造函数中就会执行thread.start()操作，所以这个调度线程在Timer被创建后就会开始运行。\n继续看看mainLoop里面干了些啥。\n/** * The main timer loop.  (See class comment.) */private void mainLoop() &#123;    while (true) &#123;        try &#123;            TimerTask task;            boolean taskFired;            synchronized(queue) &#123;                // Wait for queue to become non-empty                while (queue.isEmpty() &amp;&amp; newTasksMayBeScheduled)                    queue.wait();                if (queue.isEmpty())                    break; // Queue is empty and will forever remain; die                // Queue nonempty; look at first evt and do the right thing                long currentTime, executionTime;                task = queue.getMin();                synchronized(task.lock) &#123;                    if (task.state == TimerTask.CANCELLED) &#123;                        queue.removeMin();                        continue;  // No action required, poll queue again                    &#125;                    currentTime = System.currentTimeMillis();                    executionTime = task.nextExecutionTime;                    if (taskFired = (executionTime&lt;=currentTime)) &#123;                        if (task.period == 0) &#123; // Non-repeating, remove                            queue.removeMin();                            task.state = TimerTask.EXECUTED;                        &#125; else &#123; // Repeating task, reschedule                            queue.rescheduleMin(                                task.period&lt;0 ? currentTime   - task.period                                            : executionTime + task.period);                        &#125;                    &#125;                &#125;                if (!taskFired) // Task hasn&#x27;t yet fired; wait                    queue.wait(executionTime - currentTime);            &#125;            if (taskFired)  // Task fired; run it, holding no locks                task.run();        &#125; catch(InterruptedException e) &#123;        &#125;    &#125;&#125;\n\n首先它检查队列是否为空，如果是空的，那么这个调度线程就会进入等待状态（这就是在添加任务之后要调用notify()方法的原因）；如果不仅队列是空的，而且队列不再接收新的任务，那么就说明这个Timer正在被销毁，所以直接退出循环。\n如果队列非空，那么调度线程就从队列中取出最接近下一个执行时机的任务，然后安排下一次任务执行的时间。同时，在取出最近需要执行的任务后，调度线程会比较当前时间与计划执行时间，如果当前时间还没到，那么调度线程会wait到计划执行时间。最后，调用TimerTask的run方法，开始执行我们指定它做的事。\n总结说白了就是俩东西：\n\n一个保存任务的队列，里面的任务按照执行时间排队\n一个死循环，反复从队列里面取出最接近下一个执行时机的任务，执行它，并安排下一次的时机\n\n这玩意优点就是简单粗暴而且轻量级，但因为它是单线程的，如果某个任务执行时间过长，那么后续任务有可能会受影响；而且因为系统调度和任务执行时间的不确定性，它不能提供高精度的定时任务服务；此外，如果其中某个任务抛出了未处理的异常，那么整个Timer都会挂掉，也会影响到同一个队列里面的其他任务。\n","categories":["学知识"],"tags":["Java","Timer"]},{"title":"Java源码阅读 - 类加载的双亲委派模型","url":"/java-read-src-parent-delegation-model.html","content":"关于Java的类加载机制，尽管我看过几篇文章，知道个双亲委派模型，但是从来没钻进源码里看它到底是怎么委派的。\n\n\n什么双亲？怎么委派？我刚一开始听到双亲委派，还纳闷咋还双亲？后来才知道，这纯纯就是Parent Delegation这个词的误译。Parent这里指的并不是双亲，而是指父辈。所以看到有人翻译为“向上委托模型”，我感觉这个翻译更好一点，至于另一个翻译“啃老模型”……倒也没毛病……\n至于怎么委托，相信各位都背的滚瓜烂熟了。那就是，当类加载器收到类加载请求的时候，它首先会把这个请求委托给上一层的类加载器去尝试加载，直到委托到启动类加载器；只有当上一层类加载器无法完成这个加载请求的时候，次一级类加载器才会尝试自己加载。\n\n代码上的实现截图里面的代码就是ClassLoader#loadClass方法的实现，来自Liberica JDK 8。\n\n看得出来，逻辑还是很简单易懂的。一进来先加个锁，防止出现并发问题。然后检查这个类是不是已经被加载了。没被加载的话，就一层层向上委托，直到到达启动类加载器。如果上一层类加载器返回了null或者抛出了ClassNotFoundException异常，就说明它没找到这个类，那么本层类加载器就会尝试加载这个类，如果找不到的话，它就接着把请求交回下一层的类加载器。\n虽然上面的图和代码已经可以解释双亲委派的工作机制，但我还是喜欢调试进去看看代码具体是咋走的。所以我写了这么几行，用来调试类加载器。\npublic class Main &#123;    public static void main(String[] args) throws ClassNotFoundException &#123;        ClassLoader classLoader = Main.class.getClassLoader();        System.out.println(classLoader.toString());        // 在下面这一行下个断点，走到这一行之后再给 ClassLoader#loadClass 下个断点        classLoader.loadClass(&quot;DaemonThreadDemo&quot;);        classLoader.loadClass(&quot;java.util.HashMap&quot;);    &#125;&#125;\n\n断点从classLoader.loadClass(&quot;DaemonThreadDemo&quot;)这一行进去并停留在if (c == null)之后可以看到，目前类加载的委托请求是交给AppClassLoader，看得出来，这个就是应用类加载器。\n\n继续往下走到c = parent.loadClass(name, false)这一行，然后给parent变量添加一个监视，就可以看到接下来AppClassLoader要把这个类加载请求委托给ExtClassLoader，同理可得，这个就是扩展类加载器。\n\n接着往下走，继续调试ExtClassLoader，这时候可以看到parent是null。没有了parent，这个类加载器就会将这个类加载请求委托给启动类加载器并尝试加载这个类。\n\n逐层点进去，可以看到如下代码：\n\nemmmm……走到了一个native方法了呢……嘛，里面的代码先不管了，看名字能猜得出来，在这里会调用启动类加载器来尝试加载这个类。\n因为要加载的DaemonThreadDemo类并不归启动类加载器管，所以findBootstrapClassOrNull返回了null。ExtClassLoader得知启动类加载器加载失败，那么它自己就会再尝试加载。然而这个类也不归扩展类加载器管，所以在ExtClassLoader里面调用findClass方法会抛出ClassNotFoundException异常并返回到AppClassLoader。\n这时候，因为DaemonThreadDemo这个类归应用类加载器管，所以这一次调用findClass成功的找到了这个类。\n\n所以代码可以成功走到return c这一行，来完成一个类的加载。\n破坏双亲委派模型说到双亲委派模型，必会谈到怎么破坏它。看完上面的代码就明白了，我们可以自己创建一个自定义类加载器，并重写loadClass方法，不让它向上委派就行了。\n番外：尝试理解findBootstrapClass虽然这部分是C和C++的实现，但还是想硬着头皮尝试看一下。到Bellsoft的官网下载虚拟机的源码之后，我找到了FindBootStrapClass函数的实现：\n// 这部分代码在 java_md.c/* * The implementation for finding classes from the bootstrap * class loader, refer to java.h */static FindClassFromBootLoader_t *findBootClass = NULL;jclass FindBootStrapClass(JNIEnv *env, const char *classname)&#123;   HMODULE hJvm;   if (findBootClass == NULL) &#123;       hJvm = GetModuleHandle(JVM_DLL);       if (hJvm == NULL) return NULL;       /* need to use the demangled entry point */       findBootClass = (FindClassFromBootLoader_t *)GetProcAddress(hJvm,            &quot;JVM_FindClassFromBootLoader&quot;);       if (findBootClass == NULL) &#123;          JLI_ReportErrorMessage(DLL_ERROR4, &quot;JVM_FindClassFromBootLoader&quot;);          return NULL;       &#125;   &#125;   return findBootClass(env, classname);&#125;\n\n这个C语言……确实跟我大学学的C语言不一样啊……爬了些文，大概理解这里是要找JVM_FindClassFromBootLoader这个函数的实际地址，然后赋给findBootClass指针并执行它的代码。于是我接着挖到了JVM_FindClassFromBootLoader函数的实现。\n// 这部分代码在 jvm.cpp// Returns a class loaded by the bootstrap class loader; or null// if not found.  ClassNotFoundException is not thrown.//// Rationale behind JVM_FindClassFromBootLoader// a&gt; JVM_FindClassFromClassLoader was never exported in the export tables.// b&gt; because of (a) java.dll has a direct dependecy on the  unexported//    private symbol &quot;_JVM_FindClassFromClassLoader@20&quot;.// c&gt; the launcher cannot use the private symbol as it dynamically opens//    the entry point, so if something changes, the launcher will fail//    unexpectedly at runtime, it is safest for the launcher to dlopen a//    stable exported interface.// d&gt; re-exporting JVM_FindClassFromClassLoader as public, will cause its//    signature to change from _JVM_FindClassFromClassLoader@20 to//    JVM_FindClassFromClassLoader and will not be backward compatible//    with older JDKs.// Thus a public/stable exported entry point is the right solution,// public here means public in linker semantics, and is exported only// to the JDK, and is not intended to be a public API.JVM_ENTRY(jclass, JVM_FindClassFromBootLoader(JNIEnv* env,                                              const char* name))  JVMWrapper2(&quot;JVM_FindClassFromBootLoader %s&quot;, name);  // Java libraries should ensure that name is never null...  if (name == NULL || (int)strlen(name) &gt; Symbol::max_length()) &#123;    // It&#x27;s impossible to create this class;  the name cannot fit    // into the constant pool.    return NULL;  &#125;  TempNewSymbol h_name = SymbolTable::new_symbol(name, CHECK_NULL);  Klass* k = SystemDictionary::resolve_or_null(h_name, CHECK_NULL);  if (k == NULL) &#123;    return NULL;  &#125;  if (TraceClassResolution) &#123;    trace_class_resolution(k);  &#125;  return (jclass) JNIHandles::make_local(env, k-&gt;java_mirror());JVM_END\n\n好吧，更看不懂了。继续爬了文之后，理解这里大致做了三件事：从常量池中拿到类名的信息；查找类的信息并实例化Klass；将Klass对象转换为jclass类型并返回。\n算了，虚拟机源码就看到这吧……有兴趣的同志可以看JVM系列(四)：java方法的查找过程实现这篇博客。\n","categories":["学知识"],"tags":["Java","双亲委派","类加载"]},{"title":"Java源码阅读 - equals和hashCode方法","url":"/java-read-src-hashcode-and-equals.html","content":"做技术，不能只知其然而不知其所以然。在知道了工具的原理之后，才能更高效的使用这个工具。在程序的世界里，源码里面没有秘密，看懂了源码，也就看懂了原理。\n这次就来阅读一下Object类里面hashCode方法和equals方法的源码。\n\n\n先看看代码public native int hashCode();public boolean equals(Object obj) &#123;    return (this == obj);&#125;\n\n可以看出，hashCode方法是一个native方法，equals方法比较了两个对象是否指向同一个内存的地址。\nhashCode什么是hash要搞清楚hashCode干了什么，那就得要知道hash是什么。\n\n散列函数（英语：Hash function）又称散列算法、哈希函数，是一种从任何一种数据中创建小的数字“指纹”的方法。散列函数把消息或数据压缩成摘要，使得数据量变小，将数据的格式固定下来。该函数将数据打乱混合，重新创建一个叫做散列值（hash values，hash codes，hash sums，或hashes）的指纹。散列值通常用一个短的随机字母和数字组成的字符串来代表。好的散列函数在输入域中很少出现散列冲突。在散列表和数据处理中，不抑制冲突来区别数据，会使得数据库记录更难找到。\n\nJava中的hashCode方法在Object类中的hashCode方法是一个native方法，我们没办法直接得知它的实现方式，但是我们依旧可以从它的JavaDoc中得知一些信息。\n/**     * Returns a hash code value for the object. This method is     * supported for the benefit of hash tables such as those provided by     * &#123;@link java.util.HashMap&#125;.     * &lt;p&gt;     * The general contract of &#123;@code hashCode&#125; is:     * &lt;p&gt;     * As much as is reasonably practical, the hashCode method defined by     * class &#123;@code Object&#125; does return distinct integers for distinct     * objects. (This is typically implemented by converting the internal     * address of the object into an integer, but this implementation     * technique is not required by the     * Java&amp;trade; programming language.)     *     * @return  a hash code value for this object.     * @see     java.lang.Object#equals(java.lang.Object)     * @see     java.lang.System#identityHashCode     */    public native int hashCode();\n\n上面花了很大的篇幅讲了如果要重新实现hashCode方法所需要遵循的原则，但是很可惜，我们现在暂时不关注这些。我们现在关注的，是最后一段的内容。\n在最后一段中，它讲了通常情况下，程序是怎样计算出hashCode的值的。\n\nThis is typically implemented by converting the internal address of the object into an integer通常来说，这是通过把内部的地址转换成一个整型数来实现的\n\n当然，并不是所有的类都使用了这个计算方法，比如String就重新实现了自己的hashCode方法：\npublic int hashCode() &#123;    int h = hash;    if (h == 0 &amp;&amp; value.length &gt; 0) &#123;        char val[] = value;        for (int i = 0; i &lt; value.length; i++) &#123;            h = 31 * h + val[i];        &#125;        hash = h;    &#125;    return h;&#125;\n\nequalsequals方法的作用是比较两个对象的内容是否相同。一般来说，Object类中提供的equals方法是没办法满足各个类型自己的需要的，所以它们基本上都实现了自己的equals方法。\n用一个简单的例子来讲：\nString str1 = &quot;aaa&quot;;String str2 = &quot;aaa&quot;;str1.equals(str2); // true\n\n显然，str1和str2是两个不同的对象，如果直接比较它们的内存地址，那么得到的结果肯定是false。所以可以肯定的是，String类重写了equals方法。那么，我们就简单看一下String是怎样实现equals方法的：\npublic boolean equals(Object anObject) &#123;    // 先检查两个对象的地址是否相同    if (this == anObject) &#123;        return true;    &#125;    // 如果被比较的对象地址不同，但它类型相同    // 那么继续进行比较    if (anObject instanceof String) &#123;        String anotherString = (String)anObject;        int n = value.length;        // 如果被比较的字符串与本字符串长度相同        // 那么继续比较其中char数组中的每个元素是否相同        if (n == anotherString.value.length) &#123;            char v1[] = value;            char v2[] = anotherString.value;            int i = 0;            while (n-- != 0) &#123;                if (v1[i] != v2[i])                    return false;                i++;            &#125;            return true;        &#125;    &#125;    return false;&#125;\n\n既然每个类型都可以实现自己的equals方法，那么必然有一个规则来约束它们的实现方式，以保证在何时何地equals都可以得到合理的结果。\n在equals方法的JavaDoc中描述了重写该方法所需要遵守的规则：\n\nIt is reflexive: for any non-null reference value x, x.equals(x) should return true.It is symmetric: for any non-null reference values x and y, x.equals(y) should return true if and only if y.equals(x) returns true.It is transitive: for any non-null reference values x, y, and z, if x.equals(y) returns true and y.equals(z) returns true, then x.equals(z) should return true.It is consistent: for any non-null reference values x and y, multiple invocations of x.equals(y) consistently return true or consistently return false, provided no information used in equals comparisons on the objects is modified.For any non-null reference value x, x.equals(null) should return false.\n\n翻译过来就是：\n\n自反性：对于一个非null的引用值，x.equals(x)应当返回true。对称性：对于两个非null的引用值x和y，当且仅当y.equals(x)时，x.equals(y)返回true。传递性：对于任意非null的引用值x，y和z，如果x.equals(y)返回true，而且y.equals(z)返回true，那么x.equals(z)也应返回true。一致性：对于任意非null的引用值x和y，当两者都未被修改时，多次调用x.equals(y)都应一直返回true或者false。对于任意非null的引用值x，x.equals(null)应返回false。\n\nhashCode方法与equals方法的关系在equals方法的JavaDoc上有这么一段话：\n\nNote that it is generally necessary to override the hashCode method whenever this method is overridden, so as to maintain the general contract for the hashCode method, which states that equal objects must have equal hash codes.在重写equals方法时，通常也需要一并重写hashCode方法，以便维护hashCode方法的约定，即相等的对象必须拥有相同的哈希码\n\n而在hashCode方法的JavaDoc中，它有着这样的实现约定：\n\nWhenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application.\nIf two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result.\nIt is not required that if two objects are unequal according to the java.lang.Object#equals(java.lang.Object) method, then calling the hashCode method on each of the two objects must produce distinct integer results.  However, the programmer should be aware that producing distinct integer results for unequal objects may improve the performance of hash tables.\n\n即：\n\n在程序运行过程中，不论hashCode方法被调用了多少次，其返回结果都必须是一个恒定的整型值，以表明在使用equals比较对象时所需的信息没有被修改过。但是在程序每次运行之间，hashCode返回的值则不需要保持一致\n如果两个对象使用equals方法比较得出了相同(equal)的结论，那么对这两个对象执行hashCode方法得到的值也必须相同\n在两个对象使用equals方法比较得出了不相同(not equal)的结论时，对这两个对象执行hashCode方法得到的值却可以相同。然而，开发人员需要意识到，给不同的对象返回不同的哈希码可以提升hash table的性能\n\n综上所述，我们可以得出如下结论：\n\n两个相同(equal)的对象必须拥有相同的哈希码\n两个哈希码相同的对象却不一定相同(equal)\n\n那么，这两条结论会对我们的程序造成什么影响呢？\n首先我们看一下第一条。以Set举例，Set会根据对象的hashCode来寻找对象的存储位置，那么可想而知，如果两个对象的值相同，哈希码却不同，那么就会导致Set中出现多个重复数据的情况。\n而第二条结论出现的原因则是，目前没有任何一种哈希算法，可以保证对每个传入的值都可以计算出一个不同的哈希，这种情况的学名叫哈希碰撞，所以我们只能尽可能的减少出现哈希碰撞的可能性。至于Java如何应对哈希碰撞，我将在后续的博文中进行解释。\n","categories":["学知识"],"tags":["Java","LinkedList"]},{"title":"去掉自定义异常中的堆栈跟踪信息","url":"/java-remove-stack-trace-in-customized-exceptions.html","content":"/** * 业务异常基类 */public abstract class BaseBizException extends RuntimeException &#123;    public BaseBizException(String message) &#123;        super(message);    &#125;    /**     * 覆盖fillInStackTrace()方法，抹掉异常中的堆栈跟踪信息     */    @Override    public synchronized Throwable fillInStackTrace() &#123;        return this;    &#125;&#125;\n","categories":["小技巧"],"tags":["Java","代码技巧"]},{"title":"循序渐进写一个Servlet(1) - 介绍相关的接口和类","url":"/java-servlet-1-introducing-classes-and-interfaces.html","content":"Servlet（Server Applet），全称Java Servlet，是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。本系列将一步步地写出一个Servlet程序。\n这篇博文将介绍一些后面会用到的接口和类。\n\n\n常用的类和接口javax.servlet.Servletjavax.servlet.Servlet是一个接口，它定义了一个servlet必须要实现的方法，包括如何初始化一个servlet，如何让这个servlet处理请求，以及如何将这个servlet从容器中移除，即“生命周期方法”。它们被调用的顺序，即servlet的生命周期，是这样的：\n\n容器生成这个servlet的对象，然后调用其init()方法完成初始化步骤\n当收到请求之后，会调用其service()方法来处理请求\n当这个servlet停止服务时，destroy()方法会被调用并准备销毁\n\nServlet的启动时间由web.xml中load-on-startup属性决定，当值为负数或未设定该属性时，容器可以自由决定何时初始化该servlet；当值为零或正整数时，容器需要在启动时就初始化该servlet，此时该属性的值代表初始化的优先级，值越小优先级越高，对于有相同优先级的servlet，容器可以自行决定加载顺序。\n当应用被重新部署，或者在容器停机时，servlet会被销毁，同时servet无法被手动销毁。\n除了生命周期方法外，Servlet接口还有两个方法：\n\ngetServletConfig()可以让servlet取到启动相关的信息\ngetServletInfo()用来获取servlet相关的信息，如作者、版本号、版权信息等\n\njavax.servlet.GenericServletGenericServlet类是一个抽象类，定义了一个协议无关的，通用的servlet实现。\n它预置了ServletConfig接口的实现，并且简易实现了Servlet接口中的init()方法和destroy()方法，开发人员只需要覆盖service()方法。\njavax.servlet.http.HttpServletHttpServlet类提供了一个用于处理HTTP请求的servlet基类。一个HttpServlet的子类至少需要覆盖一个方法，而且通常是覆盖如下几个方法：\n\ndoGet()，以实现对HTTP GET请求的处理\ndoPost()，以实现对HTTP POST请求的处理\ndoPut()，以实现对HTTP PUT请求的处理\ndoDelete()，以实现对HTTP DELETE请求的处理\ninit()和destroy()，以管理servlet生命周期内所需的资源\ngetServletInfo()，可以用来自定义servlet返回哪些关于自身的信息\n\n此外，不建议直接覆盖service()方法，因为HttpServlet#service()方法中已经实现了根据HTTP请求类型调用对应的doXXX()方法。\n如果某个HTTP方法对应的doXXX()方法没有被覆盖，则视为该servlet不支持这个HTTP方法。如在没有覆盖doGet()时收到HTTP GET请求，则会返回HTTP 405 METHOD NOT ALLOWED (对应HTTP 1.1)错误码，或HTTP 400 BAD REQUEST (对应其他HTTP版本)错误码。\n需要注意的是，HttpServlet类并没有实现HTTP CONNECT和HTTP PATCH方法。当请求这两个方法，或其他非标准方法时，将会返回HTTP 501 NOT IMPLEMENTED错误码。\n常用的常量\nHTTP方法相关的常量位于HttpServlet类中，是String类型，命名规则为METHOD_方法，如METHOD_GET\nHTTP状态码相关的常量位于HttpServletResponse类中，是int类型，命名规则为SC_状态码名，如SC_OK、SC_NOT_FOUND\n\n系列博文\n循序渐进写一个Servlet(1) - 介绍相关的接口和类\n循序渐进写一个Servlet(2) - 第一个servlet\n循序渐进写一个Servlet(3) - 分别处理GET和POST\n循序渐进写一个Servlet(4) - 会话追踪\n循序渐进写一个Servlet(5) - Filter\n\n","categories":["学知识"],"tags":["Java","Servlet"]},{"title":"循序渐进写一个Servlet(2) - 第一个servlet","url":"/java-servlet-2-first-servlet.html","content":"Servlet（Server Applet），全称Java Servlet，是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。本系列将一步步地写出一个Servlet程序。\n这篇博文将演示如何创建一个servlet。\n\n\n新建项目首先，使用Maven的maven-archetype-webapp创建一个Java Webapp项目。这样就可以得到一个Java Webapp项目的基本骨架。将项目命名为servletdemo。\n创建Servlet对象新建一个名为DemoServlet的类。\n因为GenericServlet已经实现了一个servlet的生命周期方法，而且这里也不需要对生命周期方法做定制化，所以直接继承一个GenericServlet就可以。\npublic class DemoServlet extends GenericServlet &#123;    @Override    public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123;    &#125;&#125;\n\n实现这个servlet的功能service()方法定义了这个servlet的具体实现，这里先让它返回一个HTML。\n@Overridepublic void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123;    // 设定返回内容的MIME类型    res.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    res.setCharacterEncoding(&quot;utf-8&quot;);    // Java 8 新特性 - try with resources    // 该特性优点在于，try代码块执行完毕后，会自动close相关资源    // 要求资源所属的类必须实现了 AutoCloseable 或 Closeable 接口    try (PrintWriter writer = res.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n注册到容器光有servlet还不够，下面还需要让容器知道有这个servlet，以及知道要把哪些请求发往这个servlet。这里可以通过修改web.xml的方式，也可以通过注解的方式完成注册。\nweb.xmlweb.xml位于src/main/webapp/WEB_INF/web.xml。\n向其中添加如下内容：\n&lt;!-- servlet属性定义了一个servlet的名字和对应的类 --&gt;&lt;servlet&gt;    &lt;servlet-name&gt;DemoServlet&lt;/servlet-name&gt;    &lt;servlet-class&gt;com.boris.tomcatlistener.servlet.DemoServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;!-- servlet-mapping属性定义了匹配某个URL的请求应该发往哪个servlet --&gt;&lt;servlet-mapping&gt;    &lt;!-- 目标servlet的servlet-name --&gt;    &lt;servlet-name&gt;DemoServlet&lt;/servlet-name&gt;    &lt;!-- 匹配的URL --&gt;    &lt;url-pattern&gt;/demoServlet&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;\n\n注解WebServlet(String value)注解表明了这个类是一个servlet，其中value的值等同于web.xml中url-patterm参数。\n在项目部署时，容器会处理WebServlet注解，并将这个servlet与指定的URL pattern绑定。\n@WebServlet(value = &quot;/demoServlet&quot;)public class DemoServlet extends GenericServlet &#123;    @Override    public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123;        // 设定返回内容的MIME类型        res.setContentType(&quot;text/html&quot;);        // 设定内容以UTF-8编码        res.setCharacterEncoding(&quot;utf-8&quot;);        // Java 8 新特性 - try with resources        // 该特性优点在于，try代码块执行完毕后，会自动close相关资源        // 要求资源所属的类必须实现了 AutoCloseable 或 Closeable 接口        try (PrintWriter writer = res.getWriter()) &#123;            // 开始输出HTML文本            writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);            writer.print(&quot;&lt;body&gt;&quot;);            writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);            writer.print(&quot;&lt;/body&gt;&quot;);            writer.print(&quot;&lt;/html&gt;&quot;);        &#125;    &#125;&#125;\n\n运行起来吧，servlet哟！在上面步骤中，创建一个servlet的所有步骤就完成了，尽管这个servlet的功能及其有限，它甚至不能区分发来的HTTP请求。\n那么，就将这个servlet部署到容器中，让它开始工作吧。启动成功后，使用浏览器，或者HTTP请求构造工具，向http://localhost:8080/servletdemo/DemoServlet（假设Tomcat在监听8080端口）发一条请求。\n如果一切正常的话，就可以看到服务器返回了上面service()方法中指定的内容。\n系列博文\n循序渐进写一个Servlet(1) - 介绍相关的接口和类\n循序渐进写一个Servlet(2) - 第一个servlet\n循序渐进写一个Servlet(3) - 分别处理GET和POST\n循序渐进写一个Servlet(4) - 会话追踪\n循序渐进写一个Servlet(5) - Filter\n\n","categories":["学知识"],"tags":["Java","Servlet"]},{"title":"循序渐进写一个Servlet(3) - 分别处理GET和POST","url":"/java-servlet-3-handle-get-and-post.html","content":"Servlet（Server Applet），全称Java Servlet，是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。本系列将一步步地写出一个Servlet程序。\n这篇博文将演示如何分别处理GET和POST请求，以及处理请求中的参数。\n\n\n编写doGet()和doPost()方法首先把要实现的功能写好，后面才好调用不是。\npublic void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doGet()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\npublic void doPost(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doPost()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n区分HTTP方法因为servlet是调用service()方法来处理请求的，所以对请求做区分也需要在service()方法中进行。\n@Overridepublic void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123;    HttpServletRequest httpServletRequest = (HttpServletRequest) req;    HttpServletResponse httpServletResponse = (HttpServletResponse) res;    if (&quot;GET&quot;.equalsIgnoreCase(httpServletRequest.getMethod())) &#123;        doGet(httpServletRequest, httpServletResponse);    &#125; else if (&quot;POST&quot;.equalsIgnoreCase(httpServletRequest.getMethod())) &#123;        doPost(httpServletRequest, httpServletResponse);    &#125; else &#123;        // 如果请求既不是GET也不是POST        // 那么就返回HTTP 501 NOT IMPLEMENTED状态码        // 毕竟不能把请求直接扔了，总是要有个返回的        httpServletResponse.sendError(501);    &#125;&#125;\n\n运行起来看看效果首先发个GET请求\n\n再发个POST请求\n\n为什么不用HttpServlet类呢没错，上面做的，就是自己实现了一个简陋的HttpServlet类，因为是循序渐进嘛，没头没脑的直接砸上来一个，算什么循序渐进。\n那么现在就让DemoServlet继承HttpServlet。同时，因为HttpServlet已经在service()方法中实现了判断请求类型，所以DemoServlet中不要覆盖service()方法，只覆盖doGet()和doPost()方法。\npublic class DemoServlet extends HttpServlet &#123;    @Override    public void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;        // 设定返回内容的MIME类型        response.setContentType(&quot;text/html&quot;);        // 设定内容以UTF-8编码        response.setCharacterEncoding(&quot;utf-8&quot;);        try (PrintWriter writer = response.getWriter()) &#123;            // 开始输出HTML文本            writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);            writer.print(&quot;&lt;body&gt;&quot;);            writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);            writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doGet()&lt;/code&gt;&lt;/b&gt;&quot;);            writer.print(&quot;&lt;/body&gt;&quot;);            writer.print(&quot;&lt;/html&gt;&quot;);        &#125;    &#125;    @Override    public void doPost(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;        // 设定返回内容的MIME类型        response.setContentType(&quot;text/html&quot;);        // 设定内容以UTF-8编码        response.setCharacterEncoding(&quot;utf-8&quot;);        try (PrintWriter writer = response.getWriter()) &#123;            // 开始输出HTML文本            writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);            writer.print(&quot;&lt;body&gt;&quot;);            writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);            writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doPost()&lt;/code&gt;&lt;/b&gt;&quot;);            writer.print(&quot;&lt;/body&gt;&quot;);            writer.print(&quot;&lt;/html&gt;&quot;);        &#125;    &#125;&#125;\n\n处理请求中的参数HTTP请求是可以带参数的，有了参数，那就得处理。\n处理GET请求的参数GET请求里带的参数，名字叫查询字符串(query string)，是一组或多组key=value格式的键值对。\nQuery string写在URL后面，以一个问号起头，用&amp;分隔各个键值对，即类似http://localhost:8080/appname/servlet?arg1=value1&amp;arg2=value2&amp;...&amp;argN=valueN。\n在代码里使用HttpServletRequest#getQueryString()方法，就可以获取到问号后面的query string，分别用&amp;和=分割字符串，就可以取到每个参数的key和value。\n@Overridepublic void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    /*    String queryString = request.getQueryString();    String[] queryStrings;    if (queryString != null) &#123;        queryStrings = queryString.split(&quot;&amp;&quot;);    &#125; else &#123;        queryStrings = new String[]&#123;&#125;;    &#125;    */    // 使用Optional类简化null判断    Optional&lt;String&gt; optionalQueryString = Optional.ofNullable(request.getQueryString());    // 如果有query string则取出每个参数    // 如果没有则返回一个空的String数组    String[] queryStrings = optionalQueryString.isPresent() ? optionalQueryString.get().split(&quot;&amp;&quot;) : new String[]&#123;&#125;;    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doGet()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 遍历每个参数        for (String query : queryStrings) &#123;            // 取出参数的key和value            String[] q = query.split(&quot;=&quot;);            writer.print(q[0] + &quot; = &quot; + q[1] + &quot;&lt;br&gt;&quot;);        &#125;        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n运行一下，结果是这样子的：\n\n处理POST请求的参数POST请求的参数就叫参数(parameter)，位于请求体(body)里，格式由Content-Type请求头决定。详细介绍可以参考这篇MDN文档。\nHttpServletRequest#getParameterMap()方法可以取出请求中的所有参数，并放到一个Map中。\n@Overridepublic void doPost(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    // 取出所有参数，得到一个Map    Map parameterMap = request.getParameterMap();    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doPost()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 遍历parameterMap        parameterMap.forEach((k, v) -&gt; writer.print(k + &quot; = &quot; + ((String[]) v)[0] + &quot;&lt;br&gt;&quot;));        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n本文中将使用application/x-www-form-urlencoded格式做示例。\n\n系列博文\n循序渐进写一个Servlet(1) - 介绍相关的接口和类\n循序渐进写一个Servlet(2) - 第一个servlet\n循序渐进写一个Servlet(3) - 分别处理GET和POST\n循序渐进写一个Servlet(4) - 会话追踪\n循序渐进写一个Servlet(5) - Filter\n\n","categories":["学知识"],"tags":["Java","Servlet"]},{"title":"循序渐进写一个Servlet(4) - 会话追踪","url":"/java-servlet-4-session-tracking.html","content":"Servlet（Server Applet），全称Java Servlet，是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。本系列将一步步地写出一个Servlet程序。\n这篇博文将演示如何使用cookie和session进行会话追踪。\n\n\nHTTP协议是一个无状态的协议，也就是说，在服务器眼中，每一个HTTP请求都是一个全新的请求，每个请求之间没有关联。所以我们需要一个可以管理请求中携带的用户信息的方法。而会话追踪就是一个可以管理用户信息的方法。\n会话追踪可以通过下列几个方式实现：\n\nCookie\n表单隐藏域\nURL改写\nHttpSession\n\n本文将主要演示Cookie和HttpSession的用法。\nCookie什么是cookieCookie是一串可以持久化于各个请求之间的信息片段。每个cookie都有一个名字，并有一个值，同时可以包含备注、路径、域名、过期时间、版本等附加信息。\nCookie有两种：\n\n非持久cookie，这种cookie只在会话中存留，并且不具有过期时间属性，一旦用户关闭浏览器(或者标签页)，也就是使这个会话失效，这个cookie就会丢失。\n持久化cookie，这种cookie可以被用于多个会话中，而且只会在到达过期时间，或者用户主动使该cookie失效后，才会被删除。\n\n可以使用HttpServletResponse#addCookie(Cookie)方法在HTTP响应中携带cookie。\n保存cookie首先修改前文中的doPost()方法，将请求中的参数取出来，并存入cookie。\n@Overridepublic void doPost(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    // 取出所有参数，得到一个Map    Map parameterMap = request.getParameterMap();    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doPost()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 输出当前session的ID        writer.print(&quot;&lt;b&gt;Session ID: &quot; + request.getSession().getId() + &quot;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 遍历parameterMap        parameterMap.forEach((k, v) -&gt; &#123;            // 将参数以 key = value 形式输出            writer.print(k + &quot; = &quot; + ((String[]) v)[0] + &quot;&lt;br&gt;&quot;);            // 将参数的key作为cookie的name，参数的value作为cookie的value            response.addCookie(new Cookie(String.valueOf(k), ((String[]) v)[0]));        &#125;);        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n然后发送一个POST请求，在返回中可以看到请求中的参数已经被放到cookie中，并返回到了客户端。\n\n使用cookie一旦cookie被保存到了客户端，那么在下次访问这个cookie所对应的地址时，客户端就会自动将相关的cookie带入请求一并发送到服务端。所以客户端不需要对cookie主动做任何操作。\n修改前文中的doGet()方法，使其可以取出cookie的值，并输出到页面上。\n@Overridepublic void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    // 使用Optional类简化null判断    Optional&lt;String&gt; optionalQueryString = Optional.ofNullable(request.getQueryString());    // 取出每个参数    String[] queryStrings = optionalQueryString.isPresent() ? optionalQueryString.get().split(&quot;&amp;&quot;) : new String[]&#123;&#125;;    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doGet()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 输出当前session的ID        writer.print(&quot;&lt;b&gt;Session ID: &quot; + request.getSession().getId() + &quot;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Parameters: &lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 遍历每个参数        for (String query : queryStrings) &#123;            // 取出参数的key和value            String[] q = query.split(&quot;=&quot;);            writer.print(q[0] + &quot; = &quot; + q[1] + &quot;&lt;br&gt;&quot;);        &#125;        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Cookies:&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 从request中取出cookie        Cookie[] cookies = request.getCookies();        // 遍历各个cookie        for (Cookie cookie : cookies) &#123;            // 输出其name和value            writer.print(cookie.getName() + &quot; = &quot; + cookie.getValue());            writer.print(&quot;&lt;br&gt;&quot;);        &#125;        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n然后发送一个GET请求，在返回中可以看到cookie中的内容已经被输出到页面上。\n\n删除cookie将cookie的存活时间设为0，并返回到客户端，即可从客户端中删除这个cookie。\n@Overrideprotected void doDelete(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;    Cookie[] cookies = req.getCookies();    for (Cookie cookie : cookies) &#123;        // JSESSIONID存放的是当前session的ID        // 如果删掉这个cookie，那么当前的session也会被丢弃        if (!&quot;JSESSIONID&quot;.equalsIgnoreCase(cookie.getName())) &#123;            // 设定存活时间为0秒            cookie.setMaxAge(0);            // 将修改过的cookie放入响应中返回到客户端            resp.addCookie(cookie);        &#125;    &#125;    try (PrintWriter writer = resp.getWriter()) &#123;        // 因为response需要输出到客户端，才可以使新的cookie被送到客户端        // 但是又懒得输出那么多东西了        // 所以就输出了一个空字符串        // 实际上输出内容不影响对cookie的操作        writer.print(&quot;&quot;);    &#125;&#125;\n\nHttpSession什么是sessionSession记录着一次会话相关的信息。\n当一个请求到达服务器后，服务器会检查请求中是否包含session ID信息，比如在Tomcat中就是检查有无JSESSIONID这个cookie，或者URL中有无JSESSIONID这个查询字符串。如果找到了对应的session，则服务器会将这个session检索出来使用；请求中没有包含session ID，或者对应的session已经被销毁，则服务器会创建一个新的session并返回其ID。\nSession ID通常以cookie的形式返回到客户端，如果客户端禁用了cookie，那么服务端则会使用URL重写技术将session ID写到URL中。\nSession中可以键值对的形式保存附加数据，称为attributes。\n与cookie不同，session保存于服务器端，而且它能保存的数据也不仅限于字符串。\n保存attribute修改doPost()方法，编写修改session的代码。修改完成后发送一个带有参数的POST请求，以向session中写入一些数据。\n@Overridepublic void doPost(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    // 取出所有参数，得到一个Map    Map parameterMap = request.getParameterMap();    // 获取当前会话的session    // 如果没有，则会新建一个session并返回其ID    HttpSession session = request.getSession();    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doPost()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 输出当前session的ID        writer.print(&quot;&lt;b&gt;Session ID: &quot; + request.getSession().getId() + &quot;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 遍历parameterMap        parameterMap.forEach((k, v) -&gt; &#123;            // 将参数以 key = value 形式输出            writer.print(k + &quot; = &quot; + ((String[]) v)[0] + &quot;&lt;br&gt;&quot;);            // 将参数的key作为cookie的name，参数的value作为cookie的value            Cookie cookie = new Cookie(String.valueOf(k), ((String[]) v)[0]);            response.addCookie(cookie);            // 将各个参数放到session的attributes中            session.setAttribute(String.valueOf(k), ((String[]) v)[0]);        &#125;);        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n取出attribute修改doGet()方法，使其可以从session中取出attributes并显示在页面上。\n@Overridepublic void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException &#123;    // 设定返回内容的MIME类型    response.setContentType(&quot;text/html&quot;);    // 设定内容以UTF-8编码    response.setCharacterEncoding(&quot;utf-8&quot;);    // 使用Optional类简化null判断    Optional&lt;String&gt; optionalQueryString = Optional.ofNullable(request.getQueryString());    // 获取当前会话的session    HttpSession session = request.getSession();    // 取出所有attribute的name    Enumeration&lt;String&gt; attributeNames = session.getAttributeNames();    // 取出每个参数    String[] queryStrings = optionalQueryString.isPresent() ? optionalQueryString.get().split(&quot;&amp;&quot;) : new String[]&#123;&#125;;    try (PrintWriter writer = response.getWriter()) &#123;        // 开始输出HTML文本        writer.print(&quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot;);        writer.print(&quot;&lt;body&gt;&quot;);        writer.print(&quot;&lt;b&gt;Response from DemoServlet&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Handled by &lt;code&gt;doGet()&lt;/code&gt;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 输出当前session的ID        writer.print(&quot;&lt;b&gt;Session ID: &quot; + request.getSession().getId() + &quot;&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Parameters: &lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 遍历每个参数        for (String query : queryStrings) &#123;            // 取出参数的key和value            String[] q = query.split(&quot;=&quot;);            writer.print(q[0] + &quot; = &quot; + q[1] + &quot;&lt;br&gt;&quot;);        &#125;        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Cookies:&lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 从request中取出cookie        Cookie[] cookies = request.getCookies();        // 遍历各个cookie        for (Cookie cookie : cookies) &#123;            // 输出其name和value            writer.print(cookie.getName() + &quot; = &quot; + cookie.getValue());            writer.print(&quot;&lt;br&gt;&quot;);        &#125;        writer.print(&quot;&lt;br&gt;&quot;);        writer.print(&quot;&lt;b&gt;Attributes: &lt;/b&gt;&quot;);        writer.print(&quot;&lt;br&gt;&quot;);        // 遍历attribute的各个name        while(attributeNames.hasMoreElements()) &#123;            String key = attributeNames.nextElement();            // 取出attribute的值            String value = String.valueOf(session.getAttribute(key));            writer.print(key + &quot; = &quot; + value);            writer.print(&quot;&lt;br&gt;&quot;);        &#125;        writer.print(&quot;&lt;/body&gt;&quot;);        writer.print(&quot;&lt;/html&gt;&quot;);    &#125;&#125;\n\n然后发送一个GET请求，在返回中就可以看到刚才保存在session中的数据：\n\n删除attribute此外HttpSession类提供了removeAttribute()方法用于删除一个attribute。\n@Overrideprotected void doDelete(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123;    Cookie[] cookies = req.getCookies();    // 获取当前会话的session    HttpSession session = req.getSession();    Enumeration&lt;String&gt; attributeNames = session.getAttributeNames();    for (Cookie cookie : cookies) &#123;        // JSESSIONID存放的是当前session的ID        // 如果删掉这个cookie，那么当前的session也会被丢弃        if (!&quot;JSESSIONID&quot;.equalsIgnoreCase(cookie.getName())) &#123;            // 设定存活时间为0秒            cookie.setMaxAge(0);            // 将修改过的cookie放入响应中返回到客户端            resp.addCookie(cookie);        &#125;        // 遍历attribute names        while(attributeNames.hasMoreElements()) &#123;            String key = attributeNames.nextElement();            // 将其从session中移除            session.removeAttribute(key);        &#125;    &#125;    try (PrintWriter writer = resp.getWriter()) &#123;        // 因为response需要输出到客户端，才可以使新的cookie被送到客户端        // 但是又懒得输出那么多东西了        // 所以就输出了一个空字符串        // 实际上输出内容不影响对cookie的操作        writer.print(&quot;&quot;);    &#125;&#125;\n\n系列博文\n循序渐进写一个Servlet(1) - 介绍相关的接口和类\n循序渐进写一个Servlet(2) - 第一个servlet\n循序渐进写一个Servlet(3) - 分别处理GET和POST\n循序渐进写一个Servlet(4) - 会话追踪\n循序渐进写一个Servlet(5) - Filter\n\n","categories":["学知识"],"tags":["Java","Servlet"]},{"title":"循序渐进写一个Servlet(5) - Filter","url":"/java-servlet-5-filter.html","content":"Servlet（Server Applet），全称Java Servlet，是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。本系列将一步步地写出一个Servlet程序。\n这篇博文将演示如何创建和使用filter。\n\n\n什么是Filter当客户端向servlet容器发送请求时，请求通常会直接发送到servlet进行处理，就像下图这样：\n\n但是，如果希望在请求被servlet处理之前和之后，再进行一些附加的处理，就可以使用Filter完成。\n\n一个常见的使用场景是，在filter中定义如何检查请求是否合法，比如请求头中是否携带了有效的认证和鉴权信息；或者可以在filter中针对请求和响应记录日志。\n怎么使用Filterjavax.servlet.Filter接口定义了一个filter的生命周期，要创建一个filter，就要实现Filter接口。\nFilter接口包含下列方法声明：\n\ninit()，用于定义在初始化这个filter时要执行的操作，该方法在filter的生命周期内只会执行一次；\ndoFilter()，用于定义这个filter要进行的操作，每当有请求被发送到与该filter绑定的资源时，该方法都会被执行一次；\ndestroy()，用于定义在停止这个filter时要执行的操作，只会在一个filter被销毁时执行。\n\n创建一个实现Filter接口的类public class Filter1 implements Filter &#123;    @Override    public void init(FilterConfig filterConfig) throws ServletException &#123;    &#125;    @Override    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123;    &#125;    @Override    public void destroy() &#123;    &#125;&#125;\n\n定义这个filter的行为在doFilter()方法内定义这个filter的行为。\n@Overridepublic void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123;    // 在处理请求前打印信息    System.out.println(&quot;Request passing through Filter 1&quot;);    // 交由FilterChain将请求交给下一个filter或交给servlet处理    chain.doFilter(request, response);    // servlet发送响应后打印信息    System.out.println(&quot;Response passing througe Filter 1&quot;);&#125;\n\n在容器中注册filter与servlet一样，filter也需要在容器中注册之后才能发挥作用。注册filter也有两种方式：通过web.xml，或者通过@WebFilter注解。\n这里有一点需要注意，虽然filter之间没有依赖关系，但是如果要保证filter的执行顺序，那么必须使用web.xml来注册。\nServlet 3.0 规范的8.2.3节中有如下说明：\n\nIf the order in which the listeners, servlets, filters are invoked is important to an application then a deployment descriptor must be used.\n\n因为使用注解注册的filter，其调用顺序没有在规范中指定。\n\nAs described above, when using annotations to define the listeners, servlets and filters, the order in which they are invoked is unspecified.\n\n如果一定要使用注解并保证filter的执行顺序，那么可以参考Stack Overflow中这篇回答。\n为了演示filter的执行顺序，这里再增加一个名为Filter2的filter，内容与Filter1类似。\nweb.xml在web.xml中增加如下配置：\n&lt;filter&gt;    &lt;filter-name&gt;filter1&lt;/filter-name&gt;    &lt;filter-class&gt;com.boris.tomcatlistener.filter.Filter1&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt;    &lt;filter-name&gt;filter1&lt;/filter-name&gt;    &lt;url-pattern&gt;/demoServlet&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter&gt;    &lt;filter-name&gt;filter2&lt;/filter-name&gt;    &lt;filter-class&gt;com.boris.tomcatlistener.filter.Filter2&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt;    &lt;filter-name&gt;filter2&lt;/filter-name&gt;    &lt;url-pattern&gt;/demoServlet&lt;/url-pattern&gt;&lt;/filter-mapping&gt;\n\nfilter标签描述了一个filter的基本信息，其中filter名称(filter-name)和filter所在类(filter-class)为必填项。\nfilter-mapping标签描述了一个filter将与哪个URL或者与哪个servlet绑定，filter-name指定使用哪个filter处理请求，url-pattern指定发往哪个URL的请求会触发这个filter，servlet-name指定发往哪个servlet的请求会触发这个filter。url-pattern和servlet-name可以同时存在，也可以同时存在多个。\nfilter-mapping标签的先后顺序，将决定filter链中各个filter被调用的先后顺序。如上文中先配置了filter1后配置了filter2，那么在请求到达时，会先执行filter1然后再执行filter2。\n配置完毕后部署并运行该项目，向http://localhost:8080/servletdemo/DemoServlet发送一个请求，在控制台可以看到如下输出：\nRequest passing through Filter 1Request passing through Filter 2Response passing througe Filter 2Response passing througe Filter 1\n\n@WebFilter注解@WebFilter是Servlet 3.0中新增的特性，在Tomcat 7及以前版本中将无法工作。\n以Filter1为例，为其添加如下注解：\n@WebFilter(    filterName = &quot;filter1&quot;,    urlPatterns = &quot;/demoServlet&quot;)\n\nfilter-name属性指定了这个filter的名称。\n有三个属性可以指定filter的触发条件：\n\nvalue\nurlPatterns\nservletNames\n\n以上三个属性都可以接受一个字符串，或者用大括号包括起来的多个字符串。\n在注解只有一个参数，并且该参数是指定要匹配的URL时，建议使用value属性，比如这样：\n// value为默认的属性@WebFilter(&quot;/demoServlet&quot;)// 显式指定value属性@WebFilter(value = &quot;/demoServlet&quot;)\n\n否则，建议使用urlPatterns属性和servletNames属性。不允许value和urlPatterns同时出现。\nServlet 3.0 规范的8.1.2 @WebFilter节中说明原文如下：\n\nIt is recommended to use value when the only attribute on the annotation is the url pattern and to use the urlPatterns attribute when the other attributes are also used. It is illegal to have both value and urlPatterns attribute used together on the same annotation.\n\n配置完毕后部署并运行该项目，向http://localhost:8080/servletdemo/DemoServlet发送一个请求，在控制台可以看到如下输出：\nRequest passing through Filter 1Response passing througe Filter 1\n\n系列博文\n循序渐进写一个Servlet(1) - 介绍相关的接口和类\n循序渐进写一个Servlet(2) - 第一个servlet\n循序渐进写一个Servlet(3) - 分别处理GET和POST\n循序渐进写一个Servlet(4) - 会话追踪\n循序渐进写一个Servlet(5) - Filter\n\n","categories":["学知识"],"tags":["Java","Servlet","Filter"]},{"title":"理解Java中的抽象队列同步器（AQS）","url":"/java-understand-aqs.html","content":"最近项目里用到了些Lock，爬了些文了解到它们是基于AbstractQueuedSynchronizer（即AQS）实现的。那么，不如趁热打铁，看看里面是怎么工作的。\n\n\n什么是AQSAbstractQueuedSynchronizer，抽象队列同步器，是很多同步器（如ReentrantLock、CountDownLatch、Semaphore）等都是基于它实现的。\n在AQS内部，它维护了一个FIFO队列，和一个volatile类型的变量state。FIFO队列用来实现多线程的排队工作，线程加锁失败时，这个线程就会被封装成一个Node节点放到队尾，然后当锁被释放后，队列头部的线程就会被唤醒并让它重新尝试获取锁；state变量用来记录锁的状态，如Semaphore的permit就是存在state里面的。\n状态管理上面说到，AQS使用一个volatile的int变量state来管理锁的状态，state为0时说明锁被释放，反之锁被持有。\nAQS提供了三个方法来同步锁的状态：getState()，setState(int newState)和compareAndSetState(int expect, int update)。\n/** * The synchronization state. */private volatile int state;/** * Returns the current value of synchronization state. * This operation has memory semantics of a &#123;@code volatile&#125; read. * @return current state value */protected final int getState() &#123;    return state;&#125;/** * Sets the value of synchronization state. * This operation has memory semantics of a &#123;@code volatile&#125; write. * @param newState the new state value */protected final void setState(int newState) &#123;    state = newState;&#125;/** * Atomically sets synchronization state to the given updated * value if the current state value equals the expected value. * This operation has memory semantics of a &#123;@code volatile&#125; read * and write. * * @param expect the expected value * @param update the new value * @return &#123;@code true&#125; if successful. False return indicates that the actual *         value was not equal to the expected value. */protected final boolean compareAndSetState(int expect, int update) &#123;    return U.compareAndSetInt(this, STATE, expect, update);&#125;\n\n查看setState方法的引用，不难发现像CountDownLatch和Semaphore这些熟悉的身影。\n\nFIFO队列 - 线程排队等待锁的地方在AQS内部，未能成功获取锁的线程都会被包装成一个Node节点，然后放到FIFO队列尾部让它等待。\n// Node status bits, also used as argument and return valuesstatic final int WAITING   = 1;          // must be 1static final int CANCELLED = 0x80000000; // must be negativestatic final int COND      = 2;          // in a condition waitabstract static class Node &#123;    volatile Node prev;       // initially attached via casTail    volatile Node next;       // visibly nonnull when signallable    Thread waiter;            // visibly nonnull when enqueued    volatile int status;      // written by owner, atomic bit ops by others    // 略&#125;/** * Head of the wait queue, lazily initialized. */private transient volatile Node head;/** * Tail of the wait queue. After initialization, modified only via casTail. */private transient volatile Node tail;/** * Enqueues the node unless null. (Currently used only for * ConditionNodes; other cases are interleaved with acquires.) */final void enqueue(Node node) &#123;    if (node != null) &#123;        for (;;) &#123;            Node t = tail;            node.setPrevRelaxed(t);        // avoid unnecessary fence            if (t == null)                 // initialize                tryInitializeHead();            else if (casTail(t, node)) &#123;                t.next = node;                if (t.status &lt; 0)          // wake up to clean link                    LockSupport.unpark(node.waiter);                break;            &#125;        &#125;    &#125;&#125;\n\nSemaphoreSemaphore就是AQS的一个实现，从它的源码就能很容易看出来，它内部就是通过AQS的state来管理permits。\npublic class Semaphore implements java.io.Serializable &#123;    /** All mechanics via AbstractQueuedSynchronizer subclass */    private final Sync sync;    /**     * Synchronization implementation for semaphore.  Uses AQS state     * to represent permits. Subclassed into fair and nonfair     * versions.     */    abstract static class Sync extends AbstractQueuedSynchronizer &#123;        Sync(int permits) &#123;            setState(permits);        &#125;        final int getPermits() &#123;            return getState();        &#125;        final int nonfairTryAcquireShared(int acquires) &#123;            for (;;) &#123;                int available = getState();                int remaining = available - acquires;                if (remaining &lt; 0 ||                    compareAndSetState(available, remaining))                    return remaining;            &#125;        &#125;        protected final boolean tryReleaseShared(int releases) &#123;            for (;;) &#123;                int current = getState();                int next = current + releases;                if (next &lt; current) // overflow                    throw new Error(&quot;Maximum permit count exceeded&quot;);                if (compareAndSetState(current, next))                    return true;            &#125;        &#125;        final void reducePermits(int reductions) &#123;            for (;;) &#123;                int current = getState();                int next = current - reductions;                if (next &gt; current) // underflow                    throw new Error(&quot;Permit count underflow&quot;);                if (compareAndSetState(current, next))                    return;            &#125;        &#125;        final int drainPermits() &#123;            for (;;) &#123;                int current = getState();                if (current == 0 || compareAndSetState(current, 0))                    return current;            &#125;        &#125;    &#125;    /**     * NonFair version     */    static final class NonfairSync extends Sync &#123;        private static final long serialVersionUID = -2694183684443567898L;        NonfairSync(int permits) &#123;            super(permits);        &#125;        protected int tryAcquireShared(int acquires) &#123;            return nonfairTryAcquireShared(acquires);        &#125;    &#125;    /**     * Fair version     */    static final class FairSync extends Sync &#123;        private static final long serialVersionUID = 2014338818796000944L;        FairSync(int permits) &#123;            super(permits);        &#125;        protected int tryAcquireShared(int acquires) &#123;            for (;;) &#123;                if (hasQueuedPredecessors())                    return -1;                int available = getState();                int remaining = available - acquires;                if (remaining &lt; 0 ||                    compareAndSetState(available, remaining))                    return remaining;            &#125;        &#125;    &#125;    /**     * Creates a &#123;@code Semaphore&#125; with the given number of     * permits and nonfair fairness setting.     *     * @param permits the initial number of permits available.     *        This value may be negative, in which case releases     *        must occur before any acquires will be granted.     */    public Semaphore(int permits) &#123;        sync = new NonfairSync(permits);    &#125;    /**     * Creates a &#123;@code Semaphore&#125; with the given number of     * permits and the given fairness setting.     *     * @param permits the initial number of permits available.     *        This value may be negative, in which case releases     *        must occur before any acquires will be granted.     * @param fair &#123;@code true&#125; if this semaphore will guarantee     *        first-in first-out granting of permits under contention,     *        else &#123;@code false&#125;     */    public Semaphore(int permits, boolean fair) &#123;        sync = fair ? new FairSync(permits) : new NonfairSync(permits);    &#125;&#125;\n\n与synchronized的区别\nsynchronized是一个Java内置的关键字，AQS扩展的各种锁则是通过Java代码实现的\nsynchronzed锁是自动获取和释放的，而AQS的锁需要手动获取和释放\n像ReentrantLock还可以设置超时等特性，但synchronized不行\n\n","categories":["学知识"],"tags":["Java","Lock","AQS","锁"]},{"title":"volatile修饰符在双检锁单例模式中的作用","url":"/java-volatile-in-double-checked-singleton.html","content":"在实现一个双检锁单例的时候，IDEA提示我要给INSTANCE实例加上volatile修饰符。当时并不明白为啥，所以选择相信IDE。但是还是那句话，不能知其然不知其所以然啊，自己写的代码，不能自己心里没底不是。于是乎我一顿网上冲浪，终于整明白了为啥双检单例必须要用volatile修饰符。\n\n\n代码示例这个单例类没什么好说的，就是一个平平无奇的双检锁单例实现。\npublic class Singleton &#123;    private static Singleton INSTANCE;    public static Singleton getInstance() &#123;        if (INSTANCE == null) &#123;            synchronized (Singleton.class) &#123;                if (INSTANCE == null) &#123;                    INSTANCE = new Singleton();                &#125;            &#125;        &#125;        return INSTANCE;    &#125;    public void doSomething() &#123;        // Do something here    &#125;&#125;\n\n而IDEA在外层的if上标了一个警告，并且建议我给INSTANCE变量加上volatile修饰符。\n\n如果不加volatile会有什么问题上面的代码，乍一看非常严谨，在发现INSTANCE是null的时候，就对其加锁并再检查一次，还是null的话就为它创建一个新的实例，最后返回它。但是看了一些文章之后发现，在多线程场景下，有可能出现虽然成功获取到INSTANCE，但在调用其中的方法时仍然抛出空指针异常的诡异情况。\n比如有这样一个场景，Thread 1和Thread 2同时请求了Singleton#getInstance()方法，Thread 1执行到了第8行，开始实例化这个对象；而Thread 2执行到了第5行，开始检查INSTANCE是否为null。这个时候，有一定几率，虽然Thread 2检查到INSTANCE并不是null，但是调用Singleton#doSomething()方法的时候却会抛出空指针异常。\n\n造成这个问题的原因就是Java的指令重排。\n在搞清楚Thread 2看到INSTANCE虽然不是null，却在方法调用的时候会抛空指针异常的原因之前，先要搞清楚实例化对象的时候，JVM到底干了什么。\nJVM实例化一个对象的过程，大致可以分为这几步：\n\nJVM为这个对象分配一片内存\n在这片内存上初始化这个对象\n将这片内存的地址赋值给INSTANCE变量\n\n因为把内存地址赋值给INSTANCE是最后一步，所以Thread 1在这一步执行之前，Thread 2对INSTANCE == null的判断一定为true，进而因为拿不到Singleton类的锁而被阻塞，直到Thread 1完成对INSTANCE变量的实例化。\n但是，上面这三步它不是个原子操作，并且JVM可能会进行重排序，也就是说上面这三步可能被重排成\n\nJVM为这个对象分配一片内存\n将这片内存的地址赋值给INSTANCE变量\n在这片内存上初始化这个对象\n\n你看，这问题就来了，如果在Thread 1做完第二步但没做第三步的时候，Thread 2开始检查INSTANCE是不是null就会得到false，然后就走到return，得到一个不完整的INSTANCE对象。这时候，虽然INSTANCE不是null，但同时它也没有完成初始化，所以Thread 2在调用Singleton#doSomething()方法的时候，就会抛出空指针异常。\n这个问题的解决方案就是volatile修饰符，因为它可以禁止指令重排，所以在给INSTANCE加上volatile之后，JVM就会老老实实的先初始化好这个对象，再为INSTANCE赋值，这样多线程场景下每个线程得到的INSTANCE实例都会是一个初始化好了的Singleton对象。\n","categories":["学知识"],"tags":["Java","volatile","singleton","单例模式","双检锁"]},{"title":"Spring Boot 自动配置的原理","url":"/java-spring-autoconfiguration.html","content":"在使用Spring Boot时，最使我们收益的一个功能就是它的自动配置。但是，用了这么久的自动配置功能，有没有想过它是怎么实现的？本文将从源码入手，一步一步搞明白Spring Boot自动配置的原理。\n\n\n我这里就用一个简单的Eureka server的项目来举例。实际上只要是个Spring Boot项目就可以，我只是懒得再创建一个新的项目了。\n它的启动代码我们都很熟悉，是这样子的：\n@SpringBootApplication@EnableEurekaServerpublic class EurekaServer &#123;    public static void main(String[] args) &#123;        SpringApplication.run(EurekaServer.class, args);    &#125;&#125;\n\n东西很简单，一共就这么几行。我们看看@SpringBootApplication里面有什么？\n@SpringBootApplication注解@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),        @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;    // 因为里面的属性与本文关系不大，所以就略掉了&#125;\n\n这里我们可以看到一个@EnableAutoConfiguration注解，顾名思义，这个注解是用来开启自动配置的。我们继续深入进去看看。\n@EnableAutoConfiguration注解@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123;    String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;;    /**     * Exclude specific auto-configuration classes such that they will never be applied.     * @return the classes to exclude     */    Class&lt;?&gt;[] exclude() default &#123;&#125;;    /**     * Exclude specific auto-configuration class names such that they will never be     * applied.     * @return the class names to exclude     * @since 1.3.0     */    String[] excludeName() default &#123;&#125;;&#125;\n\n这个注解上面还有一大段的JavaDoc，因为篇幅太长，就不把全文放上来了。翻译过来的大意是这样的：\n\n这个注解可以启用Spring应用上下文的自动配置。开启这个配置后，Spring将会尝试猜测你需要的类，并进行配置。Spring通常会根据ClassPath和用户定义的bean来完成自动配置的操作。举例来说，如果在你的ClassPath中出现了tomcat-embedded.jar，那么你很可能会需要TomcatServletWebServerFactory这个类，除非你自己定义了ServletWebServerFactory。\n\n看完JavaDoc，我们大概知道了Spring是怎么判断哪些类需要自动配置。那么我们继续深入源码。\n这里有两个注解比较重要：@Import(AutoConfigurationImportSelector.class)和@AutoConfigurationPackage。\nAutoConfigurationImportSelector类Spring Boot应用启动过程中使用ConfigurationClassParser分析配置类时，如果发现注解中存在@Import(ImportSelector)的情况，就会创建一个相应的ImportSelector对象， 并调用其方法public String[] selectImports(AnnotationMetadata annotationMetadata)。所以我们就从selectImports方法开始看起。\n@Overridepublic String[] selectImports(AnnotationMetadata annotationMetadata) &#123;    if (!isEnabled(annotationMetadata)) &#123;        return NO_IMPORTS;    &#125;    // 从META-INF/spring-autoconfigure-metadata.properties中加载AutoConfigurationMetaData    AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader);    // 获取自动配置项    AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(autoConfigurationMetadata,annotationMetadata);    return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());&#125;\n\n上面方法通过getAutoConfigurationEntry获取到了需要自动配置的项，那么它是怎么知道哪些东西需要自动配置的呢？\n/** * Return the &#123;@link AutoConfigurationEntry&#125; based on the &#123;@link AnnotationMetadata&#125; * of the importing &#123;@link Configuration @Configuration&#125; class. * @param autoConfigurationMetadata the auto-configuration metadata * @param annotationMetadata the annotation metadata of the configuration class * @return the auto-configurations that should be imported */protected AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) &#123;    if (!isEnabled(annotationMetadata)) &#123;        return EMPTY_ENTRY;    &#125;    // 获取到注解里面的属性    AnnotationAttributes attributes = getAttributes(annotationMetadata);    // 获取到需要自动配置的类，并去重    List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes);    configurations = removeDuplicates(configurations);    // 取得要排除在自动配置之外的类的列表，并将其排除    Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes);    checkExcludedClasses(configurations, exclusions);    configurations.removeAll(exclusions);    // 应用过滤器AutoConfigurationImportFilter    configurations = filter(configurations, autoConfigurationMetadata);    // 广播自动配置事件    fireAutoConfigurationImportEvents(configurations, exclusions);    return new AutoConfigurationEntry(configurations, exclusions);&#125;\n\n上面我们看到，Spring Boot通过getCandidateConfigurations方法找到了需要自动配置的类，那么它又是怎么工作的呢？\n/** * Return the auto-configuration class names that should be considered. By default * this method will load candidates using &#123;@link SpringFactoriesLoader&#125; with * &#123;@link #getSpringFactoriesLoaderFactoryClass()&#125;. * @param metadata the source metadata * @param attributes the &#123;@link #getAttributes(AnnotationMetadata) annotation * attributes&#125; * @return a list of candidate configurations */protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123;    // 扫描ClassPath中所有的META-INF/spring.factories，并从中获取所有自动配置的类名    List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(),            getBeanClassLoader());    Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you &quot;            + &quot;are using a custom packaging, make sure that file is correct.&quot;);    return configurations;&#125;/** * Return the class used by &#123;@link SpringFactoriesLoader&#125; to load configuration * candidates. * @return the factory class */protected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() &#123;    return EnableAutoConfiguration.class;&#125;protected ClassLoader getBeanClassLoader() &#123;    return this.beanClassLoader;&#125;\n\n上面getCandidateConfigurations方法又调用了loadFactoryNames方法来获取EnableAutoConfiguration注解相关的工厂类。\n/** * The location to look for factories. * &lt;p&gt;Can be present in multiple JAR files. */public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;/** * Load the fully qualified class names of factory implementations of the * given type from &#123;@value #FACTORIES_RESOURCE_LOCATION&#125;, using the given * class loader. * @param factoryClass the interface or abstract class representing the factory * @param classLoader the ClassLoader to use for loading resources; can be * &#123;@code null&#125; to use the default * @throws IllegalArgumentException if an error occurs while loading factory names * @see #loadFactories */public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) &#123;    String factoryClassName = factoryClass.getName();    // 根据上文我们知道，factoryClass传入的是EnableAutoConfiguration.class    // 所以这里就是从spring.factories中寻找org.springframework.boot.autoconfigure.EnableAutoConfiguration对应的自动配置类    return loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());&#125;/** * 从ClassPath中寻找所有spring.factories文件，并将其包装成一个Properties对象 * 然后把Properties对象里面的各个条目包装到一个Map&lt;String, List&lt;String&gt;&gt;对象中 */private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123;    MultiValueMap&lt;String, String&gt; result = cache.get(classLoader);    if (result != null) &#123;        return result;    &#125;    try &#123;        Enumeration&lt;URL&gt; urls = (classLoader != null ?                classLoader.getResources(FACTORIES_RESOURCE_LOCATION) :                ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));        result = new LinkedMultiValueMap&lt;&gt;();        while (urls.hasMoreElements()) &#123;            URL url = urls.nextElement();            UrlResource resource = new UrlResource(url);            Properties properties = PropertiesLoaderUtils.loadProperties(resource);            for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123;                String factoryClassName = ((String) entry.getKey()).trim();                for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123;                    result.add(factoryClassName, factoryName.trim());                &#125;            &#125;        &#125;        cache.put(classLoader, result);        return result;    &#125;    catch (IOException ex) &#123;        throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; +                FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex);    &#125;&#125;\n\n举例说明这里我们用RedisAutoConfiguration类来说明一个具体的自动配置类是如何工作的。\n/** * &#123;@link EnableAutoConfiguration Auto-configuration&#125; for Spring Data&#x27;s Redis support. * * @author Dave Syer * @author Andy Wilkinson * @author Christian Dupuis * @author Christoph Strobl * @author Phillip Webb * @author Eddú Meléndez * @author Stephane Nicoll * @author Marco Aust * @author Mark Paluch * @since 1.0.0 */@Configuration // 说明这是一个配置类@ConditionalOnClass(RedisOperations.class) // 当RedisOperations类存在时这个配置类才会生效@EnableConfigurationProperties(RedisProperties.class) // 将配置文件的字段与RedisProperties类绑定@Import(&#123; LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class &#125;) // 导入两个Redis连接池的配置public class RedisAutoConfiguration &#123;    // 这里是初始化redisTemplate和stringRedisTemplate的代码，因为与本文无关，所以略掉了&#125;\n\n根据上文我们了解到的内容，我们可以推测出它是这样子被初始化的：\n\n首先Spring在spring.factories中，根据EnableAutoConfiguration发现了org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration\n然后Spring会检查RedisOperations类是否存在于ClassPath中\n如果存在，则会从配置文件的spring.redis字段中取值，并初始化RedisProperties\n然后根据LettuceConnectionConfiguration和JedisConnectionConfiguration中的条件，选择使用哪个连接池，并将其初始化\n\n总结通过上文的分析，我们知道了Spring在启动时，会加载一系列的配置类，并会根据配置类中指定的条件，来决定是否对其实施自动配置。\nxxAutoConfigurartion这样的类是自动配置类，用于向容器中添加组件。\nxxProperties这样的类是属性类，用于封装配置文件中的属性，并规定了Spring应该从配置文件的哪个字段取值用于初始化。\n","categories":["学知识"],"tags":["Java","Spring Boot"]},{"title":"Spring bean的作用域","url":"/java-spring-bean-scopes.html","content":"在Spring中，那些由IoC容器所管理的对象被称之为bean。而一个bean的定义，其实只是一个“蓝图”，指导着Spring如何去创建这样一个bean。而在这个蓝图中，有一个属性叫做“作用域”，它规定了这个bean的可见范围。这里我们看一下Spring的bean都有哪些作用域。\n\n\n支持的作用域我们先来看一下Spring支持哪些作用域。\n\n\n\n作用域\n说明\n\n\n\nsingleton\n在Spring容器中仅存在一个bean的实例，bean以单例形式存在。这是默认的作用域\n\n\nprototype\n每次从容器中获取bean时，都将生成一个新的实例，即相当于每次都执行new xxxBean()\n\n\nrequest\n在HTTP请求(request)的完整生命周期中，将创建并使用单个实例。该作用域仅适用于WebApplicatonContext环境\n\n\nsession\n在HTTP会话(session)的完整生命周期中，将创建并使用单个实例。该作用域仅适用于WebApplicationContext环境\n\n\nglobalSession\n在全局的HTTP会话(session)的完整生命周期中，将创建并使用单个实例。该作用域仅适用于WebApplicationContext环境，且通常只能用在Portlet环境中。\n\n\napplication\n在ServletContext的完整生命周期中，将创建并使用单个实例。该作用域仅适用于WebApplicationContext环境\n\n\nwebsocket\n在WebSocket的完整生命周期中，将创建并使用单个实例。该作用域仅适用于WebApplicationContext环境\n\n\n指定bean的作用域要指定一个bean的作用域，我们可以通过XML的方式或注解的方式来设定。\n使用XML指定配置bean时，可以通过scope属性来指定作用域：\n&lt;bean id=&quot;someBean&quot; class=&quot;com.demo.SomeClass&quot; scope=&quot;singleton&quot;/&gt;\n\n使用注解方式配置bean时，可以通过@Scope注解来指定作用域：\n@Component@Scope(ConfigurableBeanFactory.SCOPE_SINGLETON)public class SomeClass &#123;    // Class definitions goes here&#125;\n\n此外，如果使用注解方式配置作用域，Spring也提供了一系列常量值来方便我们配置：\n// 在ConfigurableBeanFactory类中String SCOPE_SINGLETON = &quot;singleton&quot;;String SCOPE_PROTOTYPE = &quot;prototype&quot;;// 在WebApplicationContext类中String SCOPE_REQUEST = &quot;request&quot;;String SCOPE_SESSION = &quot;session&quot;;String SCOPE_APPLICATION = &quot;application&quot;;\n\nsingleton作用域singleton是Spring容器中的默认作用域。这个作用域下，容器中只创建各管理一个bean实例，实例存在于缓存中，并在后续对该bean的请求中都返回这个实例。\nprototype作用域与singleton正相反，每次对prototype作用域的bean的请求，Spring都会生成一个新的实例，即类似我们手动使用new XxxBean()方式创建实例。\n需要注意的是，Spring不会完整的管理一个prototype的bean的生命周期。容器在初始化、配置，并将bean交由请求方(client)之后，就撒手不管了。也就是说，在销毁一个prototype的bean时，销毁bean的回调方法是不会被调用的，所以在销毁一个prototype的bean时，开发者必须手动释放它所使用的资源，或者可以尝试使用一个自定义的bean post-processor来让Spring做这些事。\n对于有状态的bean，应当使用prototype作用域；对于无状态的bean，则应当使用singleton作用域。\n向singleton bean注入prototype bean因为bean的依赖关系在实例化bean时才会被解析，所以通常来说，我们不可以将一个prototype bean注入到一个singleton bean中。\n如果我们向一个singleton bean中注入一个prototype bean，因为这个singleton bean只会被实例化一次，使得它的依赖也只会被注入一次，最终导致它依赖的那个singleton bean也只存在一个实例。\nrequest、session、global session、application和websocket作用域这几种作用域只能用在web-aware的Spring上下文中，比如XmlWebApplicationContext。如果用在一般的IoC容器中，比如ClassPathXmlApplicationContext中，那么容器会抛出一个IllegalStateException。\n要使用这几个作用域，你可能需要对你的应用进行一些配置。因为这些内容与本文无关，所以在这里就不详细说明了。感兴趣的话可以看Spring参考手册中的内容。\n注：web-aware这个词，我也不知道怎么翻译才合适。查阅了一些资料之后，感觉一个web-aware的Spring应用就是一个运行在web容器(比如Tomcat)中的应用，因为上面提到的这些作用域也是与web应用相关的。如果有好的理解，请一定在留言区写下来让在下知道。\nrequest作用域request作用域下的bean，在每次HTTP请求中，都会创建一个新的实例。当请求完成时，对应的bean就会被销毁。对一个实例的任何更改，对其他的所有实例来说都是不可见的。\nsession作用域session作用域下的bean，在每个活动的HTTP会话中，都有一个独自的实例，而当会话结束后，对应的bean就会被销毁。对一个实例的任何更改，对其他所有的实例来说都是不可见的。\nglobalSession作用域这个作用域只能用在portlet应用中。一个portlet站点中可能有多个portlet应用，而它们相关的session中都会共享同一个globalSession作用域的bean。\n注：其实我也不知道portlet到底是个啥，就算看过维基百科的Portlet条目也没看明白。\napplication作用域在整个应用范围内，容器为每个web应用程序运行时创建一个实例。这个作用域与singleton很类似，但是还是有两个不同点：\n\n在不同ServletContext中有不同的bean单例对象；singleton作用域的bean是每个ApplicationContext的单例对象。而一个应用可能有多个ApplicationContext\nbean作为ServletContext属性可见\n\n[^1]: Bean Scopes - The IoC container[^2]: Spring系列四：Bean Scopes作用域[^3]: Spring学习（十五）Spring Bean 的5种作用域介绍\n","categories":["学知识"],"tags":["Java","Spring Boot"]},{"title":"在Java中为什么不推荐在代码中直接写字符串值","url":"/java-why-prefer-string-constants-over-literals.html","content":"前两天看到个问题，说在Java中，为什么不推荐在代码中直接写字符串值？我一开始能想到的就是代码的可维护性和可读性，但是总觉得这个问题没这么简单，肯定还有JVM虚拟机里面的优化。那既然现在大语言模型这么发达，我觉得可以让它给我讲讲。\n\n\nChatGPT的回答\n在 Java 代码中直接写字符串值（即 硬编码字符串）通常是不推荐的，主要从以下两个方面考虑：\n\n代码可读性和可维护性\n\n\n降低可读性：直接在代码中写死字符串值，容易让代码的意图变得不清晰，阅读时难以理解这个字符串的作用。\n难以维护：如果字符串值在多个地方使用，一旦需要修改，就必须逐个查找替换，容易遗漏或出错。\n不利于国际化（i18n）：如果程序需要支持多种语言，硬编码字符串必须手动提取，而如果使用常量或资源文件，则可以更方便地进行国际化处理。\n\n\nJVM 内部优化\n\n\n字符串常量池优化：JVM 会对字符串字面量进行**字符串常量池（String Pool）**处理，避免重复创建相同的字符串实例。但如果字符串值是动态拼接或new关键字创建的，就可能无法享受这种优化。\n减少堆内存占用：使用String变量或final常量，JVM 可能会进行常量折叠，减少运行时重复创建的开销，提高效率。\n提高性能：直接写字符串字面量可能会在某些情况下导致 JVM 额外分配对象，而使用final常量或intern()方法可以优化内存占用。\n\n\n果然除了可读性考虑，JVM也会对字符串常量做一些优化。但，什么是常量折叠？\n常量折叠追问ChatGPT什么是常量折叠后，它是这么回到我的：\n\n常量折叠（Constant Folding） 是 JVM 编译优化（Just-In-Time Compilation, JIT） 或 Java 编译器（javac）在编译阶段执行的一种优化策略，它会在 编译期 计算出可以确定的常量表达式，并用计算结果替换原始表达式，从而减少运行时的计算开销。\n\n常量折叠的作用\n\n常量折叠的核心目标是提前计算出能确定的值，避免运行时重复计算，从而提高性能。例如，以下代码：\npublic class Test &#123;   public static void main(String[] args) &#123;       int a = 2 * 3;  // 计算常量表达式       System.out.println(a);   &#125;&#125;\n在编译阶段，JVM 会将2 * 3计算为6，然后直接将6存入字节码，而不是在运行时执行乘法计算。\n\n常量折叠的典型场景\n\n（前略）\n（2）字符串常量优化如果字符串是常量，编译器会在编译时进行字符串拼接优化，避免运行时的StringBuilder操作：\npublic class StringConstantFolding &#123;   public static void main(String[] args) &#123;       final String s1 = &quot;Hello, &quot;;       final String s2 = &quot;World!&quot;;       String s3 = s1 + s2;  // 直接折叠为 &quot;Hello, World!&quot;       System.out.println(s3);   &#125;&#125;\n优化后的字节码相当于：\nString s3 = &quot;Hello, World!&quot;;\n\n（后略）\n\n什么时候不会发生常量折叠？（1）变量不是final - 如果变量不是final，编译器无法确定其值，因此不会进行常量折叠（2）值在运行时才能确定 - 如果值依赖于外部输入、方法调用、对象实例等，JVM 不能在编译时优化\n\n\n也就是说，如果在代码里这样写：\npublic class Test &#123;    private static final String HELLO = &quot;hello, &quot;;    private static final String WORLD = &quot;world&quot;;    public static void main(String[] args) &#123;        System.out.println(HELLO + WORLD);    &#125;&#125;\n\n那么实际上JVM就会给你优化成：\npublic class Test&#123;    private static final String HELLO = &quot;hello, &quot;;    private static final String WORLD = &quot;world&quot;;    public static void main(String[] args) &#123;        System.out.println(&quot;hello, world&quot;);    &#125;&#125;\n\n将这段代码编译后再用javap反编译，可以看到这样的结果：\nClassfile /Users/boris1993/Developer/java/leetcode/src/Test.class  Last modified Mar 6, 2025; size 523 bytes  SHA-256 checksum 3443c8e0a13cd862c0195b772bb4290bde7e1d07b50818d531c2b5d66d665270  Compiled from &quot;Test.java&quot;public class Test  minor version: 0  major version: 65  flags: (0x0021) ACC_PUBLIC, ACC_SUPER  this_class: #13                         // Test  super_class: #2                         // java/lang/Object  interfaces: 0, fields: 2, methods: 2, attributes: 1Constant pool:   #1 = Methodref          #2.#3          // java/lang/Object.&quot;&lt;init&gt;&quot;:()V   #2 = Class              #4             // java/lang/Object   #3 = NameAndType        #5:#6          // &quot;&lt;init&gt;&quot;:()V   #4 = Utf8               java/lang/Object   #5 = Utf8               &lt;init&gt;   #6 = Utf8               ()V   #7 = Fieldref           #8.#9          // java/lang/System.out:Ljava/io/PrintStream;   #8 = Class              #10            // java/lang/System   #9 = NameAndType        #11:#12        // out:Ljava/io/PrintStream;  #10 = Utf8               java/lang/System  #11 = Utf8               out  #12 = Utf8               Ljava/io/PrintStream;  #13 = Class              #14            // Test  #14 = Utf8               Test  #15 = String             #16            // hello, world  #16 = Utf8               hello, world  #17 = Methodref          #18.#19        // java/io/PrintStream.println:(Ljava/lang/String;)V  #18 = Class              #20            // java/io/PrintStream  #19 = NameAndType        #21:#22        // println:(Ljava/lang/String;)V  #20 = Utf8               java/io/PrintStream  #21 = Utf8               println  #22 = Utf8               (Ljava/lang/String;)V  #23 = Utf8               HELLO  #24 = Utf8               Ljava/lang/String;  #25 = Utf8               ConstantValue  #26 = String             #27            // hello,  #27 = Utf8               hello,  #28 = Utf8               WORLD  #29 = String             #30            // world  #30 = Utf8               world  #31 = Utf8               Code  #32 = Utf8               LineNumberTable  #33 = Utf8               main  #34 = Utf8               ([Ljava/lang/String;)V  #35 = Utf8               SourceFile  #36 = Utf8               Test.java&#123;  public Test();    descriptor: ()V    flags: (0x0001) ACC_PUBLIC    Code:      stack=1, locals=1, args_size=1         0: aload_0         1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V         4: return      LineNumberTable:        line 1: 0  public static void main(java.lang.String[]);    descriptor: ([Ljava/lang/String;)V    flags: (0x0009) ACC_PUBLIC, ACC_STATIC    Code:      stack=2, locals=1, args_size=1         0: getstatic     #7                  // Field java/lang/System.out:Ljava/io/PrintStream;         3: ldc           #15                 // String hello, world         5: invokevirtual #17                 // Method java/io/PrintStream.println:(Ljava/lang/String;)V         8: return      LineNumberTable:        line 6: 0        line 7: 8&#125;SourceFile: &quot;Test.java&quot;\n\n首先可以看到，在常量池中有这么两行代码：\n#15 = String             #16            // hello, world#16 = Utf8               hello, world\n\n证明代码中HELLO + WORLD的值已经被编译器优化成了一个计算好的常量。在main方法的字节码中也可以看到\npublic static void main(java.lang.String[]);  descriptor: ([Ljava/lang/String;)V  flags: (0x0009) ACC_PUBLIC, ACC_STATIC  Code:    stack=2, locals=1, args_size=1       0: getstatic     #7                  // Field java/lang/System.out:Ljava/io/PrintStream;       3: ldc           #15                 // String hello, world       5: invokevirtual #17                 // Method java/io/PrintStream.println:(Ljava/lang/String;)V       8: return\n\nJVM直接加载了”hello, world”这个字符串，而不是调用了InvokeDynamic #0:makeConcatWithConstants:(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;在运行时进行字符串拼接。\n总结综上所述，使用字符串常量而不是直接写字符串值，除了让代码更可读更易于维护之外，我感觉最主要的优点就是针对字符串常量的拼接结果也会被放到常量池中，避免了在运行时反复创建新的字符串对象，造成性能浪费。\n","categories":["学知识"],"tags":["Java"]},{"title":"JDBC的事务和隔离级别","url":"/jdbc-transaction-and-isolation-level.html","content":"这里简单写一下我对JDBC的事务和隔离级别的理解。\n\n\n名词解释事务数据库事务是DBMS执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。一个事务一定是具有原子性(atomic)、一致性(consistency)、隔离性(isolation)、持久性(durability)，即ACID。一个事务会包含一个或多个数据操作语句(data-manipulation statements)和查询语句。通常来说，事务会依照如下的流程执行：\n\n开始一个事务\n\n执行一系列操作或查询语句\n\n如果没有发生错误，则提交这个事务，并将其结束\n\n如果发生了错误，则回滚这个事务，并将其结束。\n\n\nACID\nAtomic\n\n原子性，即一个事务内的所有操作都被作为一个整体看待，要么全部成功，要么全部失败\n\nConsistency\n\n一致性，即事务中有操作失败时，这个事务所更改的数据都必须回滚至操作前的状态。\n\nIsolation\n\n隔离性，即事务所查看到的数据，要么是一个事务提交前的状态，要么是一个事务提交后的状态，而不可能是事务在执行中的状态。\n\nDurability\n\n持久性，即事务对系统的影响是永久的。\n读现象\n脏读\n\n当一个事务允许读取另一个事务修改但尚未提交的数据时，就有可能发生脏读。\n\n不可重复读\n\n在一次事务中，对一行数据的两次读取获得了不同的结果。该现象发生于在执行SELECT时没有获得读锁，或者在读取完毕后立刻释放了读锁。\n\n幻读\n\n在事务执行过程中，两个完全一样的查询得到了不同的结果集，即是幻读。它是不可重复读的一个特殊场景。当事务1在执行两次SELECT ... WHERE操作中间，事务2在这个表中生成了一行新数据，而这条新数据正好满足事务1的WHERE条件，导致事务1的两次查询得到了不同的结果集。\n隔离级别NONENONE是一个特殊的级别，代表JDBC驱动不支持事务。\n未提交读(READ UNCOMMITED)这个是最低的隔离级别。这个隔离级别允许事务读取到其他事务尚未提交(commit)的数据，即允许脏读。\n提交读(READ COMMITED)这个隔离级别中，DBMS需要选定对象的写锁一直保持到事务结束，但是读锁会在SELECT操作完成后马上释放，所以有可能会发生“不可重复读”。\n可重复读(REPEATABLE READ)在这个隔离级别下，DBMS需要对选定对象的读锁和写锁一直保持到事务结束，但是不要求范围锁，所以有可能发生幻读。\n可串行化(SERIALIZABLE)这是最高的隔离级别。在这个隔离级别下，要求DBMS在选定对象上的读锁和写锁一直保持到事务结束，如果使用了WHERE来描述范围时，则应当获取一个范围锁。这个隔离级别可以防止幻读。\n隔离级别与读现象\n\n\n隔离级别\n脏读\n不可重复读\n幻读\n\n\n\n未提交读\n可能发生\n可能发生\n可能发生\n\n\n提交读\n不会发生\n可能发生\n可能发生\n\n\n可重复读\n不会发生\n不会发生\n可能发生\n\n\n可串行化\n不会发生\n不会发生\n不会发生\n\n\n隔离级别与锁持续时间\n\n\n隔离级别\n写操作\n读操作\n范围操作\n\n\n\n未提交读\n当前语句执行完毕\n当前语句执行完毕\n当前语句执行完毕\n\n\n提交读\n当前事务提交\n当前语句执行完毕\n当前语句执行完毕\n\n\n可重复读\n当前事务提交\n当前事务提交\n当前语句执行完毕\n\n\n可串行化\n当前事务提交\n当前事务提交\n当前事务提交\n\n\n","categories":["学知识"],"tags":["Java","JDBC","Transaction","Isolation level"]},{"title":"Java的线程安全，以及死锁","url":"/java-thread-security-deadlock.html","content":"刚才面试的时候被问到了关于线程安全和死锁的问题，有点露怯，故赶紧查漏补缺，记录于此。\n\n\n线程安全\n线程安全是程序设计中的术语，指某个函数、函数库在多线程环境中被调用时，能够正确地处理多个线程之间的公用变量，使程序功能正确完成。\n\n乐观锁与悲观锁\n乐观锁：认为在使用数据时，不会有别的线程修改数据，所以不会加锁，只在更新时判断之前有没有被别的线程更新了数据。比如在数据库中设置一个version字段，在更新前先查询该字段的值，然后在写入时比较数据库中的值是否与之前查询到的值相同。\n悲观锁：认为自己在使用数据的时候，一定有别的线程来修改数据，因此在获取数据的时候先加锁，确保数据不会被线程修改。\n\n如何保证线程安全\nsyncronized关键字，举例：ConcurrentHashMap。是悲观锁。\n锁升级机制：\n它是指在锁对象的对象头里面有一个threadid字段，在第一次访问的时候threadid为空，JVM让其持有偏向锁，并将threadid设置为其线程ID，再次进入的时候会先判断threadid是否与其线程ID一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了synchronized锁的升级。\n\n\n偏向锁（无锁）：大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得。偏向锁的目的是在某个线程获得锁之后（线程的 id 会记录在对象的Mark Word中），消除这个线程锁重入（CAS）的开销，看起来让这个线程得到了偏护。\n轻量级锁（CAS）：就是由偏向锁升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁；轻量级锁的意图是在没有多线程竞争的情况下，通过 CAS 操作尝试将Mark Word更新为指向LockRecord的指针，减少了使用重量级锁的系统互斥量产生的性能消耗。\n重量级锁：虚拟机使用CAS操作尝试将MarkWord更新为指向LockRecord的指针，如果更新成功表示线程就拥有该对象的锁；如果失败，会检查MarkWord是否指向当前线程的栈帧，如果是，表示当前线程已经拥有这个锁；如果不是，说明这个锁被其他线程抢占，此时膨胀为重量级锁。\n\n\n\n\nLock接口的实现类，常用ReentrantLock。是悲观锁。lock()加锁，unlock()解锁，不解锁会造成死锁。\n等待可中断：当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情，可中断特性对处理执行时间非常长的同步块很有帮助。\n公平锁：多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；而非公平锁则不保证这一点，在锁被释放时，任何一个等待锁的线程都有机会获得锁。synchronized中的锁是非公平的，ReentrantLock默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁。\n锁绑定多个条件：一个ReentrantLock对象可以同时绑定多个Condition对象，而在synchronized中，锁对象的wait()和notify()或 notifyAll()方法可以实现一个隐含的条件，如果要和多于一个的条件关联的时候，就不得不额外地添加一个锁，而ReentrantLock则无须这样做，只需要多次调用newCondition()方法即可。\n\n\nThreadLocal。当多个线程操作同一个变量且互不干扰的场景下，可以使用ThreadLocal来解决。它会在每个线程中对该变量创建一个副本，即每个线程内部都会有一个该变量，且在线程内部任何地方都可以使用，线程之间互不影响，这样一来就不存在线程安全问题，也不会严重影响程序执行性能。\nThreadLocal线程容器保存变量时，底层其实是通过ThreadLocalMap来实现的。它是以当前ThreadLocal变量为key，要存的变量为value。获取的时候就是以当前ThreadLocal变量去找到对应的key，然后获取到对应的值。\n\n\n\n死锁\n两个或两个以上的线程持有不同系统资源的锁，线程彼此都等待获取对方的锁来完成自己的任务，但是没有让出自己持有的锁，线程就会无休止等待下去。线程竞争的资源可以是：锁、网络连接、通知事件，磁盘、带宽，以及一切可以被称作 “资源” 的东西。\n\n检测死锁可以使用jstack检查死锁。\n命令：jstack $(jps -l | grep &#39;DeadLockExample&#39; | cut -f1 -d &#39; &#39;)。\n示例输出：\nJava stack information for the threads listed above:===================================================&quot;Thread-1&quot;:    at DeadLockExample$2.run(DeadLockExample.java:58)    - waiting to lock &lt;0x000000076ab660a0&gt; (a java.lang.Object)    - locked &lt;0x000000076ab660b0&gt; (a java.lang.Object)    at java.lang.Thread.run(Thread.java:748)&quot;Thread-0&quot;:    at DeadLockExample$1.run(DeadLockExample.java:28)    - waiting to lock &lt;0x000000076ab660b0&gt; (a java.lang.Object)    - locked &lt;0x000000076ab660a0&gt; (a java.lang.Object)    at java.lang.Thread.run(Thread.java:748)Found 1 deadlock.\n\n避免死锁\n以确定的顺序获锁\n超时放弃\n死锁检测\n尽量降低锁的使用粒度\n尽量使用同步代码块，而不是同步方法\n避免嵌套锁\n专锁专用\n\n参考文章\n4种解决线程安全问题的方式\nJava高级教程系列 - 死锁示例及解决\nJava多线程开发中避免死锁的八种方法\n\n","categories":["学知识"],"tags":["Java","多线程","线程安全","死锁"]},{"title":"看懂ParallelGC的日志","url":"/java-understand-parallel-gc-logs.html","content":"在排查垃圾回收相关的问题时，一个必不可少的技能就是要能看懂Java的垃圾回收日志。本文将介绍打印GC日志相关的JVM参数，以及使用不同参数时JVM将会打印出的日志内容。\n\n\nJDK版本openjdk version &quot;1.8.0_232&quot;OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_232-b09)OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.232-b09, mixed mode)\n\n示例代码在演示过程中，我们将通过在死循环中构造大对象的方式来触发垃圾回收\npublic class GCExample &#123;    public static void main(String[] args) &#123;        String[] strings;        while (true) &#123;            strings = new String[1024];        &#125;    &#125;&#125;\n\n同时，为了让垃圾回收更早的开始，我们还需要指定一个较小的堆内存，比如我选择只分配10MB的空间。\n-Xmx10m-Xms10m\n\nGC日志相关的JVM参数与GC日志相关的JVM参数有如下8个：\n\n\n\n参数名\n参数含义\n\n\n\n-XX:+PrintGC\n输出简要GC日志\n\n\n-verbose:gc\n等同于 -XX:+PrintGC\n\n\n-XX:+PrintGCDetails\n输出详细GC日志\n\n\n-Xloggc:gc.log\n输出GC日志到文件gc.log\n\n\n-XX:+PrintGCTimeStamps\n输出GC的时间戳（以JVM启动到当期的总时长的时间戳形式）\n\n\n-XX:+PrintGCDateStamps\n输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800）\n\n\n-XX:+PrintHeapAtGC\n在进行GC的前后打印出堆的信息\n\n\n-XX:+PrintReferenceGC\n打印年轻代各个引用的数量以及时长\n\n\n-XX:+PrintGC与-verbose:gc虽然上面说这两个参数是等价的，但是，-verbose:gc是一个标准的参数，而-XX:+PrintGC则在JDK 9之后被标为废弃的(deprecated)，所以建议使用-verbose:gc代替-XX:+PrintGC。\n打开-verbose:gc使用这个参数将会打开简略的GC日志。打开这个参数并运行程序，很快就可以看到这样的GC日志：\n[GC (Allocation Failure)  2520K-&gt;472K(9728K), 0.0005722 secs]\n\n我们将上面的日志拆分开来：\n\nGC代表这次垃圾回收的类型。GC代表发生了Minor GC，Full GC代表发生了Full GC\nAllocation Failure简述了发生GC的原因。在这个例子中，是因为在年轻代中没有合适的空间导致新的对象分配失败，从而发生GC\n2520K-&gt;472K说明了年轻代的占用量变化。在这个例子中，年轻代的使用量从2520KB减少到了472KB\n9728K表示整个堆的大小\n0.0005722 secs表示了本次GC消耗的时间\n\n打开-XX:+PrintGCDetails使用这个参数后，将会打印详细的GC日志，它打印的内容包含了-verbose:gc参数打印的内容。\n[GC (Allocation Failure) [PSYoungGen: 2048K-&gt;0K(2560K)] 2500K-&gt;452K(9728K), 0.0003785 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]\n\n这个参数打印的日志与上一个参数打印的日志有一部分重合，这里我们关注新增的内容：\n\n[PSYoungGen: 2048K-&gt;0K(2560K)]表明了使用的垃圾回收器、年轻代的占用量变化，和年轻代的大小\n2500K-&gt;452K(9728K)表明了整个堆的占用量变化，和整个堆的大小\n[Times: user=0.00 sys=0.00, real=0.00 secs]表示本次GC消耗的时间\nuser代表垃圾回收器消耗的CPU时间\nsys代表系统调用或等待系统事件消耗的时间\nreal代表应用实际停止的时间，近似于(user + sys) / 垃圾回收器使用的线程数\n\n\n\n打开-XX:+PrintGCTimeStamps这个参数不影响日志的详细程度，在打开之后，日志中会出现从JVM启动到出现日志时的毫秒数，比如下面例子的1.523。\n1.523: [GC (Allocation Failure) [PSYoungGen: 2141K-&gt;96K(2560K)] 4805K-&gt;2776K(9728K), 0.0004359 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]\n\n打开-XX:+PrintGCDateStamps这个参数也不影响日志的详细程度，在打开之后，日志头部会加上打印日志的时间和时区，比如下面例子的2020-02-06T10:13:08.634-0800。\n2020-02-06T10:13:08.634-0800: 1.122: [GC (Allocation Failure) [PSYoungGen: 2048K-&gt;0K(2560K)] 3328K-&gt;1280K(9728K), 0.0004243 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]\n\n打开-XX:+PrintHeapAtGC使用这个参数后，在打印GC日志时，会同时打印出堆在GC前后的信息，即根据不同的内存区域，分别显示在垃圾回收前后的空间使用情况。同时，日志中还以[bottom,top,end)的格式展现出了各个内存区域的地址，其中bottom为这个区域的起始位置，top为这个区域当前的顶部位置，end为这个区域可使用的地址上限[^1]。\n此外，日志中关于Metaspace相关的信息可以参考注脚4[^4]指向的那篇回答。\n&#123;Heap before GC invocations=1 (full 0): PSYoungGen      total 2560K, used 2047K [0x00000007bfd00000, 0x00000007c0000000, 0x00000007c0000000)  eden space 2048K, 99% used [0x00000007bfd00000,0x00000007bfeffe78,0x00000007bff00000)  from space 512K, 0% used [0x00000007bff80000,0x00000007bff80000,0x00000007c0000000)  to   space 512K, 0% used [0x00000007bff00000,0x00000007bff00000,0x00000007bff80000) ParOldGen       total 7168K, used 0K [0x00000007bf600000, 0x00000007bfd00000, 0x00000007bfd00000)  object space 7168K, 0% used [0x00000007bf600000,0x00000007bf600000,0x00000007bfd00000) Metaspace       used 3045K, capacity 4496K, committed 4864K, reserved 1056768K  class space    used 320K, capacity 388K, committed 512K, reserved 1048576K2020-02-06T10:21:05.317-0800: 0.219: [GC (Allocation Failure) [PSYoungGen: 2047K-&gt;496K(2560K)] 2047K-&gt;512K(9728K), 0.0019061 secs] [Times: user=0.01 sys=0.00, real=0.00 secs]Heap after GC invocations=1 (full 0): PSYoungGen      total 2560K, used 496K [0x00000007bfd00000, 0x00000007c0000000, 0x00000007c0000000)  eden space 2048K, 0% used [0x00000007bfd00000,0x00000007bfd00000,0x00000007bff00000)  from space 512K, 96% used [0x00000007bff00000,0x00000007bff7c010,0x00000007bff80000)  to   space 512K, 0% used [0x00000007bff80000,0x00000007bff80000,0x00000007c0000000) ParOldGen       total 7168K, used 16K [0x00000007bf600000, 0x00000007bfd00000, 0x00000007bfd00000)  object space 7168K, 0% used [0x00000007bf600000,0x00000007bf604000,0x00000007bfd00000) Metaspace       used 3045K, capacity 4496K, committed 4864K, reserved 1056768K  class space    used 320K, capacity 388K, committed 512K, reserved 1048576K&#125;\n\n打开-XX:+PrintReferenceGC打开这个参数后，就可以看到各种引用的数量，以及对其垃圾回收所消耗的时长。\n[GC (Allocation Failure) [SoftReference, 0 refs, 0.0000125 secs][WeakReference, 0 refs, 0.0000054 secs][FinalReference, 0 refs, 0.0000045 secs][PhantomReference, 0 refs, 0 refs, 0.0000222 secs][JNI Weak Reference, 0.0000037 secs][PSYoungGen: 2112K-&gt;64K(2560K)] 3488K-&gt;1456K(9728K), 0.0005389 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]\n\n[^1]: Meaning of heap memory addresses in GC logs using -XX:+PrintHeapAtGC?[^2]: Parallel GC - Plumber[^3]: Parallel Full GC - Plumber[^4]: Understanding metaspace line in JVM heap printout\n","categories":["学知识"],"tags":["Java","垃圾回收","GC","JVM"]},{"title":"LeetCode刷题笔记","url":"/leetcode-notes.html","content":"迫于找工作，不得不一边鄙视自己的智商，一边硬着头皮刷LeetCode。既然开始刷题，那顺便做个笔记，以便日后温习。\n因为顺序是乱的，所以请善用CTRL-F。\n\n\n217. Contains Duplicate\nExample:\nInput: nums = [1,2,3,1]Output: trueInput: nums = [1,2,3,4]Output: false\nclass Solution &#123;    public boolean containsDuplicate(int[] nums) &#123;        if (nums == null || nums.length == 0) &#123;            return false;        &#125;        HashSet&lt;Integer&gt; hashSet = new HashSet&lt;&gt;();        for (int i : nums) &#123;            // 看见一个数字就去hashSet里面查，            // 查到就说明重复过了            if (hashSet.contains(i)) &#123;                return true;            &#125;            hashSet.add(i);        &#125;        return false;    &#125;&#125;\n\n242. Valid Anagram\nExample:\nInput: s = &quot;anagram&quot;, t = &quot;nagaram&quot;Output: trueInput: s = &quot;rat&quot;, t = &quot;car&quot;Output: false\nclass Solution &#123;    public boolean isAnagram(String s, String t) &#123;        if (s.length() != t.length()) &#123;            return false;        &#125;        // 字符串s中各个字符出现的次数        Map&lt;Character, Integer&gt; countS = new HashMap&lt;&gt;();        // 字符串t中各个字符出现的次数        Map&lt;Character, Integer&gt; countT = new HashMap&lt;&gt;();        for (int i = 0; i &lt; s.length(); i++) &#123;            countS.put(                s.charAt(i),                 countS.getOrDefault(s.charAt(i), 0) + 1);        &#125;        for (int i = 0; i &lt; t.length(); i++) &#123;            countT.put(                t.charAt(i),                 countT.getOrDefault(t.charAt(i), 0) + 1);        &#125;        // Map#equals比较的是两个map中的键值对        // 键值对相同，说明两串字符都用了相同的字母，每个字母出现的次数也一样        // 即同字母异序词        return countS.equals(countT);    &#125;&#125;\n\n1299. Replace Elements with Greatest Element on Right Side\nExample:\nInput: arr = [17,18,5,4,6,1]Output: [18,6,6,6,1,-1]Explanation: - index 0 --&gt; the greatest element to the right of index 0 is index 1 (18).- index 1 --&gt; the greatest element to the right of index 1 is index 4 (6).- index 2 --&gt; the greatest element to the right of index 2 is index 4 (6).- index 3 --&gt; the greatest element to the right of index 3 is index 4 (6).- index 4 --&gt; the greatest element to the right of index 4 is index 5 (1).- index 5 --&gt; there are no elements to the right of index 5, so we put -1.\nclass Solution &#123;    public int[] replaceElements(int[] arr) &#123;        int max = -1;        for (int i = arr.length - 1; i &gt;= 0; i--) &#123;            int a = arr[i];            arr[i] = max;            max = Math.max(max, a);        &#125;        return arr;    &#125;&#125;\n\n解题思路就是把explanation反过来看，既然要找元素右边最大的数，那么就从最右开始，这样找到的最大的数必定适用于arr[i] ~ arr[length - 1]这个范围。\n392. Is Subsequence\nExample:\nInput: s = &quot;abc&quot;, t = &quot;ahbgdc&quot;Output: trueInput: s = &quot;axc&quot;, t = &quot;ahbgdc&quot;Output: falseInput: s = &quot;acb&quot;, t = &quot;ahbgdc&quot;Output: falseInput: s = &quot;aaaaaa&quot;, t = &quot;bbaaaa&quot;Output: false\nclass Solution &#123;    public boolean isSubsequence(String s, String t) &#123;        if (s.equals(t) || s.length() == 0) &#123;            return true;        &#125;        if (t.length() == 0) &#123;            return false;        &#125;        int sIndex = 0;        for (int i = 0; i &lt; t.length(); i++) &#123;            if (t.charAt(i) == s.charAt(sIndex)) &#123;                sIndex++;                if (sIndex &gt;= s.length()) &#123;                    return true;                &#125;            &#125;        &#125;        return false;    &#125;&#125;\n\n解题思路就是整两个指针，sIndex指向s的各个字符，在循环里面逐个取t的字符跟s[sIndex]对比，匹配到的话sIndex就往下走一步，如果sIndex能走到头，就说明s是t的子序列。\n58. Length of Last Word\nExample:\nInput: s = &quot;Hello World&quot;Output: 5Explanation: The last word is &quot;World&quot; with length 5.Input: s = &quot;   fly me   to   the moon  &quot;Output: 4Explanation: The last word is &quot;moon&quot; with length 4.\nclass Solution &#123;    public int lengthOfLastWord(String s) &#123;        String[] strs = s.split(&quot; &quot;);        return strs[strs.length - 1].length();    &#125;&#125;\n\n我的评价是，这道题不应该出现在LeetCode，应该出现在大学Java课程的作业里。\n14. Longest Common Prefix\nExample:\nInput: strs = [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]Output: &quot;fl&quot;\nclass Solution &#123;    public String longestCommonPrefix(String[] strs) &#123;        if (strs.length == 0) &#123;            return &quot;&quot;;        &#125;        // 拿第一个字符串当模版，匹配剩下几个字符串        String commonPrefix = strs[0];        for (int i = 1; i &lt; strs.length; i++) &#123;            // 如果commonPrefix在strs[i]的下标不是0            // 那么就去掉commonPrefix的最后一个字母，直到下标为0            // 如果没有common prefix，那么commonPrefix会被砍成空字符串            while (strs[i].indexOf(commonPrefix) != 0) &#123;                commonPrefix = commonPrefix.substring(0, commonPrefix.length() - 1);            &#125;        &#125;        return commonPrefix;    &#125;&#125;\n\n计算过程：（语言描述太费劲，直接拿Replit放示意图算了）\n\n49. Group Anagrams\nExample:\nInput: strs = [&quot;eat&quot;,&quot;tea&quot;,&quot;tan&quot;,&quot;ate&quot;,&quot;nat&quot;,&quot;bat&quot;]Output: [[&quot;bat&quot;],[&quot;nat&quot;,&quot;tan&quot;],[&quot;ate&quot;,&quot;eat&quot;,&quot;tea&quot;]]\nclass Solution &#123;    public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) &#123;        Map&lt;String, List&lt;String&gt;&gt; map = new LinkedHashMap&lt;&gt;();        for (String str: strs) &#123;            // 取一个字符串，打散成字符数组，把字符数组排序后得到一个新字符串            // 比如eat-&gt; aet，tea-&gt;aet            // 这个作为map的key            char[] charArray = str.toCharArray();            Arrays.sort(charArray);            String newStr = new String(charArray);            if (!map.containsKey(newStr)) &#123;                map.put(newStr, new ArrayList&lt;&gt;());            &#125;            // 把重排序后结果相同的字符串放在同一个key下面的List里面            map.get(newStr).add(str);        &#125;        List&lt;List&lt;String&gt;&gt; result = new ArrayList(map.values());        return result;    &#125;&#125;\n\n计算过程语言不好描述……但是代码挺易懂的吧，实在看不明白的话自己debug一下就清楚了。\n118. Pascal’s Triangle\nExample: \nInput: numRows = 5Output: [[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1]]Input: numRows = 1Output: [[1]]\n\n\nclass Solution &#123;    public List&lt;List&lt;Integer&gt;&gt; generate(int numRows) &#123;        // 需要单独处理只有一行的情况        // 否则会因为下面预设前两行数据的代码而返回错误的数据        if (numRows == 1) &#123;            return List.of(List.of(1));        &#125;        List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(numRows);                // 前两行的内容是固定的        result.add(List.of(1));        result.add(List.of(1,1));        for (int row = 2; row &lt; numRows; row++) &#123;            List&lt;Integer&gt; list = new ArrayList&lt;&gt;(row + 1);            // 这一行最左边肯定是1            list.add(1);            // 两个两个取上一行的各个数，两两相加，得到当前格子的数字            // 因为要取上一行的第i和第i+1个元素，所以循环结束条件得是上一行的个数减一            // 否则就下标越界了            int previousRow = row - 1;            for (int i = 0; i &lt; result.get(previousRow).size() - 1; i++) &#123;                int sum = result.get(previousRow).get(i) + result.get(previousRow).get(i + 1);                list.add(sum);            &#125;                        // 最右边也肯定是1            list.add(1);            result.add(list);        &#125;        return result;    &#125;&#125;\n\n27. Remove Element\nExample:\nInput: nums = [3,2,2,3], val = 3Output: 2, nums = [2,2,_,_]Explanation: Your function should return k = 2, with the first two elements of nums being 2.It does not matter what you leave beyond the returned k (hence they are underscores).\nclass Solution &#123;    public int removeElement(int[] nums, int val) &#123;        // 不等于val的数字的个数，同时当作nums的一个指针        int count = 0;        for (int i = 0; i &lt; nums.length; i++) &#123;            if (nums[i] != val) &#123;                nums[count] = nums[i];                count++;            &#125;        &#125;        return count;    &#125;&#125;\n\n929. Unique Email Addresses\nExample:\nInput: emails = [&quot;test.email+alex@leetcode.com&quot;,&quot;test.e.mail+bob.cathy@leetcode.com&quot;,&quot;testemail+david@lee.tcode.com&quot;]Output: 2Explanation: &quot;testemail@leetcode.com&quot; and &quot;testemail@lee.tcode.com&quot; actually receive mails.\nclass Solution &#123;    public int numUniqueEmails(String[] emails) &#123;        HashSet&lt;String&gt; uniqueEmails = new HashSet&lt;&gt;();        for (String email : emails) &#123;            String sanitizedEmail = &quot;&quot;;            int indexOfPlusSign = email.indexOf(&quot;+&quot;);            int indexOfAtSign = email.indexOf(&quot;@&quot;);            if (indexOfPlusSign &gt; 0) &#123;                // 按题目要求，第一个加号及其以后的用户名部分都会被忽略                sanitizedEmail += email.substring(0, indexOfPlusSign);            &#125; else &#123;                sanitizedEmail += email.substring(0, indexOfAtSign);            &#125;            // 按题目要求，用户名部分的点都会被忽略            sanitizedEmail = sanitizedEmail.replace(&quot;.&quot;, &quot;&quot;);            // 处理完了用户名部分，把域名部分拼上去            sanitizedEmail += email.substring(indexOfAtSign);            // 全处理完之后，扔进HashSet里面，顺便去重            uniqueEmails.add(sanitizedEmail);        &#125;        return uniqueEmails.size();    &#125;&#125;\n\n205. Isomorphic Strings\nExample: \nInput: s = &quot;paper&quot;, t = &quot;title&quot;Output: trueInput: s = &quot;foo&quot;, t = &quot;bar&quot;Output: false\nclass Solution &#123;    public boolean isIsomorphic(String s, String t) &#123;        // 俩字符串长度都不一样，那必然不同形        if (s.length() != t.length()) &#123;            return false;        &#125;        // 建立s中每个字符与t中同位置字符的映射关系        // 如 egg -&gt; add        // e -&gt; a, g -&gt; d        HashMap&lt;Character, Character&gt; map = new HashMap(s.length());        for (int i = 0; i &lt; t.length(); i++) &#123;            if (map.containsValue(t.charAt(i))) &#123;                continue;            &#125;            map.put(s.charAt(i), t.charAt(i));        &#125;        // 然后从map里面，按照s的每个字母，取出映射过的字符        // 拼在StringBuilder里面        StringBuilder sb = new StringBuilder();        for (int i = 0; i &lt; s.length(); i++) &#123;            Character ch = map.get(s.charAt(i));            sb.append(ch);        &#125;        // 检查用映射拼出来的字符串与t是否相同        return sb.toString().equals(t);    &#125;&#125;\n\n347. Top K Frequent Elements\nExample:\nInput: nums = [1,1,1,2,2,3], k = 2Output: [1,2]\nclass Solution &#123;    public int[] topKFrequent(int[] nums, int k) &#123;        if (nums.length == k) &#123;            return nums;        &#125;        return Arrays            .stream(nums)            // int -&gt; Integer            .boxed()            // 收集到一个map中，key是数字，value是出现的次数            .collect(Collectors.groupingBy(num -&gt; num, Collectors.summingInt(num -&gt; 1)))            .entrySet()            .stream()            // 把entry set按照value降序排列            .sorted(Map.Entry.comparingByValue((a, b) -&gt; Integer.compare(b, a)))            // 取前k个            .limit(k)            // 把key取出来，unbox成int            .mapToInt(e -&gt; e.getKey())            // 最后造个数组出来返回掉            .toArray();    &#125;&#125;\n\n还写(chao)了一个不用stream，纯手工拿entry set做比较的解法，因过于丑陋，就不贴在这了，submission在这里。\n128. Longest Consecutive Sequence\nExample:\nInput: nums = [100,4,200,1,3,2]Output: 4Explanation: The longest consecutive elements sequence is [1, 2, 3, 4]. Therefore its length is 4.Input: nums = [0,3,7,2,5,8,4,6,0,1]Output: 9Explanation: The longest consecutive elements sequence is [0, 1, 2, 3, 4, 5, 6, 7, 8]. Therefore its length is 9.\nclass Solution &#123;    public int longestConsecutive(int[] nums) &#123;        if (nums.length == 0) &#123;            return 0;        &#125;        Arrays.sort(nums);        int longestStreak = 1;        int currentStreak = 1;        for (int i = 1; i &lt; nums.length; i++) &#123;            // 因为当前数字已经记入了一个“连击”            // 所以当这个数字跟上一个数字重复的时候，直接跳到下一个数字            if (nums[i] != nums[i - 1]) &#123;                // 既然连续，那么当前数字跟上一个数字肯定差1                if (nums[i] == nums[i - 1] + 1) &#123;                    currentStreak++;                &#125; else &#123;                    // 如果不连续了，记下来最大的连击数，当前连击数重置                    longestStreak = Math.max(longestStreak, currentStreak);                    currentStreak = 1;                &#125;            &#125;        &#125;        // 最后取最大的连击数，这个不用多说吧        return Math.max(longestStreak, currentStreak);    &#125;&#125;\n\n125. Valid Palindrome\nExample:\nInput: s = &quot;A man, a plan, a canal: Panama&quot;Output: trueExplanation: &quot;amanaplanacanalpanama&quot; is a palindrome.Input: s = &quot; &quot;Output: trueExplanation: s is an empty string &quot;&quot; after removing non-alphanumeric characters.Since an empty string reads the same forward and backward, it is a palindrome.\nclass Solution &#123;    public boolean isPalindrome(String s) &#123;        // 把不是字母和数字的字符剔出去，然后转小写，方便比较        String str = s.replaceAll(&quot;[\\\\W]|_&quot;, &quot;&quot;);        str = str.toLowerCase();        String reversed = new StringBuilder(str).reverse().toString();        return str.equals(reversed);    &#125;&#125;\n\n但是很显然，这个偷鸡解法并不是two pointers这个分类想要的，所以另一个解法：\nclass Solution &#123;    public boolean isPalindrome(String s) &#123;        // 把不是字母和数字的字符剔出去，然后转小写，方便比较        String str = s.replaceAll(&quot;[\\\\W]|_&quot;, &quot;&quot;);        str = str.toLowerCase();        char[] chars = str.toCharArray();        int middle = chars.length / 2;        int head = 0;        int tail = chars.length - 1;        // head和tail两个指针逐步向middle逼近        // 一边走，一边比较两个指针指向的字母是不是一样        while (head &lt; middle) &#123;            if (chars[head] != chars[tail]) &#123;                // 不一样的话，那自然就不是回文字符串了                return false;            &#125;            head++;            tail--;        &#125;        return true;    &#125;&#125;\n\n1. Two Sum\nExample:\nInput: nums = [2,7,11,15], target = 9Output: [0,1]Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\nclass Solution &#123;    public int[] twoSum(int[] nums, int target) &#123;        // key是数字本身，value是数字所在下标        Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;();                for (int i = 0; i &lt; nums.length; i++) &#123;            // 算一下target与当前下标的差            int complement = target - nums[i];            if (map.containsKey(complement)) &#123;                return new int[] &#123; map.get(complement), i &#125;;            &#125;            map.put(nums[i], i);        &#125;                throw new IllegalArgumentException(&quot;No solution found&quot;);    &#125;&#125;\n\n计算过程：\n\ni = 0，nums[i] = 2，complement = 9 - 2 = 7，map的key里面找不到7，所以map.put(2, 0)\ni = 1，nums[i] = 7，complement = 9 - 7 = 2，map的key里面有2，即nums[i] + nums[map.get(2)] = 9，返回[map.get(2), nums[i]]即[0, 1]\n\n167. Two Sum II - Input Array Is Sorted\nExample:\nInput: numbers = [2,7,11,15], target = 9Output: [1,2]Explanation: The sum of 2 and 7 is 9. Therefore, index1 = 1, index2 = 2. We return [1, 2].\nclass Solution &#123;    public int[] twoSum(int[] numbers, int target) &#123;        // 两个指针        // 一个从头往尾走，一个从尾往头走        int low = 0;        int high = numbers.length - 1;        while (low &lt; high) &#123;            // 要求是计算两个数字之和是否等于target            // 同时因为数组已经按升序排列            // 那么如果加的结果大于target，就让尾指针往前，取更小的数            // 如果结果小于target，就让头指针往后，取更大的数            // 结果要么找到合适的两个数，要么两个指针相遇，即没有合适的数字            int sum = numbers[low] + numbers[high];            if (sum == target) &#123;                return new int[] &#123;low + 1, high + 1&#125;;            &#125; else if (sum &lt; target) &#123;                ++low;            &#125; else &#123;                --high;            &#125;        &#125;        return new int[] &#123;-1, -1&#125;;    &#125;&#125;\n\n15. 3Sum\nExample: \nInput: nums = [-1,0,1,2,-1,-4]Output: [[-1,-1,2],[-1,0,1]]Explanation: nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0.nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0.nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0.The distinct triplets are [-1,0,1] and [-1,-1,2].Notice that the order of the output and the order of the triplets does not matter.\n一种解法是，类似Two Sum II，取一个基准数，然后用Two Sum II的方法找基准数右边符合要求的两个数。\nclass Solution &#123;    public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123;        // 先把nums排序，让这个数组符合Two Sum II的要求        Arrays.sort(nums);        List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;();                // 从头到尾遍历nums，取一个数字作为基准        for (int i = 0; i &lt; nums.length &amp;&amp; nums[i] &lt;= 0; ++i) &#123;            // 这里的条件是为了不重复计算相同的数字，            // 即只在当前数字不跟上一个数字重复的时候才计算            if (i == 0 || nums[i - 1] != nums[i]) &#123;                twoSumII(nums, i, result);            &#125;        &#125;        return result;    &#125;    private void twoSumII(int[] nums, int i, List&lt;List&lt;Integer&gt;&gt; result) &#123;        // 取nums[i]右边的数组作为子数组        // 左指针指向子数组第一个元素        int low = i + 1;        // 右指针指向最后一个元素        int high = nums.length - 1;        while (low &lt; high) &#123;            // 基准数 + 左指针 + 右指针            int sum = nums[i] + nums[low] + nums[high];            if (sum &lt; 0) &#123;                // 结果小于零，那么左指针往右走，取更大的一个数                ++low;            &#125; else if (sum &gt; 0) &#123;                // 结果大于零，那么右指针往左走，取更小的一个数                --high;            &#125; else &#123;                // 等于零，那么这三个数就是我们想要的                result.add(Arrays.asList(nums[i], nums[low], nums[high]));                                // 继续缩小范围                low++;                high--;                // 并且当low指向相同数字时，继续向右走                // 然后重新在上一层循环里面继续找符合要求的数字                // 毕竟区间内可能有多组符合条件的数字                while (low &lt; high &amp;&amp; nums[low] == nums[low - 1]) &#123;                    ++low;                &#125;            &#125;        &#125;    &#125;&#125;\n\n但是，如果题目不允许改变nums数组呢？那么可以这样解，\nclass Solution &#123;    public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123;        Set&lt;List&lt;Integer&gt;&gt; result = new HashSet&lt;&gt;();        Set&lt;Integer&gt; duplicates = new HashSet&lt;&gt;();        Map&lt;Integer, Integer&gt; seen = new HashMap&lt;&gt;();        for (int i = 0; i &lt; nums.length; i++) &#123;            if (!duplicates.add(nums[i])) &#123;                continue;            &#125;            for (int j = i + 1; j &lt; nums.length; j++) &#123;                int complement = -(nums[i] + nums[j]);                // 如果在seen这个map里面找到了曾经计算过的complement                // 那么说明complement存在于nums数组中                // 即找到了三个和为零的数字，nums[i]，nums[j]，complement                if (seen.containsKey(complement) &amp;&amp; seen.get(complement) == i) &#123;                    List&lt;Integer&gt; triplet = Arrays.asList(nums[i], nums[j], complement);                    // 排序之后再放进set，避免插入内容相同位置不同的结果                    Collections.sort(triplet);                    result.add(triplet);                &#125;                // 记录nums[j]可以作为nums[i]的补充（complement）                seen.put(nums[j], i);            &#125;        &#125;        return new ArrayList&lt;&gt;(result);    &#125;&#125;\n\n11. Container With Most Water\nExample:\nInput: height = [1,8,6,2,5,4,8,3,7]Output: 49Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49.\n\n\nclass Solution &#123;    public int maxArea(int[] height) &#123;        int max = 0;        int left = 0;         int right = height.length - 1;        while (left &lt; right) &#123;            // 取容器最短的那个板子，木桶原理嘛            int ht = Math.min(height[left], height[right]);            // 板子左右有几格水            int water = right - left;            // 水的体积，也就是蓝色正方形的面积，底*高            int volume = ht * water;            // 记录最大的体积            max = Math.max(max, volume);            // 既然要找最大的体积，当然哪个板子短就换哪个            if (height[left] &lt; height[right]) &#123;                left++;            &#125; else &#123;                right--;            &#125;        &#125;        return max;    &#125;&#125;\n\n121. Best Time to Buy and Sell Stock\nExample:\nInput: prices = [7,1,5,3,6,4]Output: 5Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5.Note that buying on day 2 and selling on day 1 is not allowed because you must buy before you sell.\nclass Solution &#123;    public int maxProfit(int[] prices) &#123;        if (prices.length &lt; 2) &#123;            return 0;        &#125;        int buyPrice = Integer.MAX_VALUE;        int overallProfit = 0;        int profitIfSoldToday = 0;        for (int i = 0; i &lt; prices.length; i++) &#123;            // 让buyPrice保存最小的价格            if (prices[i] &lt; buyPrice) &#123;                buyPrice = prices[i];            &#125;            // 计算今日价格与买入价的差            profitIfSoldToday = prices[i] - buyPrice;            // overallProfit记录最大的总利润            if (overallProfit &lt; profitIfSoldToday) &#123;                overallProfit = profitIfSoldToday;            &#125;        &#125;        return overallProfit;    &#125;&#125;\n\n108. Convert Sorted Array to Binary Search Tree\nExample:\nInput: nums = [-10,-3,0,5,9]Output: [0,-3,9,-10,null,5]\n\n\n/** * Definition for a binary tree node. * public class TreeNode &#123; *     int val; *     TreeNode left; *     TreeNode right; *     TreeNode() &#123;&#125; *     TreeNode(int val) &#123; this.val = val; &#125; *     TreeNode(int val, TreeNode left, TreeNode right) &#123; *         this.val = val; *         this.left = left; *         this.right = right; *     &#125; * &#125; */class Solution &#123;    public TreeNode sortedArrayToBST(int[] nums) &#123;        return builder(nums, 0, nums.length - 1);    &#125;        private TreeNode builder(int[] nums, int left, int right) &#123;        if (left &gt; right) &#123;            return null;        &#125;        // 从中间将nums二分        // 比如在第一层递归中，        // [-10,-3,0,5,9] -&gt; [-10,-3,0] [5,9]        int middle = (left + right) / 2;        // 中间的数字作为二叉搜索树的根节点        TreeNode rootNode = new TreeNode(nums[middle]);        // 第一层递归中[-10, -3]拿去构造左边的子节点        rootNode.left = builder(nums, left, middle - 1);        // 第一层递归中[5,9]拿去构造右边的子节点        rootNode.right = builder(nums, middle + 1, right);        return rootNode;    &#125;&#125;\n\n思路就是递归+二分，或许还有些分治思想？先找见根节点，然后把数组左右分成两半，在递归里面再重复这样的操作，直到只有一个根节点，也就是最下面的叶子节点。最后往上组装。数据怎么跑的debug一下看看吧，用语言描述肯定要乱死。\n146. LRU Cache\nExample: \nInput[&quot;LRUCache&quot;, &quot;put&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;get&quot;, &quot;get&quot;][[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]Output[null, null, null, 1, null, -1, null, -1, 3, 4]ExplanationLRUCache lRUCache = new LRUCache(2);lRUCache.put(1, 1); // cache is &#123;1=1&#125;lRUCache.put(2, 2); // cache is &#123;1=1, 2=2&#125;lRUCache.get(1);    // return 1lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is &#123;1=1, 3=3&#125;lRUCache.get(2);    // returns -1 (not found)lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is &#123;4=4, 3=3&#125;lRUCache.get(1);    // return -1 (not found)lRUCache.get(3);    // return 3lRUCache.get(4);    // return 4\n解题思路：\nLRU Cache，即Least Recently Used Cache，其运作机理是，如果在cache已满的时候添加新的记录，那么要先删掉其中最不常用的记录，然后添加新的记录。\n我们可以用双向链表来实现这个效果，越靠近链表头，就代表这个元素越常被用到；反之越靠近链表尾，这个元素就越不常用，链表尾的前一个元素也将是在cache满了之后被删掉的那个元素。\n那么首先需要创建一个类Node，代表双向链表中的节点。\npublic class Node &#123;    Node next;    Node prev;    int key;    int value;    public Node(int key, int value) &#123;        this.key = key;        this.value = value;    &#125;&#125;\n\n这时候就可以初始化LRUCache这个类的结构了。\nclass LRUCache &#123;    // 链表头节点，永远不动    private final Node head = new Node(0, 0);    // 链表尾节点，永远不动    private final Node tail = new Node(0, 0);    // 存储key与Node的映射，get的时候实际上是从这取值    private final Map&lt;Integer, Node&gt; map = new HashMap&lt;&gt;();    // cache容量    private final int capacity;    public LRUCache(int capacity) &#123;        this.capacity = capacity;        // 双向链表头尾相连        head.next = tail;        tail.prev = head;    &#125;&#125;\n\n接下来先实现get方法：\n// 当一个节点被get// 那么就把它提到双向链表的头部位置public int get(int key) &#123;    // 根据题目要求，key不存在就返回-1    if (!map.containsKey(key)) &#123;        return -1;    &#125;    Node node = map.get(key);    Node nodeNext = node.next;    Node nodePrev = node.prev;    // 先把这个node从当前位置抹去    nodePrev.next = nodeNext;    nodeNext.prev = nodePrev;    // 然后插入到头部    Node headNext = head.next;    node.next = headNext;    node.prev = head;    headNext.prev = node;    head.next = node;    return node.value;&#125;\n\n接下来实现put方法：\npublic void put(int key, int value) &#123;    // 如果key已经存在    // 那么put相当于更新value，同时刷新在LRU Cache中的位置    // 所以先把已有的这个node删掉    if (map.containsKey(key)) &#123;        Node node = map.remove(key);                Node nodeNext = node.next;        Node nodePrev = node.prev;        nodePrev.next = nodeNext;        nodeNext.prev = nodePrev;    &#125;    // 如果cache满了，就要先删掉末尾的节点（末位淘汰制？    if (map.size() == capacity) &#123;        Node leastRecentUsedNode = tail.prev;        map.remove(leastRecentUsedNode.key);        leastRecentUsedNode.prev = tail;        tail.prev = leastRecentUsedNode.prev;    &#125;    Node newNode = new Node(key, value);    map.put(key, newNode);    // 把这个新建的node塞到head和headNext之间    Node headNext = head.next;    newNode.prev = head;    newNode.next = headNext;    headNext.prev = newNode;    head.next = newNode;&#125;\n\n可见新增节点和删除节点操作的代码是经常重复的，所以抽成两个单独的方法。最后完整的解题代码如下：\nclass LRUCache &#123;    private final Node head = new Node(0, 0);    private final Node tail = new Node(0, 0);    private final Map&lt;Integer, Node&gt; map = new HashMap&lt;&gt;();    private final int capacity;        public LRUCache(int capacity) &#123;        this.capacity = capacity;        head.next = tail;        tail.prev = head;    &#125;        public int get(int key) &#123;        if (!map.containsKey(key)) &#123;            return -1;        &#125;        Node node = map.get(key);        remove(node);        insert(node);        return node.value;    &#125;        public void put(int key, int value) &#123;        if (map.containsKey(key)) &#123;            remove(map.get(key));        &#125;        if (map.size() == capacity) &#123;            remove(tail.prev);        &#125;        insert(new Node(key, value));    &#125;    private void remove(Node node) &#123;        map.remove(node.key);        node.prev.next = node.next;        node.next.prev = node.prev;    &#125;    private void insert(Node node) &#123;        map.put(node.key, node);        Node headNext = head.next;        node.prev = head;        node.next = headNext;        headNext.prev = node;        head.next = node;    &#125;    public class Node &#123;        Node next;        Node prev;        int key;        int value;        public Node(int key, int value) &#123;            this.key = key;            this.value = value;        &#125;    &#125;&#125;/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */\n\n9. Palindrome number\nExample:\nInput: x = 121Output: trueExplanation: 121 reads as 121 from left to right and from right to left.\nclass Solution &#123;    public boolean isPalindrome(int x) &#123;        if (x &lt; 0) &#123;            return false;        &#125;                int input = x;                int reversedNum = 0;                // x从低位往高位取数字，然后从高位到低位填给reversedNum        // 即把x反转        while (x != 0) &#123;            // reversedNum乘10，加上x除以10的余数，即当前x的末位数            reversedNum = reversedNum * 10 + x % 10;            // x除以10，抛掉当前的末位数            x = x / 10;        &#125;            if (reversedNum &gt; Integer.MAX_VALUE || reversedNum &lt; Integer.MIN_VALUE) &#123;            return false;        &#125;                return input == reversedNum;    &#125;&#125;\n\n计算过程：\n\nx = 121，reversedNum = 0；reversedNum = reversedNum * 10 + x % 10 = 0 + 1 = 1；x = x / 10 = 121 / 10 = 12\nx = 12，reversedNum = 1；reversedNum = reversedNum * 10 + x % 10 = 10 + 2 = 12；x = x / 10 = 12 / 10 = 1\nx = 1，reversedNum = 12；reversedNum = reversedNum * 10 + x % 10 = 120 + 1 = 121；x = x / 10 = 1 / 10 = 0\ninput == reversedNum =&gt; 121 == 121 =&gt; true\n\n写到这想到还有个粗暴解法，把数字当成字符串，翻转一下然后比较两个字符串是不是一样不就行了，做什么数学题？\nclass Solution &#123;    public boolean isPalindrome(int x) &#123;        String inputString = Integer.toString(x);        String reversedString = new StringBuilder(inputString).reverse().toString();        return inputString.equals(reversedString);    &#125;&#125;\n\n多清爽，三行完事还不烧脑子。\n13. Roman to Integer\nExample:\nInput: s = &quot;MCMXCIV&quot;Output: 1994Explanation: M = 1000, CM = 900, XC = 90 and IV = 4.\nclass Solution &#123;    public int romanToInt(String s) &#123;        // 基本罗马数字        final HashMap&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(7);        map.put(&#x27;I&#x27;, 1);        map.put(&#x27;V&#x27;, 5);        map.put(&#x27;X&#x27;, 10);        map.put(&#x27;L&#x27;, 50);        map.put(&#x27;C&#x27;, 100);        map.put(&#x27;D&#x27;, 500);        map.put(&#x27;M&#x27;, 1000);        // IV这种左减格式的数字收拾起来太麻烦，        // 直接转成连续的基本数字        final String sanitized = s                .replace(&quot;IV&quot;, &quot;IIII&quot;)                .replace(&quot;IX&quot;, &quot;VIIII&quot;)                .replace(&quot;XL&quot;, &quot;XXXX&quot;)                .replace(&quot;XC&quot;, &quot;LXXXX&quot;)                .replace(&quot;CD&quot;, &quot;CCCC&quot;)                .replace(&quot;CM&quot;, &quot;DCCCC&quot;);        final char[] chars = sanitized.toCharArray();        int result = 0;        for (char aChar : chars) &#123;            result += map.get(aChar);        &#125;        return result;    &#125;&#125;\n\n计算过程：\n\nMCMXCIV(1994) &#x3D;&gt; M CM XC IV(1000 900 90 4) &#x3D;&gt; M DCCC LXXXX IIII\n从头到尾遍历MDCCCLXXXXIIII中每个字符，去map里面找对应的阿拉伯数字，加起来就完事了\n\n一开始做的时候愁死我了，没有左减格式的数字没啥难度，想破脑袋也没想到怎么处理左减，抄答案发现还能这么玩，属实打开思路了。\n252. Meeting Rooms\nGiven an array of meeting time intervals where intervals[i] = [starti, endi], determine if a person could attend all meetings.\nExample:\nInput: intervals = [[0,30],[5,10],[15,20]]Output: false\nEasy级别的题，思路很简单，先把会议安排按照开始时间升序排列，然后比较下一场会议的开始时间是否小于上一场会议的结束时间，是的话就说明这个人无法参加全部会议。\nclass Solution &#123;    public boolean canAttendMeetings(int[][] intervals) &#123;        if (intervals.length &lt;= 1) &#123;            return true;        &#125;        // Sort with the meeting start time        Arrays.sort(intervals, (a, b) -&gt; a[0] - b[0]);        int lastEndTime = intervals[0][1];        for (int i = 1; i &lt; intervals.length; i++) &#123;            int currentBeginTime = intervals[i][0];            if (currentBeginTime &lt; lastEndTime) &#123;                return false;            &#125;            lastEndTime = intervals[i][1];        &#125;        return true;    &#125;&#125;\n\n253. Meeting Rooms II\nGiven an array of meeting time intervals intervals where intervals[i] = [starti, endi], return the minimum number of conference rooms required.\nExample:\nInput: intervals = [[0,30],[5,10],[15,20]]Output: 2\n这道题要用到优先队列，具体算法见注释。\nclass Solution &#123;    public int minMeetingRooms(int[][] intervals) &#123;        if (intervals.length &lt;= 1) &#123;            return intervals.length;        &#125;        // 定义一个优先队列，存储会议的结束时间，按升序排列        // 因为会议结束，下一场会议才能占用这个会议室        // 所以只需要比较当前会议的开始时间与上一场会议的结束时间        PriorityQueue&lt;Integer&gt; allocator = new PriorityQueue&lt;&gt;(            intervals.length,            (a, b) -&gt; a - b        );        // 重新排序intervals，按照会议的开始时间升序排列        Arrays.sort(            intervals,             (a, b) -&gt; a[0] - b[0]);        // 第一场会议占用一个会议室        allocator.add(intervals[0][1]);        for (int i = 1; i &lt; intervals.length; i++) &#123;            // 从第二场会议开始检查            if (intervals[i][0] &gt;= allocator.peek()) &#123;                // 如果第二场会议在开始的时候，上一场会议已经结束                // 那么释放掉上一场会议的会议室                // 也就是删除掉优先队列的第一个元素                allocator.poll();            &#125;            // 当前会议肯定要占一个会议室            allocator.add(intervals[i][1]);        &#125;        return allocator.size();    &#125;&#125;\n\n655. Print Binary Tree抄答案Java Easy Solution - shawonnirob16\n/** * Definition for a binary tree node. * public class TreeNode &#123; *     int val; *     TreeNode left; *     TreeNode right; *     TreeNode() &#123;&#125; *     TreeNode(int val) &#123; this.val = val; &#125; *     TreeNode(int val, TreeNode left, TreeNode right) &#123; *         this.val = val; *         this.left = left; *         this.right = right; *     &#125; * &#125; */class Solution &#123;    public List&lt;List&lt;String&gt;&gt; printTree(TreeNode root) &#123;        List&lt;List&lt;String&gt;&gt; result = new ArrayList&lt;&gt;();        int height = getHeight(root);        int row = height + 1;        int column = (int) Math.pow(2, height + 1) - 1;        // 先全部填充空字符串        for (int k = 0; k &lt; row; k++) &#123;            List&lt;String&gt; list = new ArrayList&lt;&gt;();            for (int i = 0; i &lt; column; i++) &#123;                list.add(&quot;&quot;);            &#125;            result.add(list);        &#125;        int left = 0;        int right = column - 1;        int level = 0;        fill(result, left, right, level, root);        return result;    &#125;    // 递归整个树，每层高度加一，计算树的高度    private int getHeight(TreeNode root) &#123;        if (root == null) &#123;            return -1;        &#125;        int left = getHeight(root.left);        int right = getHeight(root.right);        return 1 + Math.max(left, right);    &#125;    private void fill(        List&lt;List&lt;String&gt;&gt; result,        int left,        int right,        int level,        TreeNode root    ) &#123;        if (root == null) &#123;            return;        &#125;        // 计算list的中间位置的下标        int middle = left + (right - left) / 2;        // 在中间位置填入当前节点的值        result.get(level).set(middle, String.valueOf(root.val));        fill(result, left, middle - 1, level + 1, root.left);        fill(result, middle + 1, right, level + 1, root.right);    &#125;&#125;\n","categories":["学知识"],"tags":["LeetCode","乐扣"]},{"title":"购买新的Linux服务器后需要做的安全措施","url":"/linux-server-security-measures.html","content":"在购买了一台安装Linux系统的VPS之后，首先要做好一定的安全防护措施，来尽可能地保证你的VPS不会沦为某些人的肉鸡。\n\n\n修改SSH默认端口这个世界上，总有不少无聊的人做着SSH弱口令扫描这种事情，相信我，你的IP一定是在被扫描的IP段里的，所以，更换掉SSH的默认端口会是个好主意。\n用你喜欢的文本编辑器，编辑 /etc/ssh/sshd_config ，修改如下内容：\n# SSH默认端口号为22# 将其修改为一个自定义的端口号，比如15Port 15\n\n保存配置文件，并重启SSH服务端进程，使新的配置生效。注意此时不要退出现在的SSH会话，因为一旦出现问题，我们还可以使用这个会话来修复，否则就只能通过虚拟控制台来登陆了\n接下来新开一个SSH会话，记得通过新指定的端口号来连接，以检查配置是否正确无误。没问题之后，就可以关掉其中一个SSH会话了，毕竟还是不要浪费资源，注意环保不是。\n启用防火墙一台放在公网上的，没有防火墙的服务器，那就是一台公交车。那么怎么变成私家车呢？废话，开防火墙啊！\n目前已经有一些iptables的前端工具，来简化防火墙的配置了，所以不建议直接操作iptables。当然如果您有信心，那就当我没说。\n不同发行版所使用的iptables前端可能是不一样的，比如CentOS使用的是 firewalld ，而Ubuntu使用的是 ufw 。由于我使用的是Ubuntu，所以下面就以 ufw 来举例了，使用 firewalld 的同学，可以参考这篇文章来学习。\n# 首先检查防火墙是否在运行$ sudo ufw statusStatus: inactive # 说明目前ufw并没有启动# 启动ufw$ sudo ufw enable# 防火墙已激活，并会跟随系统自动启动Firewall is active and enabled on system startup# 把SSH的端口加入到开放的列表中# 否则这个会话断掉之后，你就再也别想用SSH登陆了$ ufw allow 15Rule addedRule added (v6)# 看一下是不是成功加进去了$ sudo ufw statusStatus: activeTo                         Action      From--                         ------      ----15                         ALLOW       Anywhere15 (v6)                    ALLOW       Anywhere (v6)\n\n这样防火墙就启动了，并且仅接受来自15端口的入站请求。\n在配置放行列表的时候，注意仅添加必要的端口。乱开一气，或者直接允许所有端口，那就跟没有防火墙一样了。\n禁止root用户通过SSH登陆地球人都知道，root是Linux系统中权限最高的用户，同时也是最危险的用户。所以当然不能开放root远程登录的权限。\n在此之前，你需要先创建一个自己的用户，并且设置好密码，保证可以正常登陆到系统中。至于创建用户的方法我这里就不赘述了。\n有了自己的用户之后，编辑 /etc/ssh/sshd_config ，修改如下内容：\n# 允许使用root用户登陆PermitRootLogin no\n\n保存并重启SSH服务，就可以阻止root用户通过SSH登陆了。\n使用RSA密钥对登陆SSH在防止被破解的角度上，光是换端口和开防火墙，是远远不够的。还记得上文提到的SSH弱口令扫描吗？相信我，你绞尽脑汁想出来的密码，还真不一定打得过字典，而VPS提供商给你的默认密码，你确定你能记得住？但是RSA密钥对不仅能抵抗字典攻击，还不需要你记忆什么登陆信息，何乐而不为？\n生成密钥对Windows下生成密钥对如果你用的是Windows，那么可以参考这篇文章来生成你的密钥对，并将其上传至服务器。\nLinux下生成密钥对如果你用的是Linux，那么可以使用 ssh-keygen 命令生成密钥对。\n首先在本地电脑上生成密钥对\n$ ssh-keygen -b 2048Generating public/private rsa key pair.Enter file in which to save the key (/Users/boris/.ssh/id_rsa): 密钥文件的位置，回车使用默认值Enter passphrase (empty for no passphrase): 密钥的密码，留空表示没有密码Enter same passphrase again: 确认密码Your identification has been saved in /Users/boris/.ssh/id_rsa.Your public key has been saved in /Users/boris/.ssh/id_rsa.pub.The key fingerprint is:SHA256:K23EGsTSYVod8LbR/6MRHm3XHJbAlTxCbewMUXmfaOo boris@Boris-MacBook-Pro.localThe key&#x27;s randomart image is:+---[RSA 2048]----+|      =o..  o+Boo||     * o..   +.Oo||    o + + .   B+=||     o o o . +.=+||      . S   * o +||       = . o = . ||  .   o + . o o  ||       o   E o . ||   +        .    |+----[SHA256]-----+\n\n生成好了之后，就可以使用 ssh-copy-id 来传输公钥到远程服务器了：\nssh-copy-id -i ~/.ssh/id_rsa &lt;USERNAME&gt;@&lt;HOST&gt;\n\n然后就可以使用密钥登陆来测试了，方法很简单，只需要为 ssh 命令添加 -i 参数并指定私钥文件即可，如果私钥文件名为 id_rsa 的话， -i 参数也可以省略了。\n如果觉得使用密钥登陆的命令过长，那么可以参考我的另一篇博客使用 SSH config 简化 SSH 连接来简化SSH客户端的操作。\n配置SSH仅接受密钥登陆在服务器上编辑 /etc/ssh/sshd_config 修改如下内容：\n# 启用公钥认证PubkeyAuthentication yes# 禁用密码认证PasswordAuthentication no# 不允许空密码登陆PermitEmptyPasswords no\n\n保存文件并重启SSH服务，然后服务器就只会接受通过密钥认证的登陆请求了，在密钥错误或者未提供密钥的时候，服务器会直接拒绝请求。\n做完以上几步之后，通常的端口扫描和弱口令攻击基本上就免疫了，但是安全没有终点，在日常维护中还是需要时常检查鉴权日志和防火墙日志，以保证系统仍处于安全的状态下。在配置其他应用的时候，也要考虑到应用的安全相关的配置。\n","categories":["瞎折腾"],"tags":["Linux"]},{"title":"在macOS中通过SSH进行X11转发","url":"/macos-install-x11-client.html","content":"本文记录如何在macOS中安装X11客户端，并通过SSH进行X11转发。\n\n\n安装X11客户端在macOS中，可以使用XQuartz作为X11客户端。可以到XQuartz Releases下载安装包手动安装，也可使用Homebrew安装。\n使用Homebrew安装XQuartz的命令如下：\n$ brew cask install xquartz\n\n注意安装期间需要提供管理员密码以完成安装。安装完成之后需要完全退出并重启终端模拟器。\n检查远程服务器配置编辑/etc/ssh/sshd_config，设定如下条目：\nX11Forwarding yesX11DisplayOffset 10\n\n然后重启sshd使配置生效：\nsudo systemctl restart sshd\n\n转发远程X11程序使用ssh -X连接到远程服务器，执行任意X11程序，然后程序的窗口就会在本机显示。\n","categories":["瞎折腾"],"tags":["XQuartz","X11"]},{"title":"使用Homebrew Bundle管理电脑上安装的软件","url":"/macos-manage-softwares-with-homebrew-bundle.html","content":"今天看到一篇推文说Homebrew也支持类似package.json的方式来管理电脑上安装的应用，感觉这玩意挺实用的，像我上回换电脑还是照着brew list的输出再一个个装，有了它的话就可以一键安装了。而且除了Homebrew之外，它还可以导出通过App Store安装的软件和VS Code插件。那么，就小小折腾一下。\n\n\n首次运行brew bundle的时候它就会自动安装了。如果要导出App Store安装的程序的话，那么还需要安装mas这个Formula。\n然后就可以执行brew bundle dump，它会在当前目录下生成一个Brewfile文件，里面就是你电脑上目前装的软件，内容就像这样：\n# 各个Homebrew仓库tap &quot;beeftornado/rmtree&quot;tap &quot;bell-sw/liberica&quot;tap &quot;homebrew/autoupdate&quot;tap &quot;homebrew/bundle&quot;tap &quot;homebrew/cask-fonts&quot;tap &quot;homebrew/cask-versions&quot;# 通过Homebrew安装的Formulabrew &quot;cloudflared&quot;brew &quot;curl&quot;brew &quot;git&quot;# 通过Homebrew安装的Caskcask &quot;1password&quot;cask &quot;alfred&quot;cask &quot;altserver&quot;cask &quot;arc&quot;cask &quot;bartender&quot;# 通过App Store安装的软件mas &quot;DaVinci Resolve&quot;, id: 571213070mas &quot;Eul&quot;, id: 1541991958mas &quot;Termius&quot;, id: 1176074088mas &quot;WireGuard&quot;, id: 1451685025mas &quot;Xnip&quot;, id: 1221250572# VS Code插件vscode &quot;ms-python.debugpy&quot;vscode &quot;ms-python.python&quot;vscode &quot;redhat.vscode-yaml&quot;\n\n我的这个Brewfile算是简单的，它还支持配置安装参数等，具体可以参考Homebrew Bundle的实例。\n如果在导出的时候在当前目录已经存在一个Brewfile，那么它会报错说文件已存在。这时候可以加--force参数强制导出，要在cron里定时导出的话这个参数会很有用。此外如果你想要每个软件的简介，那么还可以加上--describe参数，出来的Brewfile就会是这样的：\n# Mac App Store command-line interfacebrew &quot;mas&quot;# Java-based project managementbrew &quot;maven&quot;# Menu bar icon organisercask &quot;bartender&quot;\n\n不过--describe参数只对brew和cask条目生效，tap、mas和vscode条目都是没有描述的。\n我试用后发现，似乎mas这部分会把你已经卸载的软件也列出来，而第二次再dump的时候又另外发现了一个之前没列出来而已经通过App Store安装的程序，所以这部分可能需要在导出之后自己再人工校验一下。\n要按照Brewfile安装软件的话，只需要cd到Brewfile所在位置然后执行brew bundle就会自动开始安装。\n如果要把不在Brewfile中的软件删除，那么可以运行brew bundle cleanup命令。\n此外，运行brew bundle check可以检查哪些软件在Brewfile里列出来了，但是还没安装。但是我用刚刚导出的Brewfile检查却报了brew bundle can&#39;t satisfy your Brewfile&#39;s dependencies.Satisfy missing dependencies with ``brew bundle install``.这样的消息，我就有点怀疑这个检查到底准不准……\n","categories":["瞎折腾"],"tags":["macOS","Homebrew"]},{"title":"为Maven配置阿里云镜像和代理服务器","url":"/maven-configure-mirrors.html","content":"Maven中央仓库在国内的速度简直是感人，好在阿里云提供了Maven中央仓库的镜像，配置方法在此记录备用。\n\n\n打开Maven的用户配置文件(默认位置在 ~/.m2/settings.xml)，在mirrrors段加入如下内容：\n&lt;mirror&gt;    &lt;!-- 镜像ID，自行定义 --&gt;    &lt;id&gt;nexus-aliyun&lt;/id&gt;    &lt;!-- 该镜像对应的仓库名，central即中央仓库 --&gt;    &lt;!-- 个人建议不要将其设为星号 [注] --&gt;    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;    &lt;!-- 镜像名，自行定义 --&gt;    &lt;name&gt;Nexus aliyun&lt;/name&gt;    &lt;!-- 镜像的地址 --&gt;    &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;\n\n[注] 有些教程在 mirrorOf 字段中填写的是星号，但根据Using Mirrors for Repositories中 Using A Single Repository 一段的解释，这将会强制使用该镜像处理所有的仓库请求，而阿里云镜像并不能达到这样的效果，所以个人建议仅使用该镜像代理中央仓库的请求。\n","categories":["小技巧"],"tags":["Maven"]},{"title":"解决Maven的unable to find valid certification path to requested target错误","url":"/maven-unable-to-find-valid-certification-path-to-requested-target.html","content":"今天我在从公司的Maven私库上下载依赖时，Maven报出了unable to find valid certification path to requested target的问题，经查询是因为没有将私库使用的自签名证书导入到Java的keystore。这里写一下我的两个解决方案，方便发生了同样问题的人参考。\n\n\n让Maven忽略证书错误因为在默认情况下，自签名证书是不受信任的，那么我们只要让Maven忽略证书错误，就可以解决这个问题了。\n我们可以在.bashrc或.zshrc（或者其他位置，看你用哪个shell了）里面加入如下语句：\nexport MAVEN_OPTS=&#x27;-Dmaven.wagon.http.ssl.insecure=true -Dmaven.wagon.http.ssl.allowall=true -Djsse.enableSNIExtension=false&#x27;\n\n或者，也可以在~/.mavenrc中配置这个参数：\nMAVEN_OPTS=&quot;-Dmaven.wagon.http.ssl.insecure=true -Dmaven.wagon.http.ssl.allowall=true -Djsse.enableSNIExtension=false&quot;\n\n导入证书到keystore首先使用这个命令得到网站上面的证书：\n# 注意将mavenrepository.mycompany.com替换成实际的地址openssl s_client -showcerts -connect mavenrepository.mycompany.com:443\n\n在输出中可以看到类似这样的证书信息：\ns:/C=US/ST=North Carolina/L=Raleigh/O=Red Hat, Inc./OU=Red Hat Network/CN=Entitlement Master CA/emailAddress=ca-support@redhat.comi:/C=US/ST=North Carolina/L=Raleigh/O=Red Hat, Inc./OU=Red Hat Network/CN=Entitlement Master CA/emailAddress=ca-support@redhat.com\n\n我们要在这里面寻找issuer(i:)和subject(s:)相同的证书，这个就是我们要导出的根证书。找到之后，保存证书的哈希（即以—–BEGIN CERTIFICATE—–开始，以—–END CERTIFICATE—–结束的这部分内容）到一个文件中，比如root.crt。\n接下来，我们需要使用keytool把这个证书导入到Java的keystore中，命令如下：\nsudo keytool \\    -importcert \\    # keystore文件位置需要修改成实际的位置    -keystore /usr/local/java/jdk1.8.0_60/jre/lib/security/cacerts \\    # keystore密码需要修改成实际的密码，默认密码是changeit    -storepass changeit \\    # 要导入的证书，就是刚才保存的那个，注意使用实际的位置    -file ~/Downloads/root.crt \\    # 给证书起个名字    -alias &quot;mavenrepository-mycompany&quot;\n\n执行后keytool会询问是否信任这个证书，输入yes来信任。\n导入完成后，Maven就可以从这个私库下载依赖了。如果你使用IDEA的话，那么可能需要重新启动一下IDEA才会使新的keystore生效。\n[^1]: How to resolve “unable to find valid certification path to requested target” error? - JFrog[^2]: How to tell Maven to disregard SSL errors (and trusting all certs)?\n","categories":["小技巧"],"tags":["Maven"]},{"title":"防止MicroServer Gen 10 Plus的风扇突然狂飙","url":"/microserver-gen10-plus-prevent-fan-goes-crazy.html","content":"今年元旦我把之前用的Gen 8换成了Gen 10 Plus。后来想着现在性能上去了，可以把NAS的功能也合并进来，于是就把给NAS用的两块机械硬盘迁进了Gen 10 Plus里。但就在这之后，我发现它的风扇时不时就会突然狂转一分多钟，然后再降回平时的转速。而且这个问题不分白天黑夜，甚至有一次大半夜把我吵醒，不胜其烦。终于在上周，我找到了问题的根源，并彻底解决了。\n\n\n一些无效的尝试\n这一节并没有解决方案，如果你不关心我之前的心路历程，那么可以放心跳到下一节。\n\n最起初，我以为是机械硬盘发热导致的，毕竟问题是出在我把机械硬盘装进来之后。为此我还调换了硬盘的安装位置，把机械硬盘从3、4号盘位移到了靠上的1、3号盘位。\n---------------------          ---------------------| 1 - SSD | 3 - HDD |    =&gt;    | 1 - HDD | 3 - HDD || 2 - SSD | 4 - HDD |          | 2 - SSD | 4 - SSD |---------------------          ---------------------\n\n可惜，并没有任何好转。风扇依旧时不时开始跟李有田似的狂飙，唯一区别是风扇能刹住车（为李有田默哀一秒……\n后来我觉得，会不会是硬盘在跑PT的时候真的太热了？但是我也在下载的时候摸过硬盘，那个温度撑死只能算温热，怎么想也不会造成整机过热啊？但是本着“能解决问题就行，哪怕再花点钱”的想法，我开始找用来替换的4TB 2.5寸SATA接口固态硬盘。结果……并不理想，现在京东自营只找到了三款。更难受的是，根据论坛上的赛博邻居反馈，其中两款是数据火葬场；而剩下的那款，一块盘要卖我两千多块，也就是说两块盘加起来比我买这台Gen 10 Plus都贵。所以，换硬盘这个方案也被否了。\n正巧就在这个时候，它又开始狂飙。我想，iLO里面不是能看见温度吗？那赶紧看看是啥玩意这么热呗？这一看，才找到了问题的根源。\n问题根源及解决方案趁着风扇狂飙之际，我进入了iLO的温度页面，发现有个传感器12 - AHCI Max HD的当前温度是50°C，而它的警告阈值是60°C，严重错误阈值是70°C。也就是说，这时候BIOS发现这个东西烫的要挂掉了，所以就要赶紧让风扇转起来散热。等了一会风扇慢下来之后，我发现这个传感器的值变成了35°C，而且怎么刷新都不变。\n搁网上一顿冲浪之后找到了惠普的一篇通告，从这里得知这并不是一个物理上的传感器，它检测到的温度是通过惠普的一个叫Agentless Management Service的软件组件向主板汇报的。此外还找到日立的一个类似的通告说硬盘在AHCI模式下会导致风扇转速异常升高。\n到ESXi里面检查了之后，发现我用的惠普定制系统里面是带着AMS组件的。这时候我灵机一动，既然是因为AMS汇报了错误的温度才导致这个问题，而要让AMS汇报正确的温度，那我就得把硬盘换成HPE认证的盘，可想而知这玩意没准会相当贵，而且作为个人用户都不一定买得到。那如果我把AMS卸了，让传感器拿不到数据呢？\n说干就干。保险起见我先去了惠普支持网站，下载好了最新版的AMS软件包，一旦出了问题我还可以把它再装回去。然后回到ESXi，开启SSH。\n# 找到AMS的软件包名[root@ESXi:~] esxcli software vib list | grep ams# 这个是给Gen 10系列用的amsd 701.11.8.5.22-1OEM.701.0.0.16850804 HPE VMwareAccepted 2023-08-08# 这个是给Gen 11系列用的amsdv 701.11.3.0.17-1OEM.701.0.0.16850804 HPE VMwareAccepted 2023-06-12# 分别给这俩都卸了# 命令输出我已经忘了，而且这不重要，就不写了[root@ESXi:~] esxcli software vib remove -n amsdv[root@ESXi:~] esxcli software vib remove -n amsd\n\n两条卸载命令都成功之后，逐个关掉虚拟机，然后重启ESXi使修改生效。果不其然，重启之后，iLO系统信息里面显示Agentless Management Service不可用，温度页面里12 - AHCI HD Max传感器也消失了。经过一星期的测试，风扇也确实没再狂飙过。问题解决！\n一点碎碎念尽管这个解决方案远称不上完美，甚至有点“解决提出问题的人”的感觉。但这应该是权衡利弊之后的最优解。买HPE认证硬盘？拜托我只是拿它存电影而已，虽然要是全丢了也挺心疼，但它们确实不值这个钱。卖掉Gen 10 Plus再自己组一套NAS？那又太折腾了，我实在懒得搞。而且就算夏天天气热起来之后，我毕竟也不是重度PT玩家，大部分时间硬盘都是空载的，我相信它不会变得很热，不会因为俩机械硬盘搞炸整台服务器。\n","categories":["瞎折腾"],"tags":["Gen 10 Plus"]},{"title":"记我将博客的图片存储迁移到Backblaze","url":"/migrating-blog-images-to-backblaze.html","content":"之前我一直把博客的图片放在个人的OneDrive上，然后用嵌入功能得到外链放在博客里。但是考虑到国内访问OneDrive的延迟还是偏高，以及不折腾不舒服的心理作祟，于是乎在三月份的时候，我把博客的图片从OneDrive迁到了Backblaze对象存储。\n\n\n前期准备开始之前，我们需要准备好这些东西：\n\n一个Backblaze免费账号\n一个CloudFlare免费账号\n一个域名\n还有你的好心情 :-)\n\n可能你会担心用对象存储是不是会产生高额的账单，或者会因为超出配额导致图片全部无法加载。说实话我之前不敢用对象存储就是因为有这方面的顾虑，但是在Backblaze这里我们完全不用担心。首先，我们用的是免费的账户，而且Backblaze甚至不要求你添加信用卡。此外，Backblaze和CloudFlare都是带宽联盟的成员，意味着Backblaze与CloudFlare之间的流量全部是免费的。\n在Backblaze创建存储桶并上传图片登录进Backblaze的B2 Cloud Storage之后，点Create a Bucket创建一个存储桶就行了。为了防止被人恶意刷流量，我建议创建一个私有的存储桶。加密和对象锁都不需要。\n创建成功后，打开这个存储桶的Bucket Settings，在Bucket Info中添加{&quot;cache-control&quot;:&quot;max-age=43200&quot;}来配置桶的缓存时间。虽然流量不要钱，但是能环保还是环保一点比较好不是？\n因为我们创建的是私有存储桶，所以需要创建一个Application Key来允许第三方服务访问这个桶。虽然Backblaze默认提供了一个Master Application Key，但是这就像天天用root登录Linux主机一样，只有中午才能用，因为早晚会出事。在Application Keys页面，点Add a New Application Key，Allow access to Bucket(s)里面建议选我们这个桶而不是All，权限当然是Read and Write。创建成功之后，注意保存好keyID和applicationKey，因为applicationKey只会显示一次。\n然后需要下载一个支持浏览对象存储的工具，比如我用的S3 Browser。然后在S3 Browser中新建一个连接，REST Endpoint填写存储桶的Endpoint，Access Key ID就是刚才记下来的keyID，Secret Access Key就是applicationKey。\n\n如果S3 Browser可以成功连接到刚才创建的存储桶，那就说明配置正确了。这时候就可以想好目录结构，以及上传图片了。比如我选择把图片按照对应的博文来分类，每个有图的博文都有一个对应的图片目录。\n\n在CloudFlare中配置域名在到CloudFlare配置域名之前，我们先要知道指向一个文件的完整URL。进入Browse Files页面，然后进入这个存储桶，接着随便挑一个文件，点它最右边的详情图标，这里的Friendly URL就是我们要找的东西。记下URL里面的域名，我们接下来要用到。\n\n接下来就可以到CloudFlare里面创建一条CNAME记录，并把刚才记下来的域名填到目标里面，并且启用CloudFlare的代理，这样我们才能享受到带宽联盟的优惠。此外，我们还会针对这个域名配置一些规则，这也需要打开CloudFlare的代理开关。\n要注意这里只能是二级域名，如blog-static.boris1993.com，而不能是多级的（blog.static.boris1993.com），否则CloudFlare会无法申请证书，也就无法正常启用HTTPS。\n\n这时候我们就可以用https://sub-domain.your-domain.com/file/folder-name/image-name.png访问这个图片了，但是目前我们只能得到一个401页面，因为我们必须要带上一个Access Token才能访问私有存储桶的文件。\n为请求配置CloudFlare规则前往CloudFlare的规则页面，选择转换规则(Transform Rule)，然后在重写URL这个tab中新增一个规则。\n首先，我希望我可以直接用https://blog-static.boris1993.com/folder-name/file-name.png就能访问到图片（因为这样看起来更好看），所以我配置了一个路径重写，如果路径中不包含/file/bucket-name，那么就在路径中补上这一段。\n选择路径的重写到，表达式类型选择动态，表达式填写concat(&quot;/file/blog-pics&quot;, http.request.uri.path)。这样CloudFlare就会自动补全完整的路径。\n然后就是访问私有存储桶的Access Token。Backblaze支持把Access Token放在Authorization这个query parameter中，所以我们可以选择查询的重写到，表达式类型选择静态(Static)，值目前可以随便写，因为你就算现在拿到一个token，在24小时后也是会过期的，所以后面我会讲怎么用CloudFlare Workers来更新这个字段。\n\n接下来，根据Backblaze官方的建议，我们需要对响应头做一些修改。\n切换到修改响应头，新增这样一条规则：\n\n首先要正确配置Access-Control-Allow-Origin，来避免跨域问题，我偷懒了直接配了个*，不知道这么配会不会有盗链的问题，暂时先这样吧。\n其次Backblaze建议修改cache-control这个header，来延长缓存的有效时间。\n最后，需要从响应头中删掉一些Backblaze的header来增强安全性。\n为了方便，我把要删掉的header放在这里：\n\nx-bz-content-sha1\nx-bz-file-id\nx-bz-file-name\nx-bz-info-s3b-last-modified\nx-bz-info-sha256\nx-bz-info-src_last_modified_millis\nx-bz-upload-timestamp\n\n同时我为了能让浏览器缓存这个图片，我还让它添加了ETag这个header，但是我在浏览器里一直看不到这个header，如果有大佬知道为什么，还请不吝赐教。\n自动更新访问存储桶的Token因为后面要修改规则的内容，所以先得拿到规则集和规则的ID。规则ID好办，打开重写URL规则的编辑页面，我们就能在URL的最后一段得到这个规则的ID。但是规则集ID只能调CloudFlare API取得。\nGET https://api.cloudflare.com/client/v4/zones/YOUR_ZONE_ID/rulesetsAuthorization: Bearer YOUR_CLOUDFLARE_API_TOKEN\n\n把YOUR_ZONE_ID替换为你的域名的区域ID，以及把YOUR_CLOUDFLARE_API_TOKEN换成你的API令牌。我当时因为不知道这个API需要哪些权限，始终创建不出带有正确权限的API令牌，所以干脆用了Global API Key。\n这个请求会返回一系列规则集，有CloudFlare内部的，也有我们自己的。理论上，名字是default并且phase是http_request_transform的那个就是我们要的。但是为了确认，可以再执行这个请求：\nGET https://api.cloudflare.com/client/v4/zones/YOUR_ZONE_ID/rulesets/RULE_SET_IDAuthorization: Bearer YOUR_CLOUDFLARE_API_TOKEN\n\n跟上条请求一样，替换掉YOUR_ZONE_ID和YOUR_CLOUDFLARE_API_TOKEN，以及将RULE_SET_ID替换为上面找到的规则集的id。执行后会返回这个规则集下的规则。如果返回内容中有我们之前创建的那条重写URL的规则，那么这就是我们要找的规则集。\n然后为了安全起见，我们要为这个CloudFlare Worker创建一个API令牌。进入我的个人资料 –&gt; API令牌，然后点击创建令牌，在接下来的页面中中选择创建自定义令牌，然后如图创建一个令牌。\n\n添加成功后，妥善保存这个令牌。\n接下来前往CloudFlare Workers，创建一个新的Worker。然后到设置 –&gt; 变量，添加如下环境变量：\n\n\n\n变量名\n值\n\n\n\nB2KeyID\nBackblaze的keyID\n\n\nB2AppKey\nBackblaze的applicationKey\n\n\nB2BucketName\nBackblaze的存储桶名\n\n\nCfAuthKey\n上面创建的CloudFlare API令牌\n\n\nCfHostname\n上面在CloudFlare创建的二级域名\n\n\nCfZoneID\n你的域名的区域ID\n\n\nCfRulesetID\n上面拿到的规则集ID\n\n\nCfRuleID\n上面拿到的规则ID\n\n\n然后进入触发器，将路由中的那条记录禁用，因为我们不会用HTTP请求来触发这个Worker。然后再Cron触发器中添加一个Cron触发器。Backblaze说一个token的有效期最大不超过24小时，我为了保险起见，选择每半小时就触发这个Worker来生成一个新的token，即*/30 * * * *。\n\n至此前置任务完成，点击右上角的快速编辑，然后将如下脚本粘贴进去，然后点击保存并部署。\naddEventListener(&quot;fetch&quot;, (event) =&gt; &#123;  event.respondWith(handleRequest(event.request));&#125;);addEventListener(&quot;scheduled&quot;, (event) =&gt; &#123;  event.waitUntil(updateRule());&#125;);const getB2Token = async () =&gt; &#123;  const res = await fetch(    &quot;https://api.backblazeb2.com/b2api/v2/b2_authorize_account&quot;,    &#123;      headers: &#123;        Authorization: &quot;Basic &quot; + btoa(B2KeyID + &quot;:&quot; + B2AppKey),      &#125;,    &#125;  );  const data = await res.json();  return data.authorizationToken;&#125;;const updateRule = async () =&gt; &#123;  const b2Token = await getB2Token();  const res = await fetch(    `https://api.cloudflare.com/client/v4/zones/$&#123;CfZoneID&#125;/rulesets/$&#123;CfRulesetID&#125;/rules/$&#123;CfRuleID&#125;`,    &#123;      method: &quot;PATCH&quot;,      headers: &#123;        &quot;Authorization&quot;: `Bearer $&#123;CfAuthKey&#125;`      &#125;,      body:       `&#123;        &quot;description&quot;: &quot;Replace path for static files for blog&quot;,        &quot;action&quot;: &quot;rewrite&quot;,        &quot;expression&quot;: &quot;(http.host eq \\\\\\&quot;$&#123;CfHostname&#125;\\\\\\&quot; and not starts_with(http.request.uri.path, \\\\\\&quot;/file/$&#123;B2BucketName&#125;\\\\\\&quot;))&quot;,        &quot;action_parameters&quot;: &#123;          &quot;uri&quot;: &#123;              &quot;path&quot;: &#123;                  &quot;expression&quot;: &quot;concat(\\\\\\&quot;/file/$&#123;B2BucketName&#125;\\\\\\&quot;, http.request.uri.path)&quot;              &#125;,              &quot;query&quot;: &#123;                  &quot;value&quot;: &quot;Authorization=$&#123;b2Token&#125;&quot;              &#125;          &#125;        &#125;      &#125;`,    &#125;  );  const data = await res.text();  console.log(data);  return data;&#125;;async function handleRequest(request) &#123;  const data = await updateRule();  return new Response(data);&#125;\n\n等Worker被触发之后，就可以在浏览器中访问上面配置的域名，来测试到存储桶的连接是否正常。如果测试没问题，就可以把博客中的图片链接换到新地址了。\n","categories":["瞎折腾"],"tags":["对象存储","OSS","博客图片","博客"]},{"title":"使用VisualVM监控应用GC","url":"/monitor-gc-with-visual-vm.html","content":"我们知道，使用VisualVM可以监控Java应用的各种运行时信息，包括资源占用、正在运行的线程等等。本文将简单介绍如何使用VisualVM的插件Visual GC来监控Java应用的垃圾回收情况。\n\n\n安装Visual GC插件因为Visual GC插件并不是随VisualVM附带的，需要到插件商店下载。\n点击Tools–&gt;Plugins打开插件管理窗口，进入Available Plugins，在左侧窗格中勾选Visual GC，点击Install，同意许可协议后，稍等片刻即可完成安装。安装完成之后无需重启VisualVM。\n\n监控垃圾回收安装完成后，在VisualVM的Applications窗格中双击要监控的应用，然后在右侧窗格中进入Visual GC，即可以图形化的方式查看当前该应用中各个内存区域的情况。\n\n窗格中的Spaces部分展示的是当前各个区域的占用情况；Graphs部分则包括：\n\nCompile Time：编译时间，包括编译的次数，和累计的编译时间。图表中的一次脉冲就代表一次编译，脉冲越宽代表编译时间越长\nClass Loader Time：类加载时间，包括加载了多少个类(loaded)，卸载了多少个类(unloaded)，和累计用于类加载的时间\nGC Time：垃圾回收时间，包括垃圾回收的次数，垃圾回收消耗的时间，和上一次垃圾回收的原因\nEden Space：Eden区的空间，括号中第一位是最大容量，第二位是当前容量，其后冒号跟着的是当前使用了的大小，再往后是发生垃圾回收的次数，和垃圾回收消耗的时间\nSurvivor 0和Survivor 1：两个Survivor区的空间，同样包括最大容量、当前容量，和当前使用了的大小\nOld Gen：老年代的空间，同样包括最大容量、当前容量、当前使用了的大小、发生垃圾回收的次数，和垃圾回收消耗的时间\nMetaspace：Metaspace的空间(如果使用的是Java 1.7及之前版本，这里则是Perm Gen)，包括最大容量、当前容量，和当前使用了的大小\n\n","categories":["学知识"],"tags":["Java","VisualVM"]},{"title":"从umami v1迁移到v2","url":"/migrating-to-umami-v2.html","content":"昨天收到Vercel发的报警邮件，说我自己部署的分析平台umami部署失败。检查之后发现是umami有个大版本升级，而升级脚本失败导致新版本部署失败了。那既然报错了，就修呗。\n虽然不是什么复杂的问题，但还是记录一下好了，也当是水一篇文。\n\n\n问题的根源一开始发现自动迁移不成功，那么按照官方的迁移文档，我手动运行了迁移脚本，但是得到了这样的错误信息：db error: ERROR: must be owner of table _prisma_migrations。看起来是表的权限问题，_prisma_migrations这个表的owner必须是我用来执行脚本的用户。\n搜了一下，PostgreSQL里面每个表都有一个所有者，而我一开始是用postgres这个用户初始化的数据库，所以这些表的所有者都是postgres，之前没有问题，只是因为我给umami的用户读写这些表的授权了。\n修复数据库首先执行了下select schemaname, tablename, tableowner from pg_tables where schemaname = &#39;umami&#39;，果不其然这些表的owner都是postgres。\n于是尝试执行了下alter table umami_analytics._prisma_migrations owner to umami_analytics，但是又报错ERROR: must be member of role umami_user。好么，我一直以为postgres用户跟MySQL的root一样是超管，结果PostgreSQL世界里面人人生而平等？好吧，你要权限那我就给你授权，grant postgres to umami_user。\n授权之后，重新执行alter table umami_analytics._prisma_migrations owner to umami_analytics，发现成功了，再执行umami的迁移脚本后，发现错误信息变成了db error: ERROR: must be owner of table account。看来，上面的解决方案奏效了，接下来就是把剩下的表的所有者全都改过来。\nalter table umami_analytics._event_old owner to umami_analytics;alter table umami_analytics._prisma_migrations owner to umami_analytics;alter table umami_analytics.account owner to umami_analytics;alter table umami_analytics.&quot;event&quot; owner to umami_analytics;alter table umami_analytics.event_data owner to umami_analytics;alter table umami_analytics.pageview owner to umami_analytics;alter table umami_analytics.&quot;session&quot; owner to umami_analytics;alter table umami_analytics.website owner to umami_analytics;select schemaname, tablename, tableowner from pg_tables where schemaname = &#x27;umami_analytics&#x27;;\n\n可以看到，umami相关的表的所有者都正确了，回到Vercel，重新运行失败的部署，发现还是报错。无奈，又试了试手动迁移脚本，竟然成功了，这时候数据库肯定是v2的了，再到Vercel重新部署，这次就成功了。\n注意脚本执行到最后会问要不要删掉v1的表，记得不要删。我发现在删掉v1的旧表之后，Vercel的部署又会出新的问题。（我为了验证到底是我的问题还是脚本的问题，回滚了两三次生产数据库。也就是这个数据不重要我才敢这么折腾，好孩子不要学我乱搞生产数据库哦～）\n更新博客的配置NexT主题是内建了对umami的支持的，但是需要手动指定脚本的位置。根据迁移文档的提示，修正_config.next.yml中umami.script_url，重新部署博客即可。\n","categories":["瞎折腾"],"tags":["umami","analytics"]},{"title":"利用Grafana监控RouterOS运行状态","url":"/monitoring-routeros-with-grafana.html","content":"乱翻收藏夹的时候发现我还有个免费的Grafana Cloud，遂想着把我这些自建的东西都用它监控起来，反正不用白不用。那么第一个就拿我的RouterOS软路由开刀吧。\n\n\n环境\nMikrotik CHR 7.12\nGrafana Cloud - Cloud Free 订阅\nPrometheus 2.37\nmktxp \nCloudFlare Tunnel，如果你像我一样把Prometheus部署在家宽的话\n\n在RouterOS系统创建组和用户毕竟还是用第三方工具登陆路由器，还是遵循最小权限原则，给mktxp创建一个只包含必要的权限的账号比较好。\n/user/group add name=prometheus policy=read,api/user add name=prometheus group=prometheus password=changeme disabled=no\n\n配置mktxpmktxp是一个面向Mikrotik RouterOS的Prometheus exporter。选择这个而不是nshttpd/mikrotik-exporter主要出于以下两个原因：\n\nnshttpd/mikrotik-exporter已经停止更新，最后一次commit停留于2022年6月17日\n它每一次获取数据都会登入和登出，而这会导致RouterOS的日志里面充斥prometheus用户的登入和登出记录，就像这样：\n\n我使用Docker部署mktxp，它需要两个配置文件：mktxp.conf和_mktxp.conf。\n_mktxp.conf负责mktxp的运行配置，比如端口号、数据获取的间隔时间等。内容如下：\n[MKTXP]    port = 49090    socket_timeout = 2    initial_delay_on_failure = 120    max_delay_on_failure = 900    delay_inc_div = 5    bandwidth = True                # Turns metrics bandwidth metrics collection on / off    bandwidth_test_interval = 420   # Interval for colllecting bandwidth metrics    minimal_collect_interval = 5    # Minimal metric collection interval    verbose_mode = False            # Set it on for troubleshooting    fetch_routers_in_parallel = False   # Set to True if you want to fetch multiple routers parallel    max_worker_threads = 5              # Max number of worker threads that can fetch routers. Meaningless if fetch_routers_in_parallel is set to False    max_scrape_duration = 10            # Max duration of individual routers&#x27; metrics collection    total_max_scrape_duration = 30      # Max overall duration of all metrics collection\n\nmktxp.conf用于配置要监控的RouterOS实例，内容如下：\n# Router为路由器的代号，可以改成自己喜欢的值# 将来在Grafana就是用这个来区分各个RouterOS设备[Router]    # 是否启用对这个RouterOS设备的监控    enabled = True    # 路由器的地址    hostname = 192.168.1.1    # RouterOS API服务的端口    port = 8728    # 填写上面创建的 prometheus 用户的账号和密码    username = prometheus    password = changeme    # SSL部分关闭就行    use_ssl = False                 # enables connection via API-SSL servis    no_ssl_certificate = False      # enables API_SSL connect without router SSL certificate    ssl_certificate_verify = False  # turns SSL certificate verification on / off    # 以下为各个监控的开关，按需设定即可    installed_packages = True       # Installed packages    dhcp = True                     # DHCP general metrics    dhcp_lease = True               # DHCP lease metrics    connections = True              # IP connections metrics    connection_stats = False        # Open IP connections metrics    pool = True                     # Pool metrics    interface = True                # Interfaces traffic metrics    firewall = True                 # IPv4 Firewall rules traffic metrics    ipv6_firewall = False           # IPv6 Firewall rules traffic metrics    ipv6_neighbor = False           # Reachable IPv6 Neighbors    poe = False                     # POE metrics    monitor = True                  # Interface monitor metrics    netwatch = True                 # Netwatch metrics    public_ip = True                # Public IP metrics    route = True                    # Routes metrics    wireless = False                # WLAN general metrics    wireless_clients = False        # WLAN clients metrics    capsman = False                 # CAPsMAN general metrics    capsman_clients = False         # CAPsMAN clients metrics    user = True                     # Active Users metrics    queue = True                    # Queues metrics    remote_dhcp_entry = None        # An MKTXP entry for remote DHCP info resolution (capsman/wireless)    use_comments_over_names = True  # when available, forces using comments over the interfaces names    check_for_updates = False       # check for available ROS updates\n\n然后用如下docker-compose.yml启动mktxp即可：\n---version: &#x27;3&#x27;services:  mktxp:    image: ghcr.io/akpw/mktxp:latest    container_name: mktxp    restart: always    environment:    - TZ=Asia/Shanghai    ports:    - 49090:49090    volumes:    - &lt;存放以上两个conf的目录&gt;:/home/mktxp/mktxp/\n\nmktxp在启动成功的情况下是没有日志输出的，访问49090端口（即_mktxp.conf中配置的端口），如果能看到一大片Prometheus的metrics，那就说明启动成功了。\n配置Prometheus在prometheus.yml中添加如下配置：\nscrape_configs:  - job_name: &#x27;mikrotik_exporter&#x27;    static_configs:      - targets: [&#x27;mktxp的主机地址:49090&#x27;]        # 标签按需，不想要可以去掉        labels:          instance: &#x27;CHR&#x27;          environment: &#x27;Production&#x27;\n\n重启Prometheus，然后到Prometheus的Status -&gt; Targets中，检查mikrotik_exporter这个target是否存在，以及State是不是UP。\n配置Grafana如果你的Prometheus是部署在家宽环境，那在配置Grafana之前需要先做个内网穿透，让Prometheus的9090/tcp端口能被外网访问到。内网穿透的方案有很多，比如我就用的CloudFlare Tunnel。因为本文不是讲内网穿透，所以就不展开讲配置了。\n到Grafana的Home -&gt; Connections -&gt; Data sources中，添加一个新的Prometheus数据源，其中Prometheus server URL填你的Prometheus服务的地址，别的不用管，Save &amp; test成功就没问题。此外，还可以到Grafana的Explore页面查询一个mktxp的metrics，来检查Grafana是否能成功获取到数据。\n\n确认Grafana能成功获取到数据后，就可以导入mktxp的Grafana Dashboard了。到Grafana的Dashboards页面，点击New按钮后选择Import，填写这个dashboard的ID13679，点Load，在下一个页面给这个dashboard绑定我们的Prometheus，然后点Import，就可以用了。\n\n","categories":["瞎折腾"],"tags":["RouterOS","Prometheus","Grafana"]},{"title":"MySQL的最左匹配原则","url":"/mysql-leftmost-prefix.html","content":"依旧是面试查漏补缺。这次是MySQL的最左匹配原则。\n废话不多说，直接看MySQL官方文档给出的例子。\n有这么一个表：\nCREATE TABLE test (    id         INT NOT NULL,    last_name  CHAR(30) NOT NULL,    first_name CHAR(30) NOT NULL,    PRIMARY KEY (id),    INDEX name (last_name, first_name));\n\n可以看出，表里面有一个联合索引(last_name, first_name)。那么，查询的时候，这些语句就可以成功走索引：\nSELECT * FROM test WHERE last_name = &#x27;Doe&#x27;;SELECT * FROM test WHERE last_name = &#x27;Doe&#x27; AND first_name = &#x27;John&#x27;;SELECT * FROM test WHERE last_name = &#x27;Doe&#x27; AND (first_name = &#x27;John&#x27; OR first_name = &#x27;Jane&#x27;);SELECT * FROM test WHERE last_name = &#x27;Doe&#x27; AND first_name &gt;= &#x27;M&#x27; AND first_name &lt; &#x27;N&#x27;;\n\n而这些查询就无法使用这个索引：\nSELECT * FROM test WHERE first_name = &#x27;John&#x27;;SELECT * FROM test WHERE last_name = &#x27;Doe&#x27; OR first_name = &#x27;John&#x27;;\n\n也就是说，上面例子中的last_name列就是索引的最左前缀，如果要在查询中使用这个索引，那么条件中必须包含last_name，或者同时包含last_name和first_name。\n\nMySQL can use multiple-column indexes for queries that test all the columns in the index, or queries that test just the first column, the first two columns, the first three columns, and so on. If you specify the columns in the right order in the index definition, a single composite index can speed up several kinds of queries on the same table.\n\n","categories":["学知识"],"tags":["MySQL","面试","最左匹配原则","leftmost prefix"]},{"title":"新玩具之文石Leaf3C电纸书阅读器","url":"/new-toy-e-ink-reader-boox-leaf3c.html","content":"上周又给自己买了个玩具 —— 文石Leaf 3C电纸书阅读器。用了一周，感觉挺有意思，随手写一篇文章聊聊使用体验，顺便分享一些技巧。\n\n\n为什么要买它决定买电纸书阅读器主要有两个原因。之前我一直用iPad Pro看书，但这玩意又大又重，用起来不方便，拿在手里不舒服不说，通勤路上还得反复从包里掏出来再塞回去，实在是麻烦。实体书也买了不少，但是搬家后又懒得翻出来，可怜的书们就只是在箱子里吃灰。所以我总是想找一个小巧的可以随时看书的玩意。正好在同事的桌子上看到一个小巧的阅读器，就决定也搞一个。\n选择文石Leaf 3C也没啥特别的理由，就是这位同事用的Leaf 2，我拿起来大概试了下，尺寸和重量都挑不出啥毛病，加上不想花太多钱，于是就买了这个二手的Leaf 3C。\n初体验一拿到手，第一感觉就是，好轻！我终于不用再端着iPad看书了(T_T)！机身只有6毫米厚，重量也只有190克，粗略感受下来，比手上的iPhone 12 Pro大概要轻一半，甚至跟它的磁吸保护壳差不多重。轻薄纤细的机身拿起来就非常的顺手。\n前面板是一整块有点磨砂质感的玻璃，触感细腻，摩擦力也正正好，而且不会留下指纹。占据大部分面积的一块7英寸的Kaleido 3彩墨触屏，右边边缘有两个可以用来翻页的按键，正好适合我用右手拿着看书。如果你不喜欢用右手操作的话，系统层面也支持将整个屏幕旋转90度、180度、270度，来让你以一个你喜欢的姿势阅读。不过这个按键的手感并不是非常好。首先它会晃，其次，它并不是像手机的音量键那样是一个平整的按键，而是按键的中部下面压着一个贴片按钮开关，如果你按的位置很靠近边缘的话，就很有可能根本按不下去。\n\n阅读黑白文本的体验不用说。7英寸，300PPI的屏幕，我觉得完全够用。屏幕还自带前光，可以调节亮度和色温，就算睡前想要看会书也是没问题的，虽然可能对眼睛不太好……\n\n但要是你想拿它看彩色的漫画，那你可能就要失望了。这块屏幕只能展现4096种颜色（根据维基百科，相当于上世纪八九十年代的Amiga电脑），彩色表现非常的素，没办法展现鲜艳的色彩。实际的显示效果可以看下面的对比图。系统虽然自带了“均衡”和“鲜艳”两种色彩预设，也可以自定义，但恕我眼拙，没看出有什么明显的区别。\n\n\n\n显示\n截图\n\n\n\n\n\n\n\n它的内置阅读器也有个小bug。我目前的系统版本是4.0.2，在使用V3引擎阅读竖版书籍时，书中的插图无法正常显示。比如下面的对比图中，“一九二九年的大崩盘”和“印钞票”这两节之间是有一个插图的，但是以竖排显示时，阅读器无法正常显示插图。好在这个问题可以通过在阅读器内单独讲这本书转为横排来解决。我也向文石开发团队反馈了这个问题，希望将来会修复。\n\n\n\n竖排\n横排\n\n\n\n\n\n\n\n还有个小问题是，它会有点阴阳屏。如果仔细盯着它看，就能隐约看到远离按键的一侧要比靠近按键的一侧要稍微暗一点。但查过资料发现，这应该是电纸书的通病，因为前光是从侧面照明的，无法避免亮度不均匀的问题，只能自己慢慢习惯。\n软件上，它的操作系统是基于Android 11深度定制的，除了自带的阅读软件，你还可以安装其他的读书平台（甚至可以直接把Z-Library的客户端装上，就省的用电脑往里面拷书了）。考虑到电纸书的使用场景，系统提供了一个方便的入口来控制应用冻结，并且新安装的软件也可以自动冻结，这样就可以有效避免各种后台进程在你并不需要的时候运行，让续航更稳。实际用下来，续航不用担心，除非你有无可救药的续航焦虑。我每天开着前光，平均用它内置的阅读软件看两小时，一周下来大约会消耗70%电量。不过这是用内置阅读软件的情况，用第三方软件的话，耗电就明显会变多。\n买书虽然Z-Library上面可以免费下载海量的书籍，但我更愿意花钱购买正版来支持作者。另外我希望买完书之后能自己保存一份电子书的副本，并用任意我喜欢的工具阅读，所以像微信读书和京东书城等国内常用的电子书平台全部被我排除在外 —— 它们只让你在自家的app里阅读，并不能下载，太不自由了。\n而且比起订阅读书软件的会员，我也更喜欢买断制。因为我觉得，只有我买断这本书并且下载到这本书的副本，我才是真正拥有了这本书。而开会员更像是租了某个图书馆的入场资格，虽然整个图书馆的书我可以随便看，但我并不拥有任何一本，平台也可以随时换源甚至下架某本书，而我对此无能为力。更别提有的书只在A平台能看，而另一本书只能在B平台看的这种麻烦事。\n最后我折腾出来一套有点拧巴的买书流程：\n\n首先去Rakuten Kobo、HyRead、读墨等港台的电子书商城购买，然后通过Calibre的插件把DRM去掉，然后导入阅读器，用它自带的软件阅读。美国亚马逊的Kindle商城里面也可以买，但我还没试过去除Kindle格式电子书的DRM，也不确定阅读器是否支持Kindle的AZW格式。\n如果在上面这三个书店都找不到，那就去微信阅读里面买断这本书，然后再去Z-Library找高质量的epub。我不清楚这样下载到的书在严格意义上还算不算盗版，但至少我为它付费了，读起来也心安理得。\n如果Z-Library上都找不到高质量的副本，那就用第三方工具从微信读书中导出，再将下载到的HTML和资源文件放进Calibre，自己手动做成epub。这做起来就比较耗时耗力了，好在下载器是开源的，我可以修改它的代码为我完成最麻烦的部分。\n\n目前我的书都是在Rakuten Kobo上面买的，唯一的门槛就是，你要有一张VISA或者万事达卡组织的借记卡或信用卡，或者有一个PayPal账户。但是各家银行都有发行VISA和万事达的信用卡，中国区PayPal注册也很简单，所以也不算什么大问题。哦对，如果你买的书很贵的话，记得去网上搜Kobo的折扣码，我用过一个7折和一个8折的折扣，还是很爽的。便宜书就别用了，因为一个折扣码只能用一次，别浪费在便宜的书上。\n还有一点要注意，因为这些平台都位于香港或台湾，所以大部分书都是繁体中文。如果你平时不习惯看繁体字，那买之前最好先确认一下。\n使用Calibre去除DRM\n    ⚠️ ⚠️ ⚠️ 本节内容所述教程仅应用来方便自己阅读 ⚠️ ⚠️ ⚠️\n    ⚠️ ⚠️ ⚠️ 尊重知识产权 ⚠️ ⚠️ ⚠️\n    ⚠️ ⚠️ ⚠️ 不要制作和分发盗版书 ⚠️ ⚠️ ⚠️\n\n\n在Kobo购买的电子书都是有DRM保护的。虽然我们可以使用Adobe Digital Editions下载得到epub文件，但是这些文件都受Adobe DRM保护，拷进阅读器也无法打开。为了能脱离Kobo平台，我们就需要使用Calibre和DeDRM插件来去除DRM。\n首先到Calibre官网下载并安装Calibre。然后前往DeDRM_tools - GitHub Releases下载最新的DeDRM插件。将下载的zip文件解压缩，可以得到两个压缩包DeDRM_plugin.zip和Obok_plugin.zip。\n接下来打开Calibre，点击顶部工具栏的首选项按钮。\n首先进入导入/导出中的保存书籍到硬盘，取消勾选将非英语字符转换为对应英语字符，并点击应用，这样在导出时就可以保留书籍原本的文件名。\n然后在首选项窗口，找到高级选项中的插件并点击，接下来点击从文件加载插件，分别加载DeDRM_plugin.zip和Obok_plugin.zip。安装完成后勾选插件列表上面的仅显示用户自己安装的插件并搜索drm，如果搜到如下图一样的结果，就说明插件安装成功了。\n\n接下来前往Adobe Digital Editions下载页，下载Adobe Digital Editions (ADE)并安装。安装完成后打开ADE，点击帮助菜单中的授权计算机。如果你有Adobe账号的话，就在这里输入账号的邮箱和密码来登录；如果没有的话，就勾选窗口下部的我想要在不使用ID的情况下对我的计算机授权。然后点击授权按钮，等待授权完成。\n接下来，前往Kobo书城的我的账户 -&gt; 书籍，找到你已经购买的电子书，点击书名上方的三个点展开菜单，再点击菜单中的下载。这时候你会下载到一个名为URLLink.acsm的文件。把这个文件拖进ADE，然后ADE会下载这本书的EPUB文件，当然，这个文件是加密的。下载成功后，在ADE中右击刚刚下载的书，并点击在资源管理器中显示文件。\n然后，从刚刚打开的资源管理器窗口中，将你的书拖到Calibre的主界面中，这时候Calibre就会自动解除这本书的DRM并导入它的书库。然后在Calibre中右击这本书，点击菜单打开书籍文件夹 -&gt; 打开书籍文件夹，就能找到去掉了DRM的EPUB文件。然后你就可以把这个文件复制到阅读器中，使用它内置的阅读器阅读了。\n\n","categories":["瞎折腾"],"tags":["电纸书","阅读器","文石"]},{"title":"在MySQL中修改表和列的排序规则","url":"/mysql-convert-tables-and-columns-collation.html","content":"使用如下SQL语句即可更新一张表的字符集(character set)和排序规则(collation)：\n-- 此处假设使用utf8字符集，以及使用utf8_unicode_ci排序规则ALTER TABLE `table_name` CONVERT TO CHARACTER SET utf8 COLLATE utf8_unicode_ci;\n\n然后可以使用如下SQL查询表和列的字符集和排序规则是否修改成功：\n-- 查询表的信息SELECT `TABLE_SCHEMA`, `TABLE_NAME`, `TABLE_COLLATION`FROM `information_schema`.`TABLES`WHERE `TABLE_NAME` = &#x27;table_name&#x27;;-- 查询表中每个列的信息SELECT `TABLE_SCHEMA`, `TABLE_NAME`, `COLUMN_NAME`, `COLLATION_NAME`FROM `information_schema`.`COLUMNS`WHERE `TABLE_NAME` = &#x27;table_name&#x27;;\n","categories":["小技巧"],"tags":["MySQL","collation"]},{"title":"通过Hexo博客完成NIP-05验证","url":"/nostr-nip-05-with-hexo.html","content":"让Hexo在生成时包含.well-known目录编辑config.yml，添加如下配置：\ninclude:  - &quot;.well-known/**&quot;\n\n转换公钥格式Damus生成的npub开头的公钥不能直接用，要先转换成十六进制格式的公钥。可以用这个网站来转换。\n把npub开头的公钥粘贴进damus key一栏，hex key会自动计算出来，复制hex key的内容备用。\n创建文件在source目录下新建一个名为.well-known的目录，然后在.well-known目录中创建nostr.json，填写如下信息：\n&#123;    &quot;names&quot;: &#123;        &quot;你的用户名&quot;: &quot;刚刚转换得到的hex key&quot;    &#125;&#125;\n\n在本地运行hexo clean &amp;&amp; hexo g，生成成功后检查public目录中是否存在.well-known/nostr.json文件，如存在则说明配置正确。\n配置Nostr提交代码并部署成功后，即可在nostr客户端配置NIP-05认证，本文以Damus为例。\n点击左上角头像，进入Profile，点击Edit，在NIP-05 VERIFICATION一栏中填写你的username@你的博客网站，比如我填写的是boris1993@www.boris1993.com，最后点击Save。\n如果一切正常的话，在你的Profile页面就能看到一个对勾图标，代表验证成功。\n","categories":["瞎折腾"],"tags":["nostr","NIP-05","Damus"]},{"title":"让MyBatis把查询结果以Map形式返回","url":"/mybatis-return-query-results-in-a-map.html","content":"在使用MyBatis查询数据库时，偶尔会出现短时间高频查询的情况。或者，会被短时间高频率查询的数据需要通过另一个微服务的接口来获取。显而易见，高频率的查询数据库，或者高频率的调用微服务的接口，是会对性能产生一定影响的。\n在这种情况下，或许可以考虑将相关的数据一次性全部查询出来，然后根据某一列作为key，将其存放在一个Map中，把这个Map当成一个临时的缓存。\n\n\n示例数据假设有这样的一组数据，其中保存了某公司的一系列基地的信息，包括基地名称、基地代号，和基地所在的区。\n\n\n\nid\nbase_name\nbase_code\nlocation\n\n\n\n1\n梅陇\nML\n徐汇区\n\n\n2\n富锦路\nFJL\n宝山区\n\n\n3\n龙阳路\nLYL\n浦东新区\n\n\n4\n北翟路\nBDL\n普陀区\n\n\n5\n川沙\nCS\n浦东新区\n\n\n6\n石龙路\nSLL\n闵行区\n\n\n代码MyBatis查询的代码还是同往常一样，一个SELECT查询，返回类型就是这个表对应的实体类。\n&lt;select id=&quot;queryBaseByNameInMap&quot; resultType=&quot;com.boris1993.demoapp.entity.BaseDTO&quot;&gt;    SELECT        `id`,        `base_name`,        `base_code`,        `location`        FROM `base`        WHERE del_flag = &#x27;0&#x27;        &lt;if test=&quot;baseName != null and baseName != &#x27;&#x27;&quot;&gt;            AND `base_name` = #&#123;baseName&#125;        &lt;/if&gt;&lt;/select&gt;\n\n但是在Mapper接口中，需要在对应的方法上面加上@MapKey()注解。这个注解向MyBatis指明了使用实体的哪一个属性(而不是列名)作为Map的key。\n@MapKey(&quot;baseName&quot;)Map&lt;String, BaseDTO&gt; queryBaseByNameInMap(@Param(&quot;baseName&quot;) String baseName);\n\n执行结果编写service、controller等部分不是本文重点，按下不表。在不传递baseName时，就可以查出这样子的数据了：\n&#123;  &quot;北翟路&quot;: &#123;    &quot;id&quot;: &quot;4&quot;,    &quot;baseName&quot;: &quot;北翟路&quot;,    &quot;baseCode&quot;: &quot;BDL&quot;,    &quot;location&quot;: &quot;普陀区&quot;  &#125;,  &quot;龙阳路&quot;: &#123;    &quot;id&quot;: &quot;3&quot;,    &quot;baseName&quot;: &quot;龙阳路&quot;,    &quot;baseCode&quot;: &quot;LYL&quot;,    &quot;location&quot;: &quot;浦东新区&quot;  &#125;,  &quot;梅陇&quot;: &#123;    &quot;id&quot;: &quot;1&quot;,    &quot;baseName&quot;: &quot;梅陇&quot;,    &quot;baseCode&quot;: &quot;ML&quot;,    &quot;location&quot;: &quot;徐汇区&quot;  &#125;,  &quot;富锦路&quot;: &#123;    &quot;id&quot;: &quot;2&quot;,    &quot;baseName&quot;: &quot;富锦路&quot;,    &quot;baseCode&quot;: &quot;FJL&quot;,    &quot;location&quot;: &quot;宝山区&quot;  &#125;,  &quot;石龙路&quot;: &#123;    &quot;id&quot;: &quot;6&quot;,    &quot;baseName&quot;: &quot;石龙路&quot;,    &quot;baseCode&quot;: &quot;SLL&quot;,    &quot;location&quot;: &quot;闵行区&quot;  &#125;,  &quot;川沙&quot;: &#123;    &quot;id&quot;: &quot;5&quot;,    &quot;baseName&quot;: &quot;川沙&quot;,    &quot;baseCode&quot;: &quot;CS&quot;,    &quot;location&quot;: &quot;浦东新区&quot;  &#125;&#125;\n\n有什么用呢试想一下，在某个表中，只保存了相关基地的名字，但是在业务中又需要返回这个基地的代号等信息，那么就可以这么做了：\n// 首先，调用另一个微服务的接口，把这些基地的信息取出来Map&lt;String, BaseDTO&gt; bases = baseDataFeignClient.queryBaseByNameInMap();// 接下来，按照业务需要，查询出了一个List// 在这个List中，就包含了一系列的基地的名称List&lt;SomethingOutputVO&gt; aListOfSomething = fetchAListOfSomething();// 接下来，遍历这个List，逐个从bases中取得基地的代号for (SomethingOutputVO item : aListOfSomething) &#123;    item.setBaseCode(bases.get(item.getBaseName()).get(&quot;baseCode&quot;));&#125;\n\n此时，这个Map就变成了一个小的缓存，避免了在循环中反复查询数据库或者反复调用微服务接口的问题。\n","categories":["学知识"],"tags":["Java","MyBatis"]},{"title":"Git连接多个远程仓库","url":"/push-to-multi-git-repo.html","content":"有时候我们可能会需要push到多个远程仓库，比如同时链接多个代码托管平台的账号，那么可以参考本文所述的方法配置。\n保险起见在操作之前请先做好备份工作，毕竟数据无价。\n\n\n方法1 - 添加多个远程仓库比如要链接两个 Github 仓库，分别是 github1 和 github2，那么：\n# 添加 github1git remote add github1 https://github.com/username/github1.git# 添加 github2git remote add github2 https://github.com/username/github2.git# 提交到 github1git push github1 master# 提交到 github2git push github2 master# 从 github1 更新git pull github1 master# 从 github2 更新git pull github2 master\n\n方法2 - 添加同名多个远程仓库# 添加一个远程仓库git remote add origin https://github.com/username/github1.git# 然后分别设定push URLgit remote set-url --add --push origin https://github.com/username/github1.gitgit remote set-url --add --push origin https://github.com/username/github2.git# 检查远程仓库配置git remote -v# 若配置正确，则结果应当包含一个fetch路径和两个push路径# 向所有远程仓库推送git push origin master\n\n方法3 - 直接修改.git&#x2F;config文件用文本编辑器打开本地仓库的 .git&#x2F;config 文件，然后修改其中的远程仓库配置\n# 假设当前的远程仓库名为 origin[remote &quot;origin&quot;]    url = https://github.com/username/github1.git    fetch = +refs/heads/*:refs/remotes/github/*    pushurl = https://github.com/username/github1.git    pushurl = https://github.com/username/github2.git\n\n然后直接使用\ngit push origin master\n\n即可提交至所有版本库\n","categories":["小技巧"],"tags":["Git"]},{"title":"使用Docker配置Redis哨兵","url":"/redis-sentinel-in-docker.html","content":"接上文使用Docker配置Redis主从复制完成之后，这篇文章主要介绍如何使用Docker在本机搭建Redis的哨兵，内容包括涉及的目录结构、docker-compose.yml的编写。\n\n\n目录结构本文将采用如下的目录结构，其中data目录将用于存放各个容器的数据，server目录存放docker-compose.yml以及针对master和slave节点的配置文件，sentinel目录存放哨兵的配置文件和docker-compose.yml。\n.├── data│   ├── redis-master│   │   └── dump.rdb│   ├── redis-slave-1│   │   └── dump.rdb│   └── redis-slave-2│       └── dump.rdb├── sentinel│   ├── docker-compose.yml│   └── redis-sentinel.conf└── server    ├── docker-compose.yml    ├── redis-master.conf    └── redis-slave.conf\n\n配置哨兵节点配置示意图下面的示意图中，将采用Redis官方文档所使用的表示方法，M代表一个Master节点，R代表一个Replica节点，S代表一个Sentinel节点。\n从整个集群的稳定性角度考虑，首先Master节点和各个Replica节点不应同时处于同一台服务器上，以避免单台虚拟机或物理机不可用造成整个集群失效。\n       +----+       | M1 |       | S1 |       +----+          |+----+    |    +----+| R2 |----+----| R3 || S2 |         | S3 |+----+         +----+配置最少投票节点为2，即有2个哨兵节点投票选举出新的Master即可完成切换。\n\n在本示例中，我将使用如下的节点配置，各个节点运行在不同的Docker容器中，来模拟运行在不同服务器中的效果：\n+----+   +----+   +----+| M1 |   | R1 |   | R2 |+----+   +----+   +----+  |        |        |  +--------+--------+  |        |        |+----+   +----+   +----+| S1 |   | S2 |   | S3 |+----+   +----+   +----+\n\n编辑配置文件编辑redis-sentinel-1.conf，修改下列配置：\nbind 127.0.0.1# 哨兵的端口号# 因为各个哨兵节点会运行在单独的Docker容器中# 所以无需担心端口重复使用# 如果需要在单机port 26379# 配置哨兵的监控参数# 格式：sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;# master-name是为这个被监控的master起的名字# ip是被监控的master的IP或主机名。因为Docker容器之间可以使用容器名访问，所以这里写master节点的容器名# redis-port是被监控节点所监听的端口号# quorom设定了当几个哨兵判定这个节点失效后，才认为这个节点真的失效了sentinel monitor local-master 127.0.0.1 6379 2# 连接主节点的密码# 格式：sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass local-master redis# master在连续多长时间无法响应PING指令后，就会主观判定节点下线，默认是30秒# 格式：sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;sentinel down-after-milliseconds local-master 30000\n\n编辑redis-sentinel-2.conf和redis-sentinel-3.conf，分别修改监听端口号为26380和26381，其余部分不变。\n配置及启动容器编写docker-compose.yml这里继续使用docker-compose管理容器。\n---version: &#x27;3&#x27;services:  redis-sentinel-1:    image: redis    container_name: redis-sentinel-1    restart: always    # 为了规避Docker中端口映射可能带来的问题    # 这里选择使用host网络    network_mode: host    volumes:      - ./redis-sentinel-1.conf:/usr/local/etc/redis/redis-sentinel.conf    # 指定时区，保证容器内时间正确    environment:      TZ: &quot;Asia/Shanghai&quot;    sysctls:      net.core.somaxconn: &#x27;511&#x27;    command: [&quot;redis-sentinel&quot;, &quot;/usr/local/etc/redis/redis-sentinel.conf&quot;]  redis-sentinel-2:    image: redis    container_name: redis-sentinel-2    restart: always    network_mode: host    volumes:      - ./redis-sentinel-2.conf:/usr/local/etc/redis/redis-sentinel.conf    environment:      TZ: &quot;Asia/Shanghai&quot;    sysctls:      net.core.somaxconn: &#x27;511&#x27;    command: [&quot;redis-sentinel&quot;, &quot;/usr/local/etc/redis/redis-sentinel.conf&quot;]  redis-sentinel-3:    image: redis    container_name: redis-sentinel-3    restart: always    network_mode: host    volumes:      - ./redis-sentinel-3.conf:/usr/local/etc/redis/redis-sentinel.conf    environment:      TZ: &quot;Asia/Shanghai&quot;    sysctls:      net.core.somaxconn: &#x27;511&#x27;    command: [&quot;redis-sentinel&quot;, &quot;/usr/local/etc/redis/redis-sentinel.conf&quot;]\n\n启动容器这里同样使用docker-compose up -d启动容器，启动日志中可以看到哨兵开始监控Master节点，以及哨兵完成互相发现。\nredis-sentinel-2    | 1:X 11 Nov 2019 14:33:06.871 # +monitor master local-master 127.0.0.1 6379 quorum 2redis-sentinel-2    | 1:X 11 Nov 2019 14:33:08.996 * +sentinel sentinel 3dc4e0bff631b994a492d51e99a7ebc48e35a054 127.0.0.1 26381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:33:06.990 # +monitor master local-master 127.0.0.1 6379 quorum 2redis-sentinel-3    | 1:X 11 Nov 2019 14:33:07.001 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:33:07.010 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:33:08.876 * +sentinel sentinel 6f646433feb264b582ffa73b5d6bed6626b97966 127.0.0.1 26380 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:33:08.968 * +sentinel sentinel c3b07d8c4ac3686511e436e71043a615e9b1d420 127.0.0.1 26379 @ local-master 127.0.0.1 6379redis-sentinel-1    | 1:X 11 Nov 2019 14:33:06.948 # +monitor master local-master 127.0.0.1 6379 quorum 2redis-sentinel-1    | 1:X 11 Nov 2019 14:33:08.997 * +sentinel sentinel 3dc4e0bff631b994a492d51e99a7ebc48e35a054 127.0.0.1 26381 @ local-master 127.0.0.1 6379\n\n然后使用redis-cli连接到哨兵节点，连接成功后，可以使用info sentinel检查哨兵的信息。\n127.0.0.1:26379&gt; info sentinel# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0sentinel_simulate_failure_flags:0master0:name=local-master,status=ok,address=127.0.0.1:6379,slaves=2,sentinels=3\n\n其中，sentinel_masters:1说明这个哨兵在监控一个master，最后一行中写明了master0这个节点别名为local-master，状态为OK，地址是10.1.0.2:6379，有2个从节点，并有3个哨兵在监控。\n测试一下哨兵光是启动了还是不够的，还需要测试一下当被监控节点下线之后，哨兵是否能作出反应。\n我先停掉一个从节点，redis-server-slave-2，等了30秒后，三个哨兵主观认为redis-server-slave-2下线。\nredis-sentinel-2    | 1:X 11 Nov 2019 14:37:42.232 # +sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:37:42.290 # +sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-1    | 1:X 11 Nov 2019 14:37:42.291 # +sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379\n\n重新启动redis-server-slave-2后，三个哨兵节点都宣布不再主观认为该节点下线。\nredis-sentinel-1    | 1:X 11 Nov 2019 14:40:19.160 * +reboot slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-1    | 1:X 11 Nov 2019 14:40:19.243 # -sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-2    | 1:X 11 Nov 2019 14:40:19.403 * +reboot slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:40:19.161 * +reboot slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:40:19.242 # -sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-2    | 1:X 11 Nov 2019 14:40:19.502 # -sdown slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379\n\n这次我停掉主节点，并经过30秒后，哨兵输出了一大堆日志，不要紧，我们一边看一边解读：\nredis-sentinel-1    | 1:X 11 Nov 2019 14:44:11.639 # +sdown master local-master 127.0.0.1 6379redis-sentinel-2    | 1:X 11 Nov 2019 14:44:11.695 # +sdown master local-master 127.0.0.1 6379redis-sentinel-2    | 1:X 11 Nov 2019 14:44:11.752 # +new-epoch 1redis-sentinel-2    | 1:X 11 Nov 2019 14:44:11.755 # +vote-for-leader 3dc4e0bff631b994a492d51e99a7ebc48e35a054 1redis-sentinel-2    | 1:X 11 Nov 2019 14:44:11.758 # +odown master local-master 127.0.0.1 6379 #quorum 3/2redis-sentinel-2    | 1:X 11 Nov 2019 14:44:11.759 # Next failover delay: I will not start a failover before Mon Nov 11 14:50:11 2019redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.661 # +sdown master local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.746 # +odown master local-master 127.0.0.1 6379 #quorum 2/2redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.746 # +new-epoch 1redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.747 # +try-failover master local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.749 # +vote-for-leader 3dc4e0bff631b994a492d51e99a7ebc48e35a054 1redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.755 # c3b07d8c4ac3686511e436e71043a615e9b1d420 voted for 3dc4e0bff631b994a492d51e99a7ebc48e35a054 1redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.756 # 6f646433feb264b582ffa73b5d6bed6626b97966 voted for 3dc4e0bff631b994a492d51e99a7ebc48e35a054 1redis-sentinel-1    | 1:X 11 Nov 2019 14:44:11.753 # +new-epoch 1redis-sentinel-1    | 1:X 11 Nov 2019 14:44:11.754 # +vote-for-leader 3dc4e0bff631b994a492d51e99a7ebc48e35a054 1redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.826 # +elected-leader master local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.832 # +failover-state-select-slave master local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.894 # +selected-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.895 * +failover-state-send-slaveof-noone slave 127.0.0.1:6380 127.0.0.1 6380 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:11.971 * +failover-state-wait-promotion slave 127.0.0.1:6380 127.0.0.1 6380 @ local-master 127.0.0.1 6379redis-sentinel-1    | 1:X 11 Nov 2019 14:44:12.436 # +config-update-from sentinel 3dc4e0bff631b994a492d51e99a7ebc48e35a054 127.0.0.1 26381 @ local-master 127.0.0.1 6379redis-sentinel-1    | 1:X 11 Nov 2019 14:44:12.436 # +switch-master local-master 127.0.0.1 6379 127.0.0.1 6380redis-sentinel-1    | 1:X 11 Nov 2019 14:44:12.437 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6380redis-sentinel-1    | 1:X 11 Nov 2019 14:44:12.439 * +slave slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380redis-sentinel-2    | 1:X 11 Nov 2019 14:44:12.434 # +config-update-from sentinel 3dc4e0bff631b994a492d51e99a7ebc48e35a054 127.0.0.1 26381 @ local-master 127.0.0.1 6379redis-sentinel-2    | 1:X 11 Nov 2019 14:44:12.435 # +switch-master local-master 127.0.0.1 6379 127.0.0.1 6380redis-sentinel-2    | 1:X 11 Nov 2019 14:44:12.435 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6380redis-sentinel-2    | 1:X 11 Nov 2019 14:44:12.437 * +slave slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380redis-sentinel-3    | 1:X 11 Nov 2019 14:44:12.372 # +promoted-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:12.373 # +failover-state-reconf-slaves master local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:12.433 * +slave-reconf-sent slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:12.753 * +slave-reconf-inprog slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:12.920 # -odown master local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:13.825 * +slave-reconf-done slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:13.883 # +failover-end master local-master 127.0.0.1 6379redis-sentinel-3    | 1:X 11 Nov 2019 14:44:13.883 # +switch-master local-master 127.0.0.1 6379 127.0.0.1 6380redis-sentinel-3    | 1:X 11 Nov 2019 14:44:13.884 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ local-master 127.0.0.1 6380redis-sentinel-3    | 1:X 11 Nov 2019 14:44:13.885 * +slave slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380redis-sentinel-2    | 1:X 11 Nov 2019 14:44:42.446 # +sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380redis-sentinel-1    | 1:X 11 Nov 2019 14:44:42.465 # +sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380redis-sentinel-3    | 1:X 11 Nov 2019 14:44:43.887 # +sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380\n\n首先，三台哨兵都宣布Master节点主观下线。\n因为在配置文件中，我们指定了当最少2个哨兵认为Master节点失效后就会开始选举(就是quorom)，所以哨兵2提出选举新的Master节点。\n接下来，哨兵将开始投票，从Slave节点中选举出新的Master节点。在达成一致后，被选举的Slave节点将成为新的Master节点，其配置文件将会被改写，来让这个变动永久生效。\n然后，哨兵会通知这个集群的其他节点来加入新的Master，包括挂掉的那个之前的Master。\n这样就完成了一次failover切换。\n此时，如果重启之前的Master节点，哨兵会发现节点上线，并不再主观认为该节点下线。但是，现在这个节点已经变成了一个Slave节点。\nredis-sentinel-1    | 1:X 11 Nov 2019 14:56:32.936 # -sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380redis-sentinel-2    | 1:X 11 Nov 2019 14:56:33.202 # -sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380redis-sentinel-3    | 1:X 11 Nov 2019 14:56:33.707 # -sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ local-master 127.0.0.1 6380\n\n参考文档\nSentinel, Docker, NAT, and possible issues - Redis Sentinel Documentation\n\n系列博文\n使用Docker配置Redis主从复制\n使用Docker配置Redis哨兵\n\n","categories":["学知识"],"tags":["Redis"]},{"title":"记一次抢救被force push的GitHub仓库","url":"/rescuing-my-github-repo-from-force-push.html","content":"就在刚刚，我一个误操作，在没有本地备份的前提下，force push了一个GitHub上的仓库。万幸最后恢复成功，数据拿回来了。惊魂未定之余，在此记录我的抢救过程以供参考。\n\n\n前景提要在闲逛GitHub的时候，发现了一个叫snk的项目，可以在我的profile readme里面放个贪吃蛇，遂照着它的example with cron job抄了一个workflow过来。但是这里我自作聪明地想着把东西全放在master上，就把38行的target_branch改成了master。结果一运行吓一跳，我的README.md没了，仓库只剩下snk生成的svg文件。这不行啊，我花了好久的时间才整出来的东西，不能说没就没啊！于是赶紧开始网上冲浪，看怎么抢救被force push的repo。\n恢复过程首先我要大力感谢这个Gist，我就是参考这里的做法成功恢复了这个repo的。\n找到上一次commit的记录首先，要通过https://api.github.com/repos/:owner/:repo/events这个API找到上次提交的sha。\n-&gt; % curl -u boris1993 https://api.github.com/repos/boris1993/boris1993/eventsEnter host password for user &#x27;boris1993&#x27;:[  &#123;    &quot;id&quot;: &quot;24633558565&quot;,    &quot;type&quot;: &quot;PushEvent&quot;,    &quot;actor&quot;: &#123;      &quot;id&quot;: 4367313,      &quot;login&quot;: &quot;boris1993&quot;,      &quot;display_login&quot;: &quot;boris1993&quot;,      &quot;gravatar_id&quot;: &quot;&quot;,      &quot;url&quot;: &quot;https://api.github.com/users/boris1993&quot;,      &quot;avatar_url&quot;: &quot;https://avatars.githubusercontent.com/u/4367313?&quot;    &#125;,    &quot;repo&quot;: &#123;      &quot;id&quot;: 297097347,      &quot;name&quot;: &quot;boris1993/boris1993&quot;,      &quot;url&quot;: &quot;https://api.github.com/repos/boris1993/boris1993&quot;    &#125;,    &quot;payload&quot;: &#123;      &quot;push_id&quot;: 11349267173,      &quot;size&quot;: 1,      &quot;distinct_size&quot;: 1,      &quot;ref&quot;: &quot;refs/heads/master&quot;,      &quot;head&quot;: &quot;98364ce80ec5bbcdb6dc6f8d2239de2256ede487&quot;,      &quot;before&quot;: &quot;32276fc643c6b34fee48f46363cfb6a44327cbe4&quot;,      &quot;commits&quot;: [        &#123;          &quot;sha&quot;: &quot;98364ce80ec5bbcdb6dc6f8d2239de2256ede487&quot;,          &quot;author&quot;: &#123;            &quot;email&quot;: &quot;41898282+github-actions[bot]@users.noreply.github.com&quot;,            &quot;name&quot;: &quot;github-actions[bot]&quot;          &#125;,          &quot;message&quot;: &quot;Deploy to GitHub pages&quot;,          &quot;distinct&quot;: true,          &quot;url&quot;: &quot;https://api.github.com/repos/boris1993/boris1993/commits/98364ce80ec5bbcdb6dc6f8d2239de2256ede487&quot;        &#125;      ]    &#125;,    &quot;public&quot;: true,    &quot;created_at&quot;: &quot;2022-10-17T05:44:11Z&quot;  &#125;,  &#123;    &quot;id&quot;: &quot;24633554175&quot;,    &quot;type&quot;: &quot;PushEvent&quot;,    &quot;actor&quot;: &#123;      &quot;id&quot;: 4367313,      &quot;login&quot;: &quot;boris1993&quot;,      &quot;display_login&quot;: &quot;boris1993&quot;,      &quot;gravatar_id&quot;: &quot;&quot;,      &quot;url&quot;: &quot;https://api.github.com/users/boris1993&quot;,      &quot;avatar_url&quot;: &quot;https://avatars.githubusercontent.com/u/4367313?&quot;    &#125;,    &quot;repo&quot;: &#123;      &quot;id&quot;: 297097347,      &quot;name&quot;: &quot;boris1993/boris1993&quot;,      &quot;url&quot;: &quot;https://api.github.com/repos/boris1993/boris1993&quot;    &#125;,    &quot;payload&quot;: &#123;      &quot;push_id&quot;: 11349264873,      &quot;size&quot;: 1,      &quot;distinct_size&quot;: 1,      &quot;ref&quot;: &quot;refs/heads/master&quot;,      &quot;head&quot;: &quot;32276fc643c6b34fee48f46363cfb6a44327cbe4&quot;,      &quot;before&quot;: &quot;b0ab0263c0693122ae8069c95526e13b7336483f&quot;,      &quot;commits&quot;: [        &#123;          &quot;sha&quot;: &quot;32276fc643c6b34fee48f46363cfb6a44327cbe4&quot;,          &quot;author&quot;: &#123;            &quot;email&quot;: &quot;boris1993@live.cn&quot;,            &quot;name&quot;: &quot;Boris Zhao&quot;          &#125;,          &quot;message&quot;: &quot;Create generate_snake_animation.yml&quot;,          &quot;distinct&quot;: true,          &quot;url&quot;: &quot;https://api.github.com/repos/boris1993/boris1993/commits/32276fc643c6b34fee48f46363cfb6a44327cbe4&quot;        &#125;      ]    &#125;,    &quot;public&quot;: true,    &quot;created_at&quot;: &quot;2022-10-17T05:43:50Z&quot;  &#125;,  &#123;    &quot;id&quot;: &quot;23024411036&quot;,    &quot;type&quot;: &quot;PushEvent&quot;,    &quot;actor&quot;: &#123;      &quot;id&quot;: 4367313,      &quot;login&quot;: &quot;boris1993&quot;,      &quot;display_login&quot;: &quot;boris1993&quot;,      &quot;gravatar_id&quot;: &quot;&quot;,      &quot;url&quot;: &quot;https://api.github.com/users/boris1993&quot;,      &quot;avatar_url&quot;: &quot;https://avatars.githubusercontent.com/u/4367313?&quot;    &#125;,    &quot;repo&quot;: &#123;      &quot;id&quot;: 297097347,      &quot;name&quot;: &quot;boris1993/boris1993&quot;,      &quot;url&quot;: &quot;https://api.github.com/repos/boris1993/boris1993&quot;    &#125;,    &quot;payload&quot;: &#123;      &quot;push_id&quot;: 10514884570,      &quot;size&quot;: 1,      &quot;distinct_size&quot;: 1,      &quot;ref&quot;: &quot;refs/heads/master&quot;,      &quot;head&quot;: &quot;b0ab0263c0693122ae8069c95526e13b7336483f&quot;,      &quot;before&quot;: &quot;660e4d3896eb523d703464f89112a2eea07ee309&quot;,      &quot;commits&quot;: [        &#123;          &quot;sha&quot;: &quot;b0ab0263c0693122ae8069c95526e13b7336483f&quot;,          &quot;author&quot;: &#123;            &quot;email&quot;: &quot;boris1993@live.cn&quot;,            &quot;name&quot;: &quot;Boris Zhao&quot;          &#125;,          &quot;message&quot;: &quot;Update README.md&quot;,          &quot;distinct&quot;: true,          &quot;url&quot;: &quot;https://api.github.com/repos/boris1993/boris1993/commits/b0ab0263c0693122ae8069c95526e13b7336483f&quot;        &#125;      ]    &#125;,    &quot;public&quot;: true,    &quot;created_at&quot;: &quot;2022-07-22T08:05:06Z&quot;  &#125;]\n\n从接口的返回可以看到，截至现在一共有三次push，仓库被覆盖就是发生在最上面的一次push中，而接下来的一个就是我添加workflow的那一次push。\n从上一次的push记录中找回数据找到push记录，那么就好办了。点击添加workflow那次push中的url，在浏览器中会打开一个新页面，返回的JSON中是这次push的详细信息。这里我们要找的是html_url这个字段。点开这个字段里面的链接，会进入GitHub里面，看到这次commit的diff。然后点击Browse files按钮，就能看到当时的文件了。这时候还等什么？赶紧下载啊！文件少的话复制内容回来就行，文件多的话，点开Code按钮，Download ZIP就好啦。\n\n后记一定要做备份啊！\n","categories":["瞎折腾"],"tags":["GitHub","git","force push","恢复GitHub仓库"]},{"title":"我的新冠感染纪实","url":"/recording-my-infection-of-corona.html","content":"我，很不幸，于2022-12-23夜，新冠奥密克戎发作。不过，最近记录自己被感染的生活好像是个流量密码，那我干脆也蹭一波热点，记录一下我感染之后的感受。\n\n\n病发说来，一开始有征兆还是在22日下午，我从医院开完药出来之后，就偶尔会咳两声，但是嗓子里的感觉又不像是感染，更像是吸入了什么灰尘，喉咙那有点麻麻沙沙的异物感，所以就没放在心上。\n正经开始感觉不对劲，是在12月23日的下午。本来这天阳光不是很足，上海又降温了，好死不死我还没开空调，屋里本来就有点冷。我慢慢开始感觉有点浑身发凉，但是没想到是发病，以为只是单纯的着凉而已，于是打开了空调，期盼能舒服一点。\n但是慢慢的事情开始朝不对劲的方向发展，晚上我开始感觉脑袋会忽然飘一下，后背也开始有一种奇怪但熟悉的痛感，我不知道该怎么描述，就是那种你摸上去就会有点钝痛的那种感觉。我发觉这事不是那么简单，我估计是发烧了，但这时候我依旧没往新冠感染上想，以为就是个普通感冒。但是苦于我的温度计找不到了，不量体温又不是个办法，遂硬着头皮，走到小区对面的药房，借了药房的温度计测了一下。结果不出预料，37度5。因为药房的温度计也卖完了，于是在咨询了一下怎么吃药之后，就回去了。\n说真的，家里必须得时常检查下温度计还能不能找见，发烧的时候顶着寒冷出门绝不是个好主意。我在走路的时候，时不时就会晕一下，过马路左右扭头的时候甚至耳朵嗡的一下，虽然神志清楚，但是总感觉险些要晕过去。也就算我命大，硬是慢慢的走回来了。\n书归正传，有时候吧，这事还真就是无巧不成书，正好我在22日那天要去医院开我的焦虑症的药，就顺路在药房买了些能对应新冠感染的药，虽然不是布洛芬和对乙酰氨基酚这种值钱玩意，但至少退烧、止痛、镇咳、缓解咽痛的药都拿到了。只是万万没想到第二天就能派上用场…..TAT。\n得了医嘱，回来开始吃退烧药。药名叫酚氨咖敏片，说主治感冒、发烧、头痛，里面包含氨基比林、对乙酰氨基酚、咖啡因、马来酸氯苯那敏。吃了药，想到没有体温计还是不行，于是打开各种外卖平台，想看看附近药房有没有能外卖过来的。要不说咱命好呢，正好交大附近的一个药房还有额温枪，不容多想，手起键落下了一单。一看预计00:23送达，我还挺美，哪知道临到凌晨的时候美团说已经送达？？？我一问骑手，才知道他们不肯单独为这一个玩意跑这么远，只能等第二天中午再送。行吧，没盼头了，一摸电热毯烧的很热，空调也够劲，睡。\n次日一晚上觉也没睡太好，醒了有三四次，期间还做了不少奇奇怪怪的梦，到了早上10点来钟，迷迷糊糊的醒了，但是感觉非常赖床，左滚右滚就是不想起，还感觉身上似乎出了很多的汗。翻腾了大概半小时，终于鼓起勇气，爬出被窝起床。一摸后背，果然湿漉漉的，出了不少的汗。起来刷过牙洗过脸，开始吃药。过了会看到我这还有不少抗原试剂盒，是中旬上海封城的时候发的，于是就拿了一个测了一下，一开始只看见C有杠杠，T没有，还庆幸不是新冠，但是15分钟后，T这里开始出现浅浅的一道。果然，还是没躲掉。\n\n既然中了，那果然还是得搞点更对症的药。问了之前感染过的兄弟，都说布洛芬好使，但是现在药房肯定是买不到了。好在微信上线了一个互助小程序，我成功联系到了两个人，一位给我送了4片对乙酰氨基酚，另一位网购了100来片布洛芬，我正在等他到货。中午吃过饭之后，跑腿小哥也把药送到了我这里。\n\n之前一直不相信自己是新冠感染的另一个原因就是，很多人都说他们感染之后丧失了嗅觉和味觉，但是我味觉丝毫不受任何影响，吃什么都很有味，包括头一天晚上吃的柚子，和这一天吃的外卖，味觉都很正常，没有发生味觉丧失和味觉改变。或许这个症状也是因人而异吧。至于嗅觉，我是个过敏性鼻炎患者，嗅觉不好使不是一年两年了，所以就算没了我也不知道……\n下午也没心情搞什么，躺在床上无所事事，大概三点来钟睡着了，一觉干到6点半。醒来没一会接到家里的电话，说已经在来上海的飞机上了。我是感觉又安心又担心，安心当然是因为父母能来照顾我，但是我又非常害怕传染给他们，不过既然来了，也劝不回去，只能车到山前必有路了吧。吃过晚饭，一边写这篇流水账，一边等父母过来。\n对了，还没说我买的额温枪呢。实话说我一开始特别怀疑这玩意的准确度，因为中午拿到之后我就测了好几次，那会身体明明感觉到还有一点烧，但是一直都只能测出来36度5、36度8。直到刚才(20:54)，我明显感觉到体温又上来之后，才测出来个37度6，而且这还夹杂在一堆37度整、36度8之间。果然还是得整个水银的才靠谱啊。\n\n第三天前一天晚上还是有点低烧，看到网上建议忍一忍再吃退烧药，让体温帮助杀灭病毒，所以睡前也没喝布洛芬，直接钻进烧热了电热毯的被窝睡了。一晚上还是各种奇怪的梦。可早上一醒，嘿，你猜怎么着？我不烧了！然而嗓子比前一天加倍的疼，咳嗽好像也略微多了点，还带着白痰。莫非病毒已经被打的溃不成军，妄图用加重咳嗽的方法往外大量散播么？如果屋里只有我一个人的话，那我才懒得管，你随便爱往哪传播就往哪传播去，但是现在我父母也来了，那我的评价就是\n\n继续说症状，今天后背还是有类似发烧引发的那种痛感，隐隐还有点头疼，而且偶尔脑袋还是会恍惚一下，大概是发烧导致的问题还没好利索。\n中午吃过饭，也没什么精神头干别的，直接从大概一两点睡到了将近6点，起来之后，因为头疼搞得很恶心，吐了一大口。好在没有影响食欲，晚饭依旧吃得很香。饭后也没啥精神，待了会就睡了。\n第四天第四天一整天的主旨就是一个字：睡。\n头天晚上睡觉的时候，一晚上醒了几次，断断续续的睡到了大概十点来钟，起来也没什么精神，什么都不想干，只想趴着，只想睡觉。但也有值得高兴的事，母上大人做的素面一如既往的好吃。\n下午断断续续睡了好几觉，睡醒了就还是头疼。喝了口热水，感觉有点恶心，暗觉不妙，但是也没多想，就想接着睡。但是刚躺下没一会，突然胃就一抽给呕出来一口。说时迟那时快，我强忍着咬紧牙关，光速戴上口罩冲进厕所，哇哇的把中午饭和晚饭都吐干净了。饭吐了不说，还吐了好几口鲜血，场面甚是恐怖。我害怕是肺出血，还特意咳了口痰，还好还好痰里没血。但又想到最近吃的药有点杂，开始担心是不是肝坏了，遂在支付宝上找了个在线问诊，大夫听完我描述说，不像是肝损伤，建议我把药都停了，饭不要吃太烫的，观察一下看看，如果不再吐就没事了。还好还好，不需要去ICU。完事喝了点水，待了会就准备继续睡觉。可是白天睡了太多，晚上断断续续的差不多两点钟才正经睡着。\n为啥我突然开始担心肝呢？因为这几天确实药吃的有点杂，有布洛芬，有胆木浸膏糖浆，还有父母从老家带来的“新冠治疗方”中药，我害怕里面有什么成分重叠，给肝脏上了太大压力，就像那些因为害怕而过量服药的人一样。\n\n不过既然大夫都说把不必要的药都停了，那就遵医嘱呗，反正新冠这玩意最后还是得靠自己，不烧也不死命的咳，那也就别瞎吃药了。\n忘了是中午还是下午，顺便又做了个自测，这次变成强阳性了，T这一杠红的甚是晃眼。\n\n这两天也算是目前为止精神最差的两天，什么都不想干，要是味觉再没了的话，我可能连饭都不想吃。\n第五天今天早上的主题是：疼！\n第四天开始，我的左边嗓子就已经开始疼起来了，但是今天我没想到竟能如此的疼！什么吞刀片感，明明是电钻打眼感好吗？而且正好是左边打穿了，正在开始往右边打！\n不过也有好消息，就是咳嗽更少了，而且也确实没有再吐血，甚至没有恶心的感觉。精神状态也比前两天好了不少。前两天不止是什么都不想干，听到微信的通知声音都烦躁的很。\n而且今天开始，咳嗽的次数又变少了，一天满打满算好像也就不到10次的样子。感觉应该是快要康复了？\n早饭吃了个黑麦片配小麦胚芽，这还是我在六七月份上海解封之后买的。然后遵医嘱吃了颗奥美拉唑肠溶胶囊养养胃。吃完早饭无所事事，瞎翻手机的时候，看到小区群有人在卖体温计，正好我缺个体温计，只有个感觉并不是那么准的额温枪。没时间多想，果断拿下。呐，就是这玩意：\n\n中午吃的是素面和金枪鱼罐头。饭后虽然说有了点精神，但还是什么都不想干，那不用说，睡！\n睡到大概四五点的时候，隐约感觉有点冷，以为是下午睡觉没开空调，屋里降温的原因。想到有了体温计，就测一下吧。可这一测吓我一跳，37度5，这是又烧上了？但是感觉又不像，嘴里哈气也没有之前发烧那种哈热气的感觉，摸脑门也不觉得热，让父母摸了一下，好像也没啥大差别，只好不管了，再观察一下。\n网上瞎冲浪的时候，看见这么一张图，又想到我晚上睡大床，父母却只能在沙发和帆布床上凑合，瞬间泪如雨下。\n\n晚饭吃的小米粥和香菇炒油菜，吃过饭之后感觉头疼的厉害，那正好吃片扑热息痛，又退烧又缓解头痛。有一说一我真的觉得扑热息痛比布洛芬好使欸，见效又快又明显。刚刚(20:52)又测了下体温，37度1，果然是真的又烧起来了，不过拜扑热息痛所赐，并不觉得很难受，甚至有精神继续更新这篇纪实。\n一边写文，一边喝椰子水，美得很！\n\n如果说大量喝白开水会感觉不舒服或者恶心的话，确实喝果汁是个不错的选择，写文的功夫，我大概已经连着干下去三分之二盒了。确实比喝白开水舒服。\n说话的功夫，又到了21:00，烧上电热毯，休息一会，准备睡觉。\n第六天 ~ 第八天这三天变化不是很多，想想干脆合起来写好了。\n从第六天开始突然感觉症状明显减轻，第六天仍稍微有一点类似发烧的头晕感，但是第七天往后就几乎没有这种感觉，精力也恢复不少，能专心做事或者动脑了。会想起前几天，“脑雾”比较严重的一次，是在厕所蹲完坑扯了张纸准备擦，但扯下来纸后我鬼使神差开始擦手，擦了两下感觉有点不对，顿了顿才想起来是要擦屁股……\n虽然说病情是在好转，但是最近这两天的抗原结果我属实没看明白。第六天测的时候是弱阳性，第七天突然强阳，缓冲液刚滴进去，T那条就红了，然后第八天呢，又变成非常弱的阳性，缓冲液滴进去15分钟之后都得仔细盯才能看到及其不明显的一条杠。难道说，病毒也在跟我玩回光返照么？\n\n\n\n第六天\n第七天\n第八天\n\n\n\n\n\n\n\n\n咳嗽倒还是那样，每天还是有十来次咳痰，后面还有一点干咳。看网上的说法，这算是长新冠症状的一种？\n然后就是最恶心的症状 – 嗓子吞刀片，这几天就还是该怎么疼就怎么疼，一天下来可能某个时间段稍微好点，但过一会好像又加重。拿手机给自己的嗓子眼拍了张特写，发现咽部有网状白色，而且扁桃腺也肿的厉害，盲猜是诱发咽炎和扁桃腺炎了。照片……我想想还是不贴出来了吧，血盆大口也不是那么好看。\n第九天 ~ 第十二天第九天往后开始没啥大变化，依旧是偶尔咳痰，以及嗓子干痛。问了药房大夫，说吃点消炎药控制咽痛就行，正好我有点蒲地兰消炎片，照着说明书就开始吃。第十一天早上醒来，突然就发现嗓子不疼了，我也懒得再看嗓子眼里面是啥样，盲猜咽炎和扁桃肿大都应该是没了。咳痰依旧偶尔有一点，打算等有时间了去医院再做个检查，顺便照个肺部CT，看看肺子有没有受影响。\n第十一天又测了个抗原，已经完全看不到T的那条杠，看来新冠病毒是已经没有了，剩下的症状大概就是所谓的长新冠？\n\n而且这几天虽然精力恢复了不少，但还是很困，早上不想起，迷迷糊糊能一直睡到中午，吃过午饭就眼皮发沉又想睡觉。\n其他查到的信息\n布洛芬和对乙酰氨基酚 你可能不知道的事\n\n","categories":["生活记录"],"tags":["新冠肺炎","奥密克戎","Omicron"]},{"title":"记一次修复生产环境中nginx出现的SNI相关的问题","url":"/resolving-nginx-sni-issue.html","content":"前段时间，我司出现了一次生产事故，调查后发现是当时的OpenResty配置不兼容SNI导致的。在这里我也记录一下整件事的排查过程，以及解决方法，供遇到类似问题的同志们参考。\n\n\n事故症状某天开始，我司的OpenResty日志中大量出现SSL握手失败的错误，并影响了正常的业务。查看OpenResty日志，看到有大量这样子的报错：\n2021/10/19 20:51:30 [warn] 16776#16776: *1110324 upstream server temporarily disabled while SSL handshaking to upstream, client: [MASKED], server: localhost, request: &quot;GET /endpoint/to/the/api?query=param HTTP/1.1&quot;, upstream: &quot;https://MASKED:443/endpoint/to/the/api?query=param&quot;, host: &quot;MASKED&quot;2021/10/19 20:51:30 [error] 16776#16776: *1110324 SSL_do_handshake() failed (SSL: error:14094438:SSL routines:ssl3_read_bytes:tlsv1 alert internal error:SSL alert number 80) while SSL handshaking to upstream, client: MASKED, server: localhost, request: &quot;GET /endpoint/to/the/api?query=param HTTP/1.1&quot;, upstream: &quot;https://MASKED:443/endpoint/to/the/api?query=param&quot;, host: &quot;MASKED&quot;\n\n调查中的弯路知道了是SSL握手失败导致的问题，那么当然接下来就开始调查为什么会握手失败。是解析配置出错？还是证书出现问题？\n按照一直的经验，我决定先用nslookup检查一下DNS解析。因为保密和时间问题，我就不把nslookup的输出放在这里了。简而言之，再出现问题之前，我司的域名都是通过CNAME记录解析到Akamai的Edge节点上的，但现在，却直接用A记录解析到了一个IP上，这让我感觉很奇怪。同时，我为了确认，又用openssl命令连接了一下解析出来的IP，看它会返回什么证书信息，可出乎意料，啥也没有。\n我感觉不对劲，于是联系了NetOps组。这时候，NetOps点出了这篇博文的主题，SNI。\n他说，给openssl命令加一个-servername参数，把目标服务器的域名放上去。我一试，好使了，Akamai返回了正确的证书信息。\n那么问题来了，解析没问题，那就是我有问题了。但问题出在哪呢？\nSNI是什么在继续之前，我想先简单介绍一下，什么是SNI。\n根据维基百科词条服务器名称指示中的说法：\n\n基于名称的虚拟主机允许多个DNS主机名由同一IP地址上的单个服务器（通常为Web服务器）托管。为了实现这一点，服务器使用客户端提供的主机名作为协议的一部分（对于HTTP，名称显示在主机头中）。但是，当使用HTTPS时，TLS握手发生在服务器看到任何HTTP头之前。因此，服务器不可能使用HTTP主机头中的信息来决定呈现哪个证书，并且因此只有由同一证书覆盖的名称才能由同一IP地址提供。……实际上，这意味着对于安全浏览来说，HTTPS服务器只能是每个IP地址服务一个域名（或一组域名）。为每个站点分配单独的IP地址会增加托管成本，因为对IP地址的请求必须为区域互联网注册机构提供证据而且现在IPv4地址已用尽。……客户端在SNI扩展中发送要连接的主机名称，作为TLS协商的一部分。这使服务器能够提前选择正确的主机名称，并向浏览器提供相应TLS证书。从而，具有单个IP地址的服务器可以在获取公共证书不现实的情况下提供一组域名的TLS连接。\n\n也就是说，在握手的时候，我需要预先提供我要访问的网站的域名，这样服务器才会把正确的证书返回给我。而上面说的openssl命令中的-servername参数就是做了这件事。\n无心插柳，柳暗花明就在我拿着各种关键词Google的时候，一篇文章引起了我的注意。\n文章里描述的问题也是在OpenResty中出现了SSL握手失败，同样作者也在proxy_pass中用了upstream。作者做了一个测试，如果在proxy_pass中直接写上游的域名，就没有问题，但是一旦用upstream，就会出现握手失败。那么问题一定出现在upstream导致的某种行为变化。\n然后作者发现，在用域名的时候，OpenResty的变量$proxy_host存放的就是域名，可在用upstream的时候，这里面就变成了那个upstream的名字。\n看到这，我知道了，这应该就是我这个问题的解决方案。\n动手解决问题首先我先展示一下修复前的OpenResty的一部分配置：\nlocation /path/to/endpoint &#123;    include /etc/nginx/conf.d/proxy.common;    proxy_set_header Host api.$DOMAIN_FOR_GCP.com;    proxy_pass https://gcp-https/path/to/the/endpoint/on/server;&#125;\n\n可见，如果按照这个配置，那么我发给Akamai的域名就是gcp-https，而不是正确的api.mycompany.com。\n所以，根据那篇文章，以及参照OpenResty的手册，我在配置中增加了一条proxy_ssl_name指令，并将其配置为实际的后端服务的域名。\nlocation /path/to/endpoint &#123;    include /etc/nginx/conf.d/proxy.common;    proxy_set_header Host api.$DOMAIN_FOR_GCP.com;    # THIS ONE    proxy_ssl_name api.$DOMAIN_FOR_GCP.com;    proxy_pass https://gcp-https/path/to/the/endpoint/on/server;&#125;\n\n可部署新配置之后，问题并未解决，SSL握手失败的问题依旧存在。\n然后我注意到，那篇文章中还出现了一个指令proxy_ssl_server_name on;。莫非，我们的OpenResty里面干脆没启用SNI支持？\n在终端里dump了一下当前的配置，果然没有显式指定proxy_ssl_server_name的值，而默认情况下这个是被关闭的。那好办，我在OpenResty的全局配置中把它打开就好了。\n然后再次部署测试，发现再没有SSL握手失败的问题，测试环境中业务也恢复了正常。火速打包上线生产环境，事故解决。\n","categories":["学知识"],"tags":["SNI","nginx","OpenResty"]},{"title":"RouterOS配置Wireguard与阿里云ECS组网","url":"/routeros-setup-wireguard.html","content":"前两天赶上阿里云搞活动，薅了一个99一年的ECS。因为要接进咱的探针，就得跟我的服务器组个网。之前都是用NAS上装的wgeasy，这回想想，咱的RouterOS也有Wireguard，那干脆吃个螃蟹，看看能不能直接在软路由上组，慢慢的逐步把wgeasy淘汰掉，顺便正经的学学Wireguard组网。\n\n\n网络规划我的LAN网段：192.168.1.0/24Wireguard网段：10.10.0.0/24分配给ECS的Wireguard地址：10.10.0.100/32\nRouterOS端的准备首先添加一个Wireguard接口：\n/interface/wireguard/add disabled=no listen-port=13231 mtu=1420 name=wireguard1\n\n然后添加防火墙规则，允许13231/udp端口：\n# in-interface更换成你的WAN接口的名字/ip/firewall/filter add chain=input in-interface=&quot;China Telecom&quot; dst-port=13231 protocol=udp action=accept comment=&quot;Allow Wireguard&quot;\n\n然后为Wireguard分配一个网段：\n/ip/address add address=10.10.0.0/24 network=10.10.0.0 interface=wireguard1 comment=Wireguard\n\n接下来配置转发规则，允许LAN和Wireguard网段互通：\n/ip/firewall/filter add chain=forward src-address=192.168.1.0/24 dst-address=10.10.0.0/24 action=accept comment=&quot;Forward Wireguard to LAN&quot;/ip/firewall/filter add chain=forward src-address=10.10.0.0/24 dst-address=192.168.1.0/24 action=accept comment=&quot;Forward LAN to Wireguard&quot;\n\n至此，RouterOS上的Wireguard接口配置完毕。接下来需要取得RouterOS端Wireguard的公钥备用。\n# 记下输出中的 public-key 备用&gt; /interface/wireguard/printFlags: X - disabled; R - running  0  R name=&quot;wireguard1&quot; mtu=1420 listen-port=13231 private-key=&quot;routeros-private-key&quot; public-key=&quot;routeros-public-key&quot; \n\nECS端的准备接下来登陆到ECS，安装Wireguard。\n# 安装Wireguardsudo apt install wireguard-tools# 生成密钥对mkdir wireguard &amp;&amp; cd wireguardwg genkey | tee privatekey | wg pubkey &gt; publickey# 查看公钥和私钥，备用cat privatekeycat publickey\n\n开始组网先回到RouterOS，为Wireguard添加一个Peer：\n# allowed-address为我们要分配给这个节点的地址# public-key要填写上面在ECS上得到的publickey# preshared-key相当于密码，用来保护这个peer不会被陌生的节点连接# comment按需填写，用来标识这个节点/interface/wireguard/peers add allowed-address=10.10.0.100/32 interface=wireguard1 public-key=&quot;ECS-PUBLIC-KEY&quot; preshared-key=&quot;the-preshared-key&quot; comment=ECS-SH-1\n\n接下来到ECS，在/etc/wireguard目录下创建配置文件wg0.conf，填写如下内容：\n[Interface]PrivateKey = &lt;填写上面生成的私钥&gt;Address = 10.10.0.100/32DNS = 119.29.29.29[Peer]PublicKey = &lt;填写RouterOS的公钥&gt;PresharedKey = &lt;填写在RouterOS中Peer的preshared-key&gt;AllowedIPs = 192.168.1.0/24PersistentKeepalive = 0Endpoint = &lt;你的公网IP或DDNS域名&gt;:13231\n\n保存后就可以启动wg0这个接口并测试组网是否成功。\n# 启动wg0$ sudo systemctl start wg-quick@wg0# ping一个内网地址看有没有响应$ ping 192.168.1.20PING 192.168.1.20 (192.168.1.20) 56(84) bytes of data.64 bytes from 192.168.1.20: icmp_seq=1 ttl=63 time=6.07 ms64 bytes from 192.168.1.20: icmp_seq=2 ttl=63 time=4.72 ms64 bytes from 192.168.1.20: icmp_seq=3 ttl=63 time=5.25 ms64 bytes from 192.168.1.20: icmp_seq=4 ttl=63 time=6.07 ms\n\n如果能从ECS上ping通LAN侧的主机，那就说明组网成功了。接下来可以让wg0接口在开机后自动启动：\nsudo systemctl enable wg-quick@wg0\n\n故障恢复文章发布之后没两天，我的软路由突然内核崩溃重启，导致我的公网IPv4地址变了，而Wireguard在建立连接后，并不会检查对端的地址是否有变化，一旦变化，就会导致Wireguard隧道不通。于是我写了一段小脚本放在crontab里，每分钟ping一次对端的Wireguard接口地址，如果不通，就重启Wireguard服务。\n#!/bin/bash# Ping对端Wireguard地址，仅ping一次，超时1秒ping 10.10.0.1 -qc 1 -w 1 &gt; /dev/null# 如果ping不成功，那么返回码是1RESULT=$?# 只要返回码不是0，就重启Wireguard连接if [ $RESULT -ne 0 ]; then  systemctl restart wg-quick@wg0fi\n","categories":["瞎折腾"],"tags":["RouterOS","Wireguard"]},{"title":"在RouterOS中配置MSS Clamping解决部分网站图片无法加载的问题","url":"/routeros-mss-clamping.html","content":"换成自己用RouterOS拨号之后，经常发现有的图片加载不出来，网上一顿冲浪之后发现可能是在RouterOS中没有正确配置MTU及MSS，导致部分包被丢弃，也就是传说中的PMTU黑洞。\n\n\n症状最常见的就是微信公众号的图片始终加载不出来，就像这样（自己当时没截图，借用知乎文章的图）：\n\n说来也很奇怪，我在iOS的微信上是能正常加载的，但是换到我妈的Android微信就会出现这样的情况，之前一直以为是我妈的手机出毛病了……\nPMTU黑洞所谓MTU，指的是一条链路上可以通过的三层数据包的最大尺寸（包含IP包头）。以太网默认的MTU是1500字节。但是从我的设备到目标服务器之间的路径上可能存在MTU小于1500的链路，那么这条路径上最小的MTU，就是整条链路的Path MTU（PMTU）。\n路由器在转发包的时候，如果包的大小超过了MTU，那么这个包会被分片（fragmentation）。而终端设备在发包时，也可以设置DF标志位（Don’t Fragment）来告诉路由器不要对这个包分片，此时如果这个包大小超过了MTU，那么路由器就会丢掉这个包，并回复一条ICMP Fragmentation Needed消息。发送者收到这个消息后，下次就会发送小一点的包。这个过程叫做PMTU发现（PMTU Discovery）。\n但是互联网中有大量的设备因为各种原因，会配置为不回应ICMP Fragmentation Needed消息，这使得大小超过MTU的包会被无声地丢掉，直到TCP协议发现超时丢包并进行重传。这种情况就是PMTU黑洞。\n此外，IPv6包不支持分片，换句话说就是所有IPv6数据包全都带有DF标记。中间的路由器在遇到尺寸大于MTU的包的时候，应该回应ICMPv6 Packet Too Big消息，而同样的，由于各种原因，某些中间设备可能会直接丢掉这个包而不返回这条消息，直到TCP协议发现超时而进行重传。\n为什么用光猫或者硬路由拨号就没有这个问题这是因为，多数家用路由器默认开启了一个叫MSS Clamping的功能。这是针对PMTU黑洞的一个workaround，简单来说就是在TCP握手时，服务器会通过一个字段告知客户端它愿意接收的TCP包的最大尺寸，这样客户端就可以限制自己发送的包的大小，保证不会超出服务端要求的尺寸。\n在RouterOS中配置MSS Clamping配置非常简单，分别对IPv4和IPv6的防火墙Mangle表添加如下配置即可。\n# 将命令中 out-interface 参数的值替换成你的PPPoE接口的名字/ip/firewall/mangleadd action=change-mss chain=forward comment=&quot;IPv4 MSS clamp to PMTU&quot; \\    new-mss=clamp-to-pmtu out-interface=&quot;China Telecom&quot; passthrough=yes \\    protocol=tcp tcp-flags=syn/ipv6/firewall/mangleadd action=change-mss chain=forward comment=&quot;IPv6 MSS clamp to PMTU&quot; \\    new-mss=clamp-to-pmtu out-interface=&quot;China Telecom&quot; passthrough=yes \\    protocol=tcp tcp-flags=syn\n\n参考文档\n开启 IPv6 后网速变得很慢？可能是 PMTU 黑洞的问题 - V2EX\nROS修改MTU和MSS解决上网慢和页面显示不全问题\n什么是最大分段大小 (MSS)？ - CloudFlare\n\n","categories":["瞎折腾"],"tags":["RouterOS","二手MicroServer Gen8折腾记","Gen8","软路由"]},{"title":"在群晖的Docker环境中运行甜糖星愿服务","url":"/run-tiantang-on-synology-docker.html","content":"前段时间在网上看到了一个叫“甜糖星愿计划”的东西，声称可以通过贡献闲置带宽来获得积分。正好我有个NAS，正好我的宽带一天从白天闲到黑夜，不如利用起来，少少挣一些零花钱。\n需要注意的一点是，本文提到的镜像仅在我的群晖DS218+上测试过，虽然镜像中未使用任何群晖限定的依赖，理论上适用于任何x86架构的平台，但并不保证运行效果。而且本文目标平台是x86，如果你拥有ARM平台的机器，那根本不需要废这个劲，你可以直接运行甜糖星愿的可执行程序。\n本文提到的操作全部基于Docker，故在按照本文操作前，请先确保你已经拥有足够的知识来使用Docker和docker-compose。\n\n\n我的网络环境因为这个镜像相关的一些配置是以我的网络结构为基础的，所以我觉得有必要简单说一下我的网络结构是什么样的。\n简单来说，我的网络就是这样子的。虽然网络里还有别的设备，但是因为与本文无关，我就略掉了。\n[公网] -- [光猫] -- [群晖]\n\n其中光猫负责拨号，所以它也是最外层的一个路由器，光猫的IP地址是192.168.1.1，它下面有一个192.168.1.0/24的子网，群晖与光猫直连，有一个192.168.1.0/24子网下的IP地址。在群晖的Docker中，会有一个macvlan驱动的网络，与物理网卡共享同一个子网。甜糖星愿的容器将会接入到这个macvlan网络，在接入时我会手动为其分配一个IP地址。\n安装Docker及配置网络因为本文是通过Docker来实现的，所以第一步当然是安装Docker。群晖嘛就从套件商店安装，其他平台就用自己平台的方法安装，总之装上Docker就行。安装好Docker之后，进入控制面板 -- 网络 -- 网络界面，选择连接公网的接口，如局域网1，点击管理 -- Open vSwitch设置，勾选启用 Open vSwitch。\n然后我们需要在Docker里面创建一个macvlan驱动的网络，命令如：\nsudo docker network create \\    -d macvlan \\    --subnet=192.168.1.0/24 \\    --gateway=192.168.1.1 \\    -o parent=ovs_eth0 \\    macvlan\n\n上面命令中subnet的值替换成群晖所在网络的子网，gateway指向该子网的网关，parent指向要绑定的物理网卡，最后面的macvlan是网络名，可以按需修改。\n准备二进制翻译器接下来，我们要借助QEMU的力量，放一个幻术。毕竟，甜糖星愿只给了ARM平台的可执行程序，直接在x86平台跑肯定是跑不起来的，所以我们要向内核注册一个翻译器，这样Docker在执行其他平台的机器码时就可以通过这个翻译器将其翻译成x86指令来执行了。是的，我知道这玩意效率不高，我已经体验过了。\n首先，克隆这个镜像的源码仓库[^2]，找到resources/qemu-aarch64-static，或者你直接从GitHub下载qemu-aarch64-static也行，把这个文件放到$PATH指定的目录，赋予执行权限。\n然后施法吟唱，很简单，就一句话：\nsudo docker run --rm --privileged multiarch/qemu-user-static:register\n\n但是，这个幻术在群晖重启之后就会消失，幻术没了，甜糖星愿就起不来。所以我们还得配一个开机后的自启动任务，让群晖帮我们自动吟唱。操作也很简单，首先将本仓库的set_qemu_user_static.sh放到一个合适的位置，比如我放到了/var/services/homes/boris1993/scripts，然后前往控制面板 -- 任务计划，然后按照如下说明新增一个任务计划：\n\n任务名称：可自选\n用户账号：root\n事件：开机\n任务设置页面的运行命令：/var/services/homes/boris1993/scripts/set_qemu_user_static.sh\n\n如果要看这个脚本的日志的话，在运行命令里面把脚本输出重定向到一个文件就可以了。\n运行镜像进入本仓库所在目录，或者下载docker-compose.yml到一个合适的目录下，然后稍微编辑一下，以符合你的网络状况。主要要检查的就是networks.macvlan.ipv4_address的值，要确保它处在上面创建的那个macvlan网络的子网中。\n确认无误后，执行sudo docker-compose up -d，Docker就会自动拉取镜像，并启动一个名为tiantang的容器。\n容器内的甜糖星愿在首次启动时会自动退出，这是因为它在自我更新（我就不说这个操作一开始给了我多大的困扰了）。不要怕，容器里面有一个每分钟执行一次的定时任务，在没有发现甜糖星愿的进程时会自动将它启动。\n除此之外，我会在容器启动两分钟后，检测甜糖星愿监听了哪些端口，然后通过脚本自动设置路由器上的UPnP规则，这样你就不需要将甜糖星愿容器的IP放到DMZ里，也不需要手动配置端口转发了。当然这个操作的前提是，你的路由器支持UPnP，并且你启用了UPnP，如果没有的话，那你只能手动操作了。或者，如果你不想让脚本自动配置UPnP，或者客观条件下不允许你这么做，那么你可以把docker-compose.yml中SKIP_UPNP_AUTOCONFIG这一环境变量置为true，这样脚本就不会执行了。\n在甜糖星愿稳定运行，且端口转发规则也配置成功后，就可以通过手机客户端绑定这个节点了。但是这里有个问题，就是手机与甜糖星愿必须处在同一个子网里才能自动发现，如果因为各种原因没能自动发现，那么你可以在tiantang这个容器中打开一个bash终端，执行ttnode_168 -p /data，在输出中会打印出这个节点的UID，将其复制到任意二维码生成工具中生成一个二维码，然后用手机端扫描这个二维码，即可完成绑定。\n在这之后，就没什么我们能做的了。我们就慢慢等着甜糖星愿给你分配任务吧。我是等了大概有两三天才开始跑流量的。如果在配置好UPnP后，手机端仍提示“未配置网络”，那有可能是在配置UPnP的脚本启动时，甜糖星愿尚未打开所有端口，导致UPnP规则不完整。你可以前往路由器的UPnP页面，将页面显示结果与netstat -nlp | grep qemu命令的输出做对比，如果不一致的话，重新手动运行set-port-forwarding.sh脚本即可。\n深入了解如果你不止满足于把它跑起来，还想要了解这个镜像背后的运作方式，那么你可以前往这个镜像的GitHub仓库[^3]阅读该镜像相关的源码。在源码中我也写了注释，可以方便你理解我的想法。\n我的邀请码如果我的镜像帮到了你，或者我的这篇文章帮到了你，那么如果你愿意的话，你可以在手机客户端的填写邀请码处，填上我的邀请码**804744**，这样你可以获得15张加成卡，当然我也会得到一些加成。反正互利互惠，我的加成不会从你的身上扣掉，何乐而不为呢？\n[^2]: tiantang-x86-docker - Docker Hub[^3]: tiantang-x86-docker - GitHub\n","categories":["瞎折腾"],"tags":["甜糖星愿","甜糖"]},{"title":"手动编写代码调用JSR-303 Bean Validation","url":"/programatically-call-jsr-303.html","content":"最近做了一个有点不一样的项目，它是将传入接口的业务参数以JSON的形式放在了一个统一的请求体里面，我要将它取出来，再反序列化到一个Bean里面。这样会带来一个问题，就是我不能直接使用@Valid注解来让框架自行校验参数的合法性，而需要手动调用Validator实现对bean的校验。\n\n\n在这里我就不去还原从请求体取出业务数据并反序列化这个过程了，因为这个操作对于我们实际要实现的功能没有关系。我将在这里新建一个简单的类，设定好适当的校验规则，然后通过一个简单的示例来演示。\n示例bean示例的bean就是一个喜闻乐见的学生信息，使用javax.validation.constraints包中的注解来设定校验规则。\n@Datapublic class Student &#123;    // 姓名是必填项    @NotNull(message = &quot;Student name is mandatory&quot;)    private String name;    // 性别是必填项，仅接受male和female，首字母可以大写也可以小写    @Pattern(regexp = &quot;(M|male)|(F|female)&quot;, message = &quot;Only male or female are accepted&quot;)    @NotNull(message = &quot;Student gender is mandatory&quot;)    private String gender;    // 成绩不是必填项    // 成绩必须大于等于0，且小于等于100    // 因为Max的值是开区间，所以得写101    @Max(value = 101, message = &quot;Maximum value of score is 100&quot;)    @PositiveOrZero(message = &quot;Score cannot be negative&quot;)    private Integer score;&#125;\n\n编写实现实现的中心思想就是手动获得一个Validator实例，然后调用它来对传入的bean进行校验。\n@Slf4j@RestControllerpublic class StudentController &#123;    @PostMapping(&quot;/student&quot;)    public String showStudent(@RequestBody Student student) &#123;        // 取得一个Validator实例        Validator validator = Validation.buildDefaultValidatorFactory().getValidator();        log.info(&quot;Validating bean with validator &#123;&#125;&quot;, validator.getClass().getCanonicalName());        // 调用Validator#validate方法对这个bean进行校验        // 所有的        // ConstraintViolation的泛型类型要设定为被校验bean的类型        Set&lt;ConstraintViolation&lt;Student&gt;&gt; errors = validator.validate(student);        // 这里遍历errors这个set，打印出各个错误的信息        errors.forEach(error -&gt; &#123;            log.error(&quot;=======================&quot;);            // 对应校验规则里面的message属性            log.error(&quot;Error message: &#123;&#125;&quot;, error.getMessage());            // 校验失败的属性名            log.error(&quot;Property path: &#123;&#125;&quot;, error.getPropertyPath());            // 导致校验失败的值            log.error(&quot;Error value: &#123;&#125;&quot;, error.getInvalidValue());            log.error(&quot;=======================&quot;);        &#125;);        if (errors.size() &gt; 0) &#123;            // 可以取出所有的校验失败信息，拼接起来之后返回给调用方            final String errMessages = errors.stream()                    .map(ConstraintViolation::getMessage)                    .collect(Collectors.joining(&quot;, &quot;));            // 这里为了省事直接抛出了RuntimeException            // 实际使用时建议新建一个自定义业务异常代表这种情况            throw new RuntimeException(errMessages);        &#125;        return student.toString();    &#125;&#125;\n\n测试一下我使用一个这样子的数据来测试上面的校验功能：\n&#123;  &quot;name&quot;: &quot;Boris&quot;,  &quot;score&quot;: 180&#125;\n\n可见，这个数据是无法通过校验的，它没有填写性别，而且分数超过了上限。请求发出去之后，我得到了这样的错误信息：\n&#123;  &quot;timestamp&quot;: &quot;2020-01-06T03:03:18.125+0000&quot;,  &quot;status&quot;: 500,  &quot;error&quot;: &quot;Internal Server Error&quot;,  &quot;message&quot;: &quot;Maximum value of score is 100, Student gender is mandatory&quot;,  &quot;path&quot;: &quot;/student&quot;&#125;\n\n同时，控制台里出现了这样的日志：\n2020-01-06 11:21:25.971  INFO 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : Validating bean with validator org.hibernate.validator.internal.engine.ValidatorImpl2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error message: Student gender is mandatory2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : Property path: gender2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error value: null2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error message: Maximum value of score is 1002020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : Property path: score2020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error value: 1802020-01-06 11:21:26.214 ERROR 68021 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:21:26.252 ERROR 68021 --- [nio-9999-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.RuntimeException: Student gender is mandatory, Maximum value of score is 100] with root causejava.lang.RuntimeException: Student gender is mandatory, Maximum value of score is 100    at com.example.demo.StudentController.showStudent(StudentController.java:42) ~[classes/:na]    堆栈信息太多，下面的略掉了\n\n看来，校验的代码成功起作用了。\n简化代码因为我们现在基本上都是面向Spring编程，所以其实上面那些手动获取Validator的代码也是不必要的。我们可以让Spring自动注入一个Validator来实现功能。\n@Slf4j@RestController@RequiredArgsConstructorpublic class StudentController &#123;    // 直接注入一个Validator的实例    private final Validator validator;    @PostMapping(&quot;/student&quot;)    public String showStudent(@RequestBody Student student) &#123;        // 我们来看看它到底注入了谁        log.info(&quot;Validating bean with validator &#123;&#125;&quot;, validator.getClass().getCanonicalName());        // 调用Validator#validate方法对这个bean进行校验        // 所有的        // ConstraintViolation的泛型类型要设定为被校验bean的类型        Set&lt;ConstraintViolation&lt;Student&gt;&gt; errors = validator.validate(student);        // 这里遍历errors这个set，打印出各个错误的信息        errors.forEach(error -&gt; &#123;            log.error(&quot;=======================&quot;);            // 对应校验规则里面的message属性            log.error(&quot;Error message: &#123;&#125;&quot;, error.getMessage());            // 校验失败的属性名            log.error(&quot;Property path: &#123;&#125;&quot;, error.getPropertyPath());            // 导致校验失败的值            log.error(&quot;Error value: &#123;&#125;&quot;, error.getInvalidValue());            log.error(&quot;=======================&quot;);        &#125;);        if (errors.size() &gt; 0) &#123;            // 可以取出所有的校验失败信息，拼接起来之后返回给调用方            final String errMessages = errors.stream()                    .map(ConstraintViolation::getMessage)                    .collect(Collectors.joining(&quot;, &quot;));            // 这里为了省事直接抛出了RuntimeException            // 实际使用时建议新建一个自定义业务异常代表这种情况            throw new RuntimeException(errMessages);        &#125;        return student.toString();    &#125;&#125;\n\n重新启动应用，并用相同的数据测试之后，我们得到了这样的日志：\n2020-01-06 11:15:17.957  INFO 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : Validating bean with validator org.springframework.validation.beanvalidation.LocalValidatorFactoryBean2020-01-06 11:15:18.071 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error message: Student gender is mandatory2020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : Property path: gender2020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error value: null2020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error message: Maximum value of score is 1002020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : Property path: score2020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : Error value: 1802020-01-06 11:15:18.072 ERROR 67745 --- [nio-9999-exec-1] com.example.demo.StudentController       : =======================2020-01-06 11:15:18.089 ERROR 67745 --- [nio-9999-exec-1] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.RuntimeException: Student gender is mandatory, Maximum value of score is 100] with root causejava.lang.RuntimeException: Student gender is mandatory, Maximum value of score is 100    at com.example.demo.StudentController.showStudent(StudentController.java:42) ~[classes/:na]    下面的堆栈信息依旧略掉\n\n看来这种方式使用了另一个Validator实现，但是没关系，我们依旧能得到正确的结果，并可以使用完全一样的方法来处理错误信息。\n[^1]: How to Invoke JSR 303 Bean Validation Programmatically[^2]: How to manually trigger spring validation? - StackOverflow\n","categories":["小技巧"],"tags":["Java","代码技巧"]},{"title":"使用Coze工作流将小红书笔记保存至Notion - 工作流配置","url":"/save_rednote_to_notion_with_coze_workflow.html","content":"半个月前我的小红书号不明不白的被封了。而在被封之后我才发现，“我”这个页面也彻底被屏蔽，我再也看不到我收藏的帖子。痛定思痛，既然保存在小红书上的收藏有再也看不到的可能，那么，我把帖子保存到我自己的Notion不就好了？\n说干就干，在一顿网上冲浪之后，我看到有人已经实现了利用字节跳动火山引擎的“扣子（Coze）”实现了这个功能，但作者并没有放出来具体的实现。我一方面很不喜欢需要私信进群这种操作，另一方面还是自己做一个才最符合自己的需求（拜托，我可是程序员欸），所以自己动手实现了一个工作流，可以完整转存帖子的标题、内容、图片到自己的Notion。\n\n\n大致的功能设计关于这个工作流，我希望它能做到这些：\n\n自动解析小红书分享链接（类似94 某某某发布了一篇小红书笔记，快来看吧！ 😆 zop6CK2fPRDGwIT 😆 http://xhslink.com/a/5ZOdZhppuYteb，复制本条信息，打开【小红书】App查看精彩内容！）的这种。我需要它能从这个分享链接中获取到对应笔记的真实地址。哦对，这条链接就不用试了，因为这是一条我自己编的。\n自动解析小红书笔记，将里面的标题、内容、图片全部提取出来。\n在成功提取出笔记内容之后，它可以将图片上传到Notion，并将其嵌入到Notion文档中。\n可以通过手机直接分享到工作流，而不是拷出来分享链接，再打开Coze手动运行它。\n\n我不需要它做的是：\n\n转存视频。因为视频我可以直接保存到相册，或者手机录屏。而且对我来说，视频的信息密度比文字低，就算保存下来，我也要再把内容提炼成文字版保存。\n转存评论。评论区的内容通常会很杂乱，没有全部转存的必要。有价值的内容我再手动拷到自己的帖子里面就是了。\n监控小红书收藏夹。我觉得它暂时没这个本事，要做到这个，我可能需要单独开发一个程序。\n\n简单介绍一下它具体是怎么工作的这个工作流看起来是这样的\n\n具体工作起来，会进行下面这几步操作：\n\n首先使用配置好的token尝试连接Notion，如果连不上，那也就别费劲干后面的事了，赶紧报错通知用户检查\n接下来先解析分享链接，把实际的短链接拿出来，然后再用短链接换到对应的长链接\n得到长链接后，就可以调用插件得到这个笔记的标题、内容、图片地址，并能以JSON格式输出\n得到了笔记的内容之后，我就可以把图片逐个下载下来，再上传到Notion，然后使用这些内容创建一篇帖子\n最后，如果帖子创建成功，那么返回帖子的链接\n\n在Notion中要做的准备我的这个工作流是基于Notion的数据库实现的，所以首先需要在Notion中创建一个新的数据库。这个数据库将会包含三个列：\n\nName - 数据库默认存在并不可删除的列，作为帖子的标题\nImages - 一个Files &amp; media类型的列，图片将在这里保存及展示\nTags - 一个Multi-select类型的列，用于保存帖子的标签\n\n最终你应该会得到一个这样的数据库：\n\n接下来，因为我们需要调用Notion的API来上传图片和创建帖子，所以我们需要前往Notion Integrations页面为其创建一个API key，配置它的权限，并将刚才创建好的数据库与它绑定。\n首先点New Integration创建一个API key。名字无所谓，类型选Internal。\n创建成功后会自动跳转到Configuration页面。在Capabilities的Content Capabilities中勾选Read、Update和Insert权限，然后在User Capabilities中选择No user information。\n\n接下来到Access页面，点Edit access并勾上刚刚创建的数据库。\n\n至此Notion部分的准备工作就结束了。\n创建工作流接下来就是到扣子平台创建工作流了。登陆后点击创建，然后选择创建智能体，起个名字点击创建，就会进入智能体的编辑界面。\n配置工作流的变量首先点击变量部分的加号，并创建三个变量：\n\nrednote_cookie - 小红书的cookie，解析笔记内容要用到，并且这个cookie似乎不能来自被封禁的账号\nnotion_page_root - Notion数据库的ID，在Notion中点进去上面创建的数据库，将浏览器地址栏中的URL去掉https://www.notion.so/和问号后面的东西，剩下的这部分就是数据库ID\nnotion_secret - Notion的API key，回到创建API key的页面，点击Internal Integration Secret的Show并把内容拷过来就行\n\n\n然后，就可以开始开发工作流本体了。\n配置工作流组件这里我会逐个介绍每个节点的配置，至于节点之间的连接关系，如果你看文字看不明白的话，就参考上面的截图。\n\n在开始节点添加一个string类型的输入，名为rednote_share，用来传入小红书的分享链接\n添加notion数据库助手服务的get_database_info节点，添加两个输入变量\nsecret引用用户变量notion_secret\nurl填写数据库的URL，就是从浏览器地址栏复制出来的那个，记得去掉问号和后面的东西\n\n\n添加一个选择器节点，检查get_database_info节点的code输出字段是否不等于0。如果满足条件，即说明数据库连接不成功，应当报错并退出\n为上一步的选择器节点的如果添加一个输出节点\n添加两个输出变量\ndatabase_id引用get_database_info的输出字段id\nmessage引用get_database_info的输出字段msg\n\n\n配置输出内容为连接Notion数据库&#123;&#123;database_id&#125;&#125;失败，错误信息：&#123;&#123;message&#125;&#125;\n\n\n再为上一步的选择器节点的否则添加一个小红书的get_url节点，其输入变量share_content引用开始节点的rednote_share\n继续为get_url添加小红书连接处理的shoturl节点，其输入变量short_url引用上一步get_url节点的url输出字段\n然后添加一个小红书笔记详情的GetNoteData节点，其输入变量url引用上一步shoturl节点的long_url输出字段\n接下来创建一个代码节点，这是整个工作流的核心\n添加如下输入变量：\ntitle引用GetNoteData的title输出字段\ncontent引用GetNoteData的content输出字段\nimages引用GetNoteData的images输出字段\ntags引用GetNoteData的tags输出字段\nparent_id引用用户变量notion_page_root\nsecret引用用户变量notion_secret\n\n\n添加一个类型为Object的输出变量response，并添加如下输出字段\nstatus，类型为Integer\nmessage，类型为String\nurl，类型为String\n\n\n点击代码节点的在IDE中编辑，清空内容并粘贴如下代码片段（我知道，这代码很不Pythonic）\n\n\n\nimport osimport tempfileimport requests_async as requestsasync def main(args: Args) -&gt; Output:    params = args.params    parent_id = params[&#x27;parent_id&#x27;]    secret = params[&#x27;secret&#x27;]    title = params[&#x27;title&#x27;]    content = params[&#x27;content&#x27;]    images = params[&#x27;images&#x27;]    tags = params[&#x27;tags&#x27;]    file_upload_id_list = await transfer_images(secret, images)    post = build_post(parent_id, title, content, file_upload_id_list, tags)    create_post_response = await create_post(secret, post)    # 构建输出对象    ret: Output = &#123;        &quot;response&quot;: create_post_response    &#125;    return retasync def transfer_images(secret: str, images: list[str]) -&gt; list[str]:    file_upload_id_list = []    for image in images:        tmp_file_path = await download_image_file(image)        file_upload_id = await upload_file_to_notion(secret, tmp_file_path)        file_upload_id_list.append(file_upload_id)    return file_upload_id_listasync def download_image_file(url: str) -&gt; str:    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=&#x27;.jpg&#x27;)    tmp_file_path = tmp_file.name    tmp_file.close()    response = await requests.get(url)    with open(tmp_file_path, &#x27;wb&#x27;) as file:        file.write(response.content)        return tmp_file_pathasync def upload_file_to_notion(secret: str, file_path: str) -&gt; str:    # Initiate file upload    url = &#x27;https://api.notion.com/v1/file_uploads&#x27;    response = await requests.post(url, headers=&#123;        &#x27;Authorization&#x27;: &#x27;Bearer &#x27; + secret,        &#x27;Notion-Version&#x27;: &#x27;2022-06-28&#x27;,    &#125;, data=&#123;        &#x27;mode&#x27;: &#x27;single_part&#x27;,    &#125;)    file_upload_id = response.json()[&#x27;id&#x27;]    # Upload the file    with open(file_path, &#x27;rb&#x27;) as file:        files =  &#123;            &#x27;file&#x27;: (os.path.basename(file_path), file, &#x27;image/jpeg&#x27;)        &#125;        url = f&#x27;https://api.notion.com/v1/file_uploads/&#123;file_upload_id&#125;/send&#x27;        await requests.post(url, headers=&#123;            &#x27;Authorization&#x27;: &#x27;Bearer &#x27; + secret,            &#x27;Notion-Version&#x27;: &#x27;2022-06-28&#x27;,        &#125;, files=files)        return file_upload_iddef build_post(parent_id: str, title: str, content: str, file_ids: list[str], tags: list[str]) -&gt; object:    content_blocks = build_content_blocks(content)        files = []    for file_id in file_ids:        files.append(&#123;            &#x27;type&#x27;: &#x27;file_upload&#x27;,            &#x27;file_upload&#x27;: &#123;                &#x27;id&#x27;: file_id            &#125;        &#125;)    return &#123;        &#x27;parent&#x27;: &#123;            &#x27;database_id&#x27;: parent_id        &#125;,        &#x27;properties&#x27;: &#123;            &#x27;Name&#x27;: &#123;                &#x27;title&#x27;: [                    &#123;                        &#x27;text&#x27;: &#123;                            &#x27;content&#x27;: title                        &#125;                    &#125;                ]            &#125;,            &#x27;Images&#x27;: &#123;                &#x27;files&#x27;: files            &#125;,            &#x27;Tags&#x27;: &#123;                &quot;multi_select&quot;: build_tag_properties(tags)            &#125;        &#125;,        &#x27;children&#x27;: content_blocks    &#125;def build_content_blocks(content: str) -&gt; list[object]:    blocks = [];    paragraphs = content.replace(&#x27;\\t&#x27;, &#x27;&#x27;).split(&#x27;\\n&#x27;)    for paragraph in paragraphs:        blocks.append(&#123;            &#x27;object&#x27;: &#x27;block&#x27;,            &#x27;type&#x27;: &#x27;paragraph&#x27;,            &#x27;paragraph&#x27;: &#123;                &#x27;rich_text&#x27;: [                    &#123;                        &#x27;type&#x27;: &#x27;text&#x27;,                        &#x27;text&#x27;: &#123;                            &#x27;content&#x27;: paragraph                        &#125;                    &#125;                ]            &#125;        &#125;)    return blocks;def build_tag_properties(tags: list[str]) -&gt; list[object]:    items = []    for tag in tags:        items.append(&#123;            &quot;name&quot;: tag        &#125;)    return itemsasync def create_post(secret: str, post: object) -&gt; str:    url = &#x27;https://api.notion.com/v1/pages&#x27;    response = await requests.post(url, headers=&#123;        &#x27;Authorization&#x27;: &#x27;Bearer &#x27; + secret,        &#x27;Notion-Version&#x27;: &#x27;2022-06-28&#x27;,    &#125;, json=post)    return response.json()\n\n为代码节点添加一个选择器节点，判断代码节点的输出字段status是否为200，并分别为其添加如下后续节点\n如果部分添加输出节点，添加一个输入变量output，引用代码节点的url输出字段，输出内容为&#123;&#123;output&#125;&#125;，负责在帖子创建成功后输出帖子URL\n否则部分添加输出节点，添加一个输入变量output，引用代码节点的message输出字段，输出内容为&#123;&#123;output&#125;&#125;，负责在帖子创建失败后输出错误信息\n\n\n添加一个变量聚合节点并关联连接Notion失败的输出节点、创建帖子成功的URL输出节点、创建帖子失败的错误信息输出节点，其Group1的配置参考下图\n最后为变量聚合节点添加一个结束节点，添加一个输出变量output引用变量聚合节点的Group1输出字段\n\n如果最后你的工作流与我上面的截图一致，那么就大致可以认为你的工作流配置是正确的了。接下来就可以点击试运行按钮，贴一个小红书分享链接，来测试这个工作流。一切正常的话，工作流会输出一个Notion的URL，同时你在Notion的这个数据库中就能看见刚刚转存过来的帖子。\n\n接下来的计划目前这个工作流虽然实现了转存小红书笔记的功能，但仍有下面几个问题：\n\n小红书的cookie不会自动刷新，那么在cookie过期后，整个工作流就不能正常运行，需要用户手动去浏览器拷贝小红书的cookie并进入智能体配置页面更新\n在手机上不能很方便的调用这个工作流，目前只能进入到工作流编辑页面，通过试运行的方式调用\nNotion中的帖子不会去重，意味着如果我反复转存同一篇笔记，那么Notion中就会出现多个一模一样的帖子\n小红书解析相关功能依赖别人提供的插件，而这些插件有被下架的风险\n\n所以接下来我计划做两件事：\n\n一个Chrome浏览器插件，它可以定时刷新小红书页面并截获cookie，然后调用扣子的API更新对应的变量值\n一个基于iOS的Scriptable应用的脚本，使其支持系统的分享功能，它将可以调用扣子的API来调用这个工作流，并将小红书分享链接作为参数传入\n\n至于帖子重复的问题我不打算用代码的方式解决，因为收益实在太低了，就算重复了，也无非手动去Notion里删一下的事。插件被下架的风险暂时也不打算考虑，现有的能用就先用着，哪天不能用了再说。\n","categories":["瞎折腾"],"tags":["小红书","工作流","Coze","智能体"]},{"title":"微信小程序scroll-view填满剩余可用高度","url":"/scroll-view-auto-fit-height.html","content":"根据微信小程序scroll-view文档所述，scroll-view必须给定一个固定高度。那么如果我们想要让它自动填充剩余高度，该怎么办呢？\n\n\n前言在说出我的解决方案之前，先来看一下我的页面设计，以便于理解。\n\n如图所示，我将这个页面分成了三部分：最顶部的导航栏navbar，用于显示概要信息的header，以及本文的主角scroll-view。可见，scroll-view位于页面的最下方，如果我直接给它设定一个固定的高度，那么在不同尺寸的屏幕上，就可能会有高度过小而在下方留白，或者高度过大超出屏幕下边界的可能。那么，自动计算scroll-view的高度，看起来是一个可行的办法。\n思路有了，接下来就开始挑趁手的工具吧！\n需要的API首先，在计算过程中，整个页面的高度是必须要有的。而小程序的wx.getSystemInfo API正好可以提供这样的功能。\n其次，我们还得想办法拿到scroll-view上面各个组件的高度。小程序虽然没有DOM操作，但也提供WXML节点信息的API。\n撸起袖子开始干既然工具有了，那么，talk is cheap, I’ll show you the code!\n当然，简洁起见，我只会写出相关的代码，其余的代码我将直接略掉。\nPage(&#123;    data: &#123;        // 页面总高度将会放在这里        windowHeight: 0,        // navbar的高度        navbarHeight: 0,        // header的高度        headerHeight: 0,        // scroll-view的高度        scrollViewHeight: 0    &#125;,    onLoad: function(option) &#123;        // 先取出页面高度 windowHeight        wx.getSystemInfo(&#123;            success: function(res) &#123;                that.setData(&#123;                    windowHeight: res.windowHeight                &#125;);            &#125;        &#125;);        // 然后取出navbar和header的高度        // 根据文档，先创建一个SelectorQuery对象实例        let query = wx.createSelectorQuery().in(this);        // 然后逐个取出navbar和header的节点信息        // 选择器的语法与jQuery语法相同        query.select(&#x27;#navbar&#x27;).boundingClientRect();        query.select(&#x27;#header&#x27;).boundingClientRect();        // 执行上面所指定的请求，结果会按照顺序存放于一个数组中，在callback的第一个参数中返回        query.exec((res) =&gt; &#123;            // 分别取出navbar和header的高度            let navbarHeight = res[0].height;            let headerHeight = res[1].height;            // 然后就是做个减法            let scrollViewHeight = this.data.windowHeight - navbarHeight - headerHeight;            // 算出来之后存到data对象里面            this.setData(&#123;                scrollViewHeight: scrollViewHeight            &#125;);        &#125;);    &#125;&#125;)\n\n至于WXML里面，就还是使用双大括号来将data部分的scrollViewHeight的值绑定到height属性上面就是了。\n需要注意的是，上面计算出来的值，单位是px而不是rpx。\n&lt;scroll-view style=&quot;height: &#123;&#123;scrollViewHeight&#125;&#125;px&quot; scroll-y=&quot;true&quot;&gt;  &lt;!-- scroll-view里面的内容 --&gt;&lt;/scroll-view&gt;\n\n这样，我们就得到了一个可以自动填满屏幕最下方剩余空间的scroll-view啦～\n","categories":["学知识"],"tags":["小程序","scroll-view"]},{"title":"自建Tailscale的DERP节点","url":"/self-hosting-tailscale-derp.html","content":"迫于Tailscale官方的中继节点在国内几乎没法用，于是在轻量云香港区自建了一个DERP服务。\n\n\n安装derper因为derper是依赖go install实现分发，所以要先安装Go语言SDK。\n$ wget https://go.dev/dl/go1.20.linux-amd64.tar.gz$ sudo rm -rf /usr/local/go &amp;&amp; sudo tar -C /usr/local -xzf go1.20.linux-amd64.tar.gz\n\n安装成功后，需要配置环境变量，把Go的可执行文件，和~/go/bin目录加入PATH。在.bashrc（如果你用zsh，那就是.zshrc）中添加如下命令：\nexport PATH=$PATH:/usr/local/go/bin:~/go/bin\n\n重新载入.bashrc后，执行go install tailscale.com/cmd/derper@main，安装derper。\n此外，因为derper不能自动更新，所以需要设定一个定时任务来定期重新运行go install命令，如：\n0 0 * * * go install tailscale.com/cmd/derper@main\n\n测试derper在运行derper之前，你需要给这个服务器绑定一个域名，derper会用这个域名来申请Let’s Encrypt的SSL证书。申请和购买域名我就不在这废话了，网上有很多相关文章。\n此外，因为derper需要监听443端口，而我又不想以root身份运行它，所以我选择给它分配CAP_NET_BIND_SERVICE这个capability，来让它有监听低位端口的权限。\nsudo setcap CAP_NET_BIND_SERVICE=+eip ~/go/bin/derper\n\n然后就可以手动运行derper来测试了。\n# derper.conf它会自动创建，你给它指定一个位置就行$ go/bin/derper -c derper/derper.conf -hostname derp.mydomain.com2023/02/08 13:35:11 derper: serving on :443 with TLS2023/02/08 13:35:11 running STUN server on [::]:3478\n\n这时候derper就会开始监听443端口。\n如果443端口被占用，那么就需要手动指定监听的端口（比如4443），SSL证书也要自己通过acme.sh之类的工具生成，而且证书的名字必须符合域名.crt（即derp.mydomain.com.crt）这种格式。\nderper -c=derper/derper.conf -hostname derp.mydomain.com -a :4443 -certmode manual -certdir /home/admin/certs\n\n此时用浏览器访问https://derp.mydomain.com，会得到一个这样的网页：\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;DERP&lt;/h1&gt;&lt;p&gt;  This is a  &lt;a href=&quot;https://tailscale.com/&quot;&gt;Tailscale&lt;/a&gt;  &lt;a href=&quot;https://pkg.go.dev/tailscale.com/derp&quot;&gt;DERP&lt;/a&gt;  server.&lt;/p&gt;\n\n配置服务和防火墙每次手动执行derper命令肯定不现实，所以这时候就要systemd出马了。进入/etc/systemd/system，新建一个名为derper.service的文件，输入如下内容：\n[Unit]# 服务名字Description=Tailscale DERP Server# 在网络服务启动后启动这个服务After=network.target[Service]# 改成你的用户名User=admin# 总是自动重新启动Restart=always# 重启前等待5秒RestartSec=5# 启动derper的命令，跟上面测试用的命令一样ExecStart=/home/admin/go/bin/derper -c=/home/admin/derper/derper.conf -hostname derp.mydomain.com# 停止derper的命令ExecStop=/bin/kill $MAINPID# 赋予CAP_NET_BIND_SERVICE这个capabilityAmbientCapabilities=CAP_NET_BIND_SERVICE[Install]WantedBy=multi-user.target\n\n然后分别执行如下命令：\n# 载入service文件的变更$ sudo systemctl daemon-reload# 启动derper$ sudo systemctl start derper# 检查状态$ sudo systemctl status derper● derper.service - Tailscale DERP Server     Loaded: loaded (/etc/systemd/system/derper.service; enabled; vendor preset: enabled)     Active: active (running) since Wed 2023-02-08 14:27:13 CST; 2s ago   Main PID: 4944 (derper)      Tasks: 6 (limit: 1006)     Memory: 2.6M        CPU: 16ms     CGroup: /system.slice/derper.service             └─4944 /home/admin/go/bin/derper -c=/home/admin/derper/derper.conf -hostname derp.mydomain.comFeb 08 14:27:13 iZj6caykwpo11gr659a9avZ systemd[1]: Started Tailscale DERP Server.Feb 08 14:27:13 iZj6caykwpo11gr659a9avZ derper[4944]: 2023/02/08 14:27:13 derper: serving on :443 with TLSFeb 08 14:27:13 iZj6caykwpo11gr659a9avZ derper[4944]: 2023/02/08 14:27:13 running STUN server on [::]:3478# 开机自启动$ sudo systemctl enable derperCreated symlink /etc/systemd/system/multiuser.target.wants/derper.service → /etc/systemd/system/derper.service.Unit /etc/systemd/system/derper.service is added as a dependency to a non-existent unit multiuser.target.\n\n因为derper依赖HTTP、HTTPS和STUN协议，所以需要配置防火墙或安全组，开放80/tcp，443/tcp，和3478/udp端口。\n配置Tailscale现在自建的DERP节点就成功启动了，接下来我们就需要让Tailscale知道这个节点的信息。进入Tailscale的Admin console，进入Access controls，在JSON中增加如下配置：\n&#123;    // 前略    &quot;derpMap&quot;: &#123;        // 如果想要所有节点只使用自建中继的话，就启用这条配置        // &quot;OmitDefaultRegions&quot;: true,        &quot;Regions&quot;: &#123;            &quot;900&quot;: &#123;                &quot;RegionID&quot;:   900,                &quot;RegionCode&quot;: &quot;Aliyun-HKG&quot;,                &quot;Nodes&quot;: [                    &#123;                        &quot;Name&quot;: &quot;Aliyun-HKG-1&quot;,                        &quot;RegionID&quot;: 900,                        &quot;HostName&quot;: &quot;derp.mydomain.com&quot;,                    &#125;,                ],            &#125;,            // 如果有多个区域、多个节点，或者使用了自定义端口，那么可以参考这部分            &quot;901&quot;: &#123;                &quot;RegionID&quot;: 901,                &quot;RegionCode&quot;: &quot;Oracle-OSAKA&quot;,                &quot;Nodes&quot;: [                    &#123;                        &quot;Name&quot;: &quot;Oracle-OSAKA-1&quot;,                        &quot;RegionID&quot;: 901,                        &quot;HostName&quot;: &quot;osaka1.derp.mydomain.com&quot;,                        &quot;DERPPort&quot;: 4443,                    &#125;,                    &#123;                        &quot;Name&quot;: &quot;Oracle-OSAKA-2&quot;,                        &quot;RegionID&quot;: 901,                        &quot;HostName&quot;: &quot;osaka2.derp.mydomain.com&quot;,                        &quot;DERPPort&quot;: 4443,                    &#125;,                ]            &#125;        &#125;,    &#125;,&#125;\n\n最后不要忘了点击Save保存。保存成功后，可以在Machines中随便点进一台机器，看Relays里面有没有出现刚刚添加的DERP服务器。\n\n官方文档\nCustom DERP Servers\n\n","categories":["瞎折腾"],"tags":["Tailscale","DERP","内网穿透"]},{"title":"配置Caddy作为静态网站服务器和前置代理","url":"/set-up-caddy-server.html","content":"之前听闻有个新的Web Server，名曰Caddy，其配置简单，还默认启用HTTP&#x2F;2，并且可以自动申请Let’s Encrypt的HTTPS证书。试用了一番，觉得不错，便把这个博客的服务程序换成了Caddy。在这里呢，记录一下安装和配置的过程。\n\n\n安装万事第一步，先安装。\n下载页面概览打开Caddy的下载页面，页面的内容简洁明了，左侧是4个要配置的项，右侧是每个配置项实际的内容。\n\n选择运行平台首先，选择好Caddy要在哪个操作系统下运行。Caddy支持的平台还是足够多的，而且覆盖到了主流的操作系统，所以甭管您是Windows，还是Linux，抑或是BSD，都可以运行Caddy。因为我的服务器运行的是64位Ubuntu，所以选择Linux 64-bit。\n实话说，看到Plan 9的时候，心里还是被惊到了。\n\n选择插件接下来是选择要安装哪些插件，通常来说，根据自己的需要来选择就可以了。如果后期要安装更多的插件的话，重新来下载页面勾选需要的插件并重新安装就可以了。毕竟是用Go写的，最后就一个可执行文件，替换掉原来的，就算重装好了。\n我的需求有这么几点：\n\n我需要Caddy可以作为一个反向代理，所以选择了http.forwardproxy插件\n我的博客的源文件放置于我的GitHub中，我希望Caddy可以直接clone这个仓库，并且能通过WebHook监听这个仓库的更新事件，所以选择了http.git插件\n我在使用Cloudflare的DNS服务，并且Caddy可以通过DNS验证的方式申请HTTPS证书，所以需要tls.dns.cloudflare插件\n我想要Caddy作为一个系统服务，并且可以随系统自动启动，但是我又懒得自己写配置文件，所以使用hook.service插件来为我提供已经写好并经过了测试的配置文件\n\n\n选择是否开启遥测功能Caddy提供了一个叫做“遥测”的功能，可以监控您的Caddy实例的状态。具体针对该功能的描述，可以到其文档页面Telemetry阅读。这个功能开启与否与功能无关，开不开看您心情。\n选择适合您的许可证接下来，就是选择您要使用哪一种许可证来运行Caddy。像在下的博客是个人项目，不涉及商业应用，所以当然选择个人许可证。\n\n下载终于，到了下载这一步了。Caddy提供了多种下载的方式，您可以在浏览器中将可执行文件下载到本地，或者通过命令行来下载，还可以直接使用一句话脚本来安装。\n\n如果使用自动安装脚本的话，Caddy会被安装到/usr/local/bin/caddy中。如果选择手动安装，那么需要将Caddy的可执行文件放到PATH所包含的目录中，或者将Caddy所在的位置加入到PATH中。\n检查安装是否成功完成安装后，可以直接使用caddy命令启动一个Caddy服务器，它会开始监听本机的2015端口，并列出当前工作目录的内容。使用http://localhost:2015即可访问。如果能成功打开，或者可以看到一个404页面，那么说明Caddy安装成功了。\n配置Caddy的所有配置都将被写到一个名为Caddyfile的文件中。点击这里阅读Caddy官方提供的入门指导，以及Caddy官方文档。\n在以下实例中，我们假定Caddyfile的位置是/etc/caddy/Caddyfile，并且所有与Caddy相关的文件、目录，都存放于/etc/caddy下。\n配置网站的地址首先要配置Caddy所服务的网站的地址，如果只有一个地址的话，那么可以将地址写到Caddyfile的第一行，同时Caddyfile的第一行也必须是网站的地址。比如下面这样：\nwww.boris1993.tk\n\n这样Caddy就会监听www.boris1993.tk所绑定的地址，并监听80端口提供HTTP服务，以及443端口提供HTTPS服务。在默认情况下，Caddy会自动将HTTP请求使用HTTP 301返回码重定向到HTTPS，除非显式配置禁用HTTPS服务。\n如果需要指定端口号，那么可以在地址后面跟上端口号，比如www.boris1993.tk:8080。因为我没有用到这项功能，所以没有测试过这样配置的效果。如果您有需要还请自行测试。\n如果要同时开启多个网站，那么各个网站的配置需要以大括号包围起来，比如下面这样：\nwww.boris1993.tk &#123;    &#125;www2.boris1993.tk &#123;&#125;\n\n我们这里就只演示仅有一个地址的情况。多个地址的配置与单个地址的配置方式相同，故不再赘述。\n配置静态文件所在的位置并启用gzip压缩有了地址之后，我们需要告诉Caddy要提供的静态文件在什么位置，这个可以使用root指令来制定，如下面这样：\nwww.boris1993.tk &#123;    root    /var/www&#125;\n\n然后Caddy就会到/var/www目录寻找index.html等默认的主页文件。\n启用gzip压缩，可以使我们的网站打开的更快。在Caddy中启用gzip，也只需要一条指令：\nwww.boris1993.tk &#123;    root    /var/www    gzip&#125;\n\n提供申请HTTPS证书的信息在默认情况下，Caddy会自动搞定申请HTTPS证书的事情，不需要用户进行干预。如果需要覆盖默认的配置，可以参考Caddy文档的TLS部分。\n配置日志访问日志网站的访问日志可以使用log指令来配置，该指令的文档可以参考这里。\n在这里我先放出我的配置，然后再逐行来解释。简明起见，我就只写出日志的部分，其余无关内容就不在这里写出来了。\nlog /   /var/log/caddy/access.log   &quot;&#123;combined&#125;&quot; &#123;    rotate_size 1    rotate_age  7    rotate_keep 2    rotate_compress&#125;\n\n第一行中，我指定要记录所有对网站根目录/的访问，将日志写到/var/log/caddy/access.log中，记录的方式是combined。\nCaddy提供了两种日志格式，common和combined，common是默认的记录格式。\ncommon的格式是这样子的：\n{客户端IP地址} - {HTTP基础验证的用户名} [{访问时间}] \\&quot;{HTTP方式} {请求的URI} {协议版本}\\&quot; {HTTP状态码} {响应体的大小}\n而combined格式，则是在common格式的末尾，追加如下内容：\n\\&quot;{&gt;Referer}\\&quot; \\&quot;{&gt;User-Agent}\\&quot;\n第二行rotate_size指定了在日志到达1MB大小之前不进行日志翻转，这个指令的单位是MB。\n第三行rotate_age指定了保留7天的翻转日志。\n第四行rotate_keep指定了只保留最近2个翻转日志，之前的版本将被删除。\n第五行rotate_compress指定使用gzip压缩翻转日志。\n错误日志错误日志可以使用errors指令来配置，该指令的文档可以参考这里。\n同样，我将以我的配置作为范例来解释，如果需要其他的配置可以参考官方文档。\nerrors  /var/log/Caddy/error.log &#123;    404         /var/www/error/HTTP404.html    rotate_age  7    rotate_keep 2    rotate_compress&#125;\n\n第一行配置了错误日志将被写入到/var/log/Caddy/error.log中。\n第二行配置了当发生404错误后显示的页面，这里还可以为其他错误码指定错误页面，语法参见官方文档。\n其余三行的含义与上文log指令中对应参数的含义一致，不再赘述。\n配置自动从Git拉取页面内容Caddy支持从一个指定的Git仓库克隆以及更新页面的内容到某个目录，并可以通过WebHook来监视仓库的更新，参考配置如下：\ngit https://github.com/boris1993/boris1993.github.io.git &#123;    path        /var/www    hook        /hook       hook.password    hook_type   github&#125;\n\n这里我配置Caddy从https://github.com/boris1993/boris1993.github.io.git这个仓库拉取静态页面文件，这就是本博客所在的GitHub仓库，拉去之后文件将被放到/var/www目录下。因为我要实现博客文件自动更新，所以这里的地址需要与root指令配置的位置相同。\nhook参数配置Caddy使用www.boris1993.tk/hook作为WebHook的监听地址，这个hook的访问密码是hook.password，并且使用下一行中的hook_type指令显式指定Hook的类型是github，也就是来自GitHub的hook。\n这样配置完毕后，还需要为远程Git仓库配置hook，然后才可以实现自动更新。具体配置方式请参考Git仓库服务商的文档。\n配置Caddy作为前置代理一部分代理工具，比如v2ray，支持使用一个HTTP服务器作为其前置代理，Caddy就可以实现这样的功能。本示例中我配置Caddy作为v2ray的WebSocket代理，配置文件片段如下：\nproxy /v2ray localhost:12345 &#123;    websocket    header_upstream -Origin&#125;\n\n这段配置指定了将/v2ray这个路径作为localhost:12345这个地址的前置代理，代理协议为websocket。具体的配置方法请参考被代理程序的文档。\n配置开机自启动hook.service插件可以一键生成systemd格式的自启动配置文件，只需要如下命令即可完成配置：\ncaddy -service install -conf /etc/caddy/Caddyfile\n\n注意将-conf参数的值指向实际的Caddyfile的路径。\n结束至此，一个可以正常提供服务的Caddy服务器就配置完成了，现在Caddy可以提供正常的HTTP和HTTPS访问，并且会自动申请和续订HTTPS证书，在远端Git仓库有更新之后，Caddy也会自动更新本地的文件，一切都变成了自动化操作，正常情况下完全可以实现无人值守运行。怎么样，是不是很方便？\n","categories":["瞎折腾"],"tags":["Caddy","WebServer"]},{"title":"二手MicroServer Gen8折腾记(2/n) - 配置RouterOS软路由","url":"/setting-up-microserver-gen8-routeros.html","content":"我之前宽带一直都是光猫拨号，电脑和NAS直连光猫，光猫下挂个二级无线路由器来给其他设备联网，但我早就对这个拓扑不爽了，主要原因是自己花大钱买的AC86U派不上用场，只能干点边缘的杂活，而且搞两套局域网怎么看都挺恶心的。之前也试过把路由器改成AP模式，让所有设备都直连光猫，但是万恶的电信光猫限制了设备数，只有前5台机器能上网，其它的机器虽然能拿到地址，但是上网必定超时。那现在既然有了台服务器，那干脆装个软路由。\n软路由系统我打算玩玩RouterOS，毕竟iKuai听说过没用过，据说还出过劫持流量这种问题，不敢用也不想用；OpenWRT以前玩过，没啥新鲜感，而RouterOS，之前装着玩的时候就觉得倍有逼格，早就想搞搞它玩玩了。\n本篇为二手MicroServer Gen8折腾记的第二篇，记录配置猫棒和搭建RouterOS软路由的过程。\n\n\n获取光猫中的配置我找装维师傅说要玩猫棒，需要进后台看参数，师傅就把密码查给我了。\n用超级密码登录光猫后台，把LOID、PPPoE的用户名和密码（密码需要base64解码才能得到真实的密码）、IPv6前缀长度记在一个文档里备查。\n准备猫棒因为我懒得找运营商的装维师傅给我改桥接（之前沟通过，师傅说要先换光猫，新光猫又得等），也考虑到留着原来的光猫配置不变的话，在我折腾翻车的时候也不需要再找装维师傅改回路由模式，直接把光纤接回去就能恢复网络，所以我直接在淘宝上买了根ODI的猫棒。又因为服务器上没有SFP口，我也不想再买个网卡，就顺便一起买了个光电转换器。\n\n之前了解到，光纤的端面接口有PC、UPC、APC三种，之所以选择ODI的光猫，就是因为我们一般宽带光纤的端面是UPC的，而诺基亚和阿尔卡特之类的猫棒是APC接口，还要套个转换器才行，硬插进去可能会损坏光纤，但ODI这款采用的就是UPC接口，不用再买转换器。但是我无法保证覆盖所有情况，如果你也想用猫棒，请一定提前跟店家和装维师傅确认好，如果顶坏了光纤端面，你只能喊装维师傅来给你重新做接头，别无他法！\n\n抓住光纤的蓝色滑套，往后抽，就可以把光纤从光猫上拔出来，然后对准接口插进猫棒，再把猫棒插进光电转换器，然后接上网线和电源，就可以了。\n\n稍等几分钟等猫棒开机，开机之后，转换器在猫棒方向的灯会亮起来。这时候先把网线跟电脑连起来，给电脑分配一个192.168.1.0/24范围的地址（当然别用192.168.1.1，那是猫棒后台的地址），然后用浏览器打开192.168.1.1，就能看见猫棒后台的登陆页面了。账号和密码找商家要，我这款是admin/admin，但不保证所有ODI猫棒都是这一对账号密码。登陆进去后，进入Settings页面，填入LOID，点Apply Changes，然后进入Commit/Reboot页面，点Commit and Reboot按钮保存配置并重启，猫棒端就配置完成了。\n不过电信、联通、移动的光猫注册方式各有不同，上面说的操作都是针对电信的。其他运营商可以参考猫棒商家发的文档，或参考网上其他人的成功案例。这里引用猫棒说明中的一部分内容：\n\n★注册-初级:（部分型号光猫没有 GPON SN，可不修改）电信&amp;联通 GPON&#x2F;XGPON&#x2F;XGSPON：Loid移动 GPON&#x2F;XGPON&#x2F;XGSPON：Ploam 密码电信&amp;联通&amp;移动 EPON&#x2F;10G EPON：Loid\n★注册-中级:电信&amp;联&amp;移动 GPON&#x2F;XGPON&#x2F;XGSPON：GPON SN电信&amp;联通&amp;移动 EPON&#x2F;10G EPON：MAC&#x2F;PON MAC+MAC KEY\n★注册-高级：（GPON&#x2F;XGPON&#x2F;XGSPON）厂商 ID&#x2F;Vendor ID：如 HWTC&#x2F;华为、FHTT&#x2F;烽火、ZTEG&#x2F;中兴、SCTY&#x2F;天邑、YHTC&#x2F;友华设备型号OUI：6 位设备标号：一般为 17 位软件版本号ONT 版本&#x2F;硬件版本号\n★注册-高级：（EPON&#x2F;10G EPON）设备型号ONT 版本&#x2F;硬件版本号\n\n\n本来重启之后，应该先看一下ONU状态是不是O5来着，但是当时我忘了看，直接电脑上拨号成功，间接也证明了LOID注册成功。不过还是放个文档里扒出来的图好了。\n\n安装RouterOS一开始我以为装这玩意跟装别的系统没啥区别，就ISO一挂，顺着向导装就行。可看了文档发现，RouterOS有一个专门面向虚拟平台的版本，叫Cloud Hosted Router (CHR)，在官网的下载页面下载OVA Template，然后导入ESXi就行。具体的安装方法可以看Mikrotik的Wiki。\nRouterOS CHR作为一个付费的系统，当然也需要买授权才能用。买授权的方法也可以跟着Mikrotik的Wiki操作，就是注意别买成RouterOS的Key，这两者是不通用的。\n另外安装之后，如果你更改了RouterOS的CPU数量，那么内存大小也要跟着调整。官方文档提供了如下公式：\n\nRouterOS 6：RAM &#x3D; 128 + [8 * CPU数量 * (网络接口数量 - 1)]RouterOS 7：RAM &#x3D; 256 + [8 * CPU数量 * (网络接口数量 - 1)]注：建议为CHR实例分配至少1024MB内存。\n\n我因为没注意到这个信息，只把CPU改成了4核心，但内存还是160MB，导致几乎每24小时就会出一次kernel failure in previous boot错误，为这事我还给MikroTik开了个工单……\n配置ESXi的网络如果我没记错的话，ESXi一开始只给Ethernet 1口做了配置，我们得手动给Ethernet 2配置虚拟网络。\n首先到网络的虚拟交换机页面，创建一个新的虚拟交换机，名字按自己喜好起就行，比如我就顺着已有的vSwitch0给它命名vSwitch1。然后点进vSwitch1，点添加上行链路，把vmnic1分配给它，这样在ESXi里面就可以用这个接口了。\n\n然后到端口组页面，添加一个新的端口组VM Network 2，虚拟交换机选择刚创建的vSwitch1。\n\n接下来点进虚拟机RouterOS，关机之后编辑设置，给它添加一个网络适配器，然后给网络适配器1分配VM Network 2，给网络适配器2分配VM Network 1，保存，开机。\n\n配置RouterOS如果猫棒的ONU状态是O5，那就可以把转换器连到Gen 8的Ethernet 2口上了。你问为啥不是Ethernet 1？因为Ethernet 1我改成了与iLO共享，所以只能拿Ethernet 2当WAN口。下面进入RouterOS，开始配置软路由。\n首次登录第一次登录的时候，因为我并不知道路由器的地址，所以没办法用IP地址打开RouterOS的配置页面，但MikroTik很贴心的提供了用MAC地址连接的功能。在WinBox的Neighbors页面双击RouterOS的条目，用admin用户和空密码登录就可以。\n\n为了后面分的清楚，我先到Interfaces页面，把LAN口和WAN口对应接口的名字改了过来，毕竟默认的ether1和ether2不如LAN和WAN看的清楚。通常来说观察接口的Tx和Rx就能分清哪个是LAN口，因为这时候没配置拨号，WAN口的流量通常只会是0。如果分不清，那就先把Gen 8的Ethernet 1拔了。\n\n规划地址范围，配置DHCP、DNS在配置DHCP服务器前，要先规划好自己的局域网的网段。地址范围可以从IPv4的三个私有地址范围10.0.0.0/8、172.16.0.0/12和192.168.0.0/16之间选，一定不要作死用公网网段（比如一些无能IT给路由器配了个1.1.1.1），不然我很难说你的网络会出什么奇怪的问题。我干脆就遵循之前的网络，用了192.168.1.0/24段作为我的内网网段。\n确定好地址范围之后，就可以用这条命令给LAN口指定一个静态IP地址：\n/ip/address/add \\  interface=LAN \\  address=192.168.1.3/24 \\  network=192.168.1.0\n\n选择给RouterOS分配192.168.1.3的原因是，192.168.1.1留给猫棒（虽然后来发现连不上），192.168.1.2留给AC86U路由器，它本来就是这个地址，我懒得再动它。\n然后先给DHCP服务器添加一个地址池：\n/ip/pool/add \\  name=192.168.1.0/24 \\  ranges=192.168.1.2-192.168.1.254\n\n接下来配置DHCP服务器：\n# 添加DHCP服务器/ip/dhcp-server/add \\  name=LAN \\  address-pool=192.168.1.0/24 \\  interface=LAN \\  lease-time=1d# 配置DHCP服务器下发的网络参数/ip/dhcp-server/network/add \\  address=192.168.1.0/24 \\  dns-server=192.168.1.3 \\  gateway=192.168.1.3\n\n此外我还想要给ESXi之类的机器配置一个固定的地址，于是：\n/ip/dhcp-server/lease/add \\  address=192.168.1.4 \\  mac-address=D0:BF:9C:46:94:D8 \\  server=LAN \\  comment=ESXi\n\n上面DHCP网络参数中的dns-server=192.168.1.3配置的是让RouterOS作为DNS服务器，但现在RouterOS还不能响应远程的DNS请求，所以要用下面这条命令更改DNS的配置：\n/ip/dns/set allow-remote-requests=yes\n\n创建本地管理员，禁用admin安全起见，我建议给RouterOS创建一个新的管理员账户，并禁用默认的admin账户。\n# 创建新的管理员用户，仅允许在192.168.1.0/24网络内登录/user/add \\  name=新用户的用户名 \\  password=新用户的密码 \\  group=full \\  address=192.168.1.0/24# 禁用admin/user/set [ find name=&quot;admin&quot; ] disabled=yes\n\n配置PPPoE拨号和NAT规则首先创建一个PPPoE客户端来拨号：\n/interface/pppoe-client/add \\  name=&quot;PPPoE客户端名字&quot; \\  interface=WAN \\  user=&quot;宽带账号&quot; \\  password=&quot;宽带密码&quot; \\  profile=default \\  use-peer-dns=yes \\  add-default-route=yes\n\n添加成功后，RouterOS会自动开始拨号，在终端也可以用monitor命令实时监控PPPoE拨号端口的状态：\n/interface/pppoe-client/monitor [ find running ]               status: connected               uptime: 13h59m53s         active-links: 1             encoding:          service-name:               ac-name: SH-SH-ML-MSE-2.MAN.NE40E               ac-mac: D0:D0:4B:06:EC:CC                  mtu: 1492                  mru: 1492        local-address: 124.78.123.123       remote-address: 124.78.123.1   local-ipv6-address: fe80::5  remote-ipv6-address: fe80::d2d0:4bff:fe06:eccc-- [Q quit|D dump|C-z pause]\n\n这时候RouterOS自己已经可以上网了，但是路由器下面的设备还不行，因为没有配置NAT。在终端执行这条命令，来创建一条NAT规则：\n/ip/firewall/nat/add \\  chain=srcnat \\  action=masquerade \\  out-interface=&quot;PPPoE接口的名字&quot; \\  comment=NAT\n\n现在路由下的设备就可以上网了。\n配置IPv6电信宽带已经支持了IPv6，那既然有了，我不能不用。电信会给宽带分配一个前缀，下级设备通过前缀来分配自己的地址，就是prefix delegation。所以我只需要在RouterOS中配置一个IPv6的DHCP client就行。\n/ipv6/dhcp-client/add \\  interface=&quot;PPPoE客户端名字&quot; \\  add-default-route=yes \\  pool-name=ipv6_pool \\  pool-prefix-length=60 \\  prefix-hint=::/56 \\  request=prefix \\  use-interface-duid=yes\n\n然后检查DHCP client的状态，是bound的话就说明已经成功获取到地址和前缀。\n/ipv6/dhcp-client/printColumns: INTERFACE, STATUS, REQUEST, PREFIX# INTERFACE      STATUS  REQUEST  PREFIX                              0 China Telecom  bound   prefix   240e:38a:5d27:5c00::/56, 2d21h48m59s\n\n然后让电脑重新连接路由，就能看到电脑成功获取到IPv6地址了。\n配置防火墙毕竟路由器这种玩意不能在网上裸奔，该禁的端口就得禁。\n首先创建两个interface list，把公网接口和内网接口归类到各自的列表中，方便后面创建防火墙规则。\n/interface list# 给内网接口的listadd name=local# 给公网接口的listadd name=internet/interface list member# LAN口归进local这个listadd interface=LAN list=local# WAN口和PPPoE口归进internet这个listadd interface=WAN list=internetadd interface=&quot;China Telecom&quot; list=internet\n\n然后添加IPv4的防火墙规则：\n/ip firewall filter# 放行状态为established、related、untracked的数据包add action=accept chain=input comment=&quot;accept established,related,untracked&quot; connection-state=established,related,untracked# 允许外网ping，不想允许的话就不执行这条add action=accept chain=input comment=&quot;accept ping&quot; protocol=icmp# 丢弃invalid状态的包add action=drop chain=input comment=&quot;drop invalid&quot; connection-state=invalid# 兜底规则，丢弃一切来自公网的包add action=drop chain=input comment=&quot;drop all from WAN&quot; in-interface-list=internet# RouterOS的快速跟踪功能，可以跟踪已建立的连接并让这些流量走快速通道，以降低设备的CPU负载# 好像必须要专门的硬件才能发挥作用，但总之先加在这里add action=fasttrack-connection chain=forward comment=&quot;defconf: fasttrack&quot; connection-state=established,related hw-offload=yesadd action=accept chain=forward comment=&quot;accept established,related, untracked&quot; connection-state=established,related,untracked# 在forward链上也丢弃掉invalid状态的包add action=drop chain=forward comment=&quot;drop invalid&quot; connection-state=invalid# 丢弃所有来自公网，且无端口转发规则匹配的包add action=drop chain=forward comment=&quot;drop all from WAN not DSTNATed&quot; connection-nat-state=!dstnat connection-state=new in-interface-list=internet\n\n对于IPv6，要先创建两份address list，分别包含可以放行的地址和有问题的地址：\n/ipv6 firewall address-listadd address=fe80::/16 list=allowedadd address=ff02::/16 comment=multicast list=allowedadd address=::/128 comment=&quot;defconf: unspecified address&quot; list=bad_ipv6add address=::1/128 comment=&quot;defconf: lo&quot; list=bad_ipv6add address=fec0::/10 comment=&quot;defconf: site-local&quot; list=bad_ipv6add address=::ffff:0.0.0.0/96 comment=&quot;defconf: ipv4-mapped&quot; list=bad_ipv6add address=::/96 comment=&quot;defconf: ipv4 compat&quot; list=bad_ipv6add address=100::/64 comment=&quot;defconf: discard only &quot; list=bad_ipv6add address=2001:db8::/32 comment=&quot;defconf: documentation&quot; list=bad_ipv6add address=2001:10::/28 comment=&quot;defconf: ORCHID&quot; list=bad_ipv6add address=3ffe::/16 comment=&quot;defconf: 6bone&quot; list=bad_ipv6add address=::224.0.0.0/100 comment=&quot;defconf: other&quot; list=bad_ipv6add address=::127.0.0.0/104 comment=&quot;defconf: other&quot; list=bad_ipv6add address=::/104 comment=&quot;defconf: other&quot; list=bad_ipv6add address=::255.0.0.0/104 comment=&quot;defconf: other&quot; list=bad_ipv6\n\n然后配置防火墙规则（其中一些规则我也不明白是啥作用，但是既然包含在默认配置里，那配进去应该不会造成什么问题）：\n/ipv6 firewall filter# 放行状态为established、related、untracked的数据包add action=accept chain=input comment=&quot;defconf: accept established,related,untracked&quot; connection-state=established,related,untracked# 丢弃invalid状态的包add action=drop chain=input comment=&quot;defconf: drop invalid&quot; connection-state=invalid# 允许IPv6 pingadd action=accept chain=input comment=&quot;defconf: accept ICMPv6&quot; protocol=icmpv6# 允许IPv6 tracerouteadd action=accept chain=input comment=&quot;defconf: accept UDP traceroute&quot; port=33434-33534 protocol=udp# 允许DHCPv6 prefix delegation的包add action=accept chain=input comment=&quot;defconf: accept DHCPv6-Client prefix delegation.&quot; dst-port=546 protocol=udp src-address=fe80::/16# 允许IKE数据包add action=accept chain=input comment=&quot;defconf: accept IKE&quot; dst-port=500,4500 protocol=udpadd action=accept chain=input comment=&quot;defconf: accept ipsec AH&quot; protocol=ipsec-ahadd action=accept chain=input comment=&quot;defconf: accept ipsec ESP&quot; protocol=ipsec-espadd action=accept chain=input comment=&quot;defconf: accept all that matches ipsec policy&quot; ipsec-policy=in,ipsec# 兜底规则，丢弃一切不是从local接口列表发出的包add action=drop chain=input comment=&quot;defconf: drop everything else not coming from LAN&quot; in-interface-list=!local# forward链上放行状态为established、related、untracked的数据包add action=accept chain=forward comment=&quot;defconf: accept established,related,untracked&quot; connection-state=established,related,untracked# forward链上丢弃invalid状态的包add action=drop chain=forward comment=&quot;defconf: drop invalid&quot; connection-state=invalid# 禁止来自bad_ipv6这个地址列表里面的地址add action=drop chain=forward comment=&quot;defconf: drop packets with bad src ipv6&quot; src-address-list=bad_ipv6add action=drop chain=forward comment=&quot;defconf: drop packets with bad dst ipv6&quot; dst-address-list=bad_ipv6add action=drop chain=forward comment=&quot;defconf: rfc4890 drop hop-limit=1&quot; hop-limit=equal:1 protocol=icmpv6add action=accept chain=forward comment=&quot;defconf: accept ICMPv6&quot; protocol=icmpv6add action=accept chain=forward comment=&quot;defconf: accept HIP&quot; protocol=139add action=accept chain=forward comment=&quot;defconf: accept IKE&quot; dst-port=500,4500 protocol=udpadd action=accept chain=forward comment=&quot;defconf: accept ipsec AH&quot; protocol=ipsec-ahadd action=accept chain=forward comment=&quot;defconf: accept ipsec ESP&quot; protocol=ipsec-espadd action=accept chain=forward comment=&quot;defconf: accept all that matches ipsec policy&quot; ipsec-policy=in,ipsecadd action=drop chain=forward comment=&quot;defconf: drop everything else not coming from LAN&quot; in-interface-list=!local\n\nRouterOS里一些其它的配置最后就是一些杂七杂八的配置，比如开启NTP客户端来自动同步时间：\n/system ntp clientset enabled=yes/system ntp client serversadd address=time.windows.comadd address=time.nist.govadd address=time.apple.com\n\n禁用掉不需要的服务以减小攻击面：\n/ip service# 禁用telnetset telnet disabled=yes# 禁用ftpset ftp disabled=yes# 限制www、ssh、winbox服务都只能从内网访问set www address=192.168.1.0/24set ssh address=192.168.1.0/24set www-ssl address=192.168.1.0/24 disabled=noset winbox address=192.168.1.0/24# API用不到，所以禁用set api disabled=yesset api-ssl disabled=yes\n\n开启UPnP，虽然说开这玩意会有安全隐患，但是开了这么多年也没因为它出过事，这玩意也没法从外网开端口转发，也就是说首先内网得有机器中毒，我自认为我的使用习惯还算是安全的，所以，开。\n# 启用UPnP/ip upnpset enabled=yes/ip upnp interfaces# 配置LAN口作为内网端add interface=LAN type=internal# 配置PPPoE端口作为公网端add interface=&quot;PPPoE接口的名字&quot; type=external\n\n开启云服务，包括DDNS和自动备份：\n/ip cloudset ddns-enabled=yes ddns-update-interval=1h update-time=yes\n","categories":["瞎折腾"],"tags":["RouterOS","二手MicroServer Gen8折腾记","Gen8","软路由"]},{"title":"二手MicroServer Gen8折腾记(1/n) - 初始化服务器","url":"/setting-up-microserver-gen8.html","content":"上个月冲动消费，入手了一台HPE ProLiant MicroServer Gen8。其实我早想搞一台正经的服务器来玩了，因为群晖的魔改Linux总是玩的不那么爽。现在心愿实现，开始折腾。\n本篇为二手MicroServer Gen8折腾记的第一篇，记录拿到服务器之后做的一些初始化的工作。\n\n\n重置iLO到手之后，我发现我把事想简单了，服务器一般只给一个VGA口，而这台也不例外，然而，我这里没有一个能接VGA口的设备。是的，你没看错。不止显示器没有VGA口，连电视也没有。遂果断下单了一个VGA到HDMI转接器。第二天收到快递，兴冲冲的接上，却发现只能显示POST画面，之后无论是正常继续启动，还是进RBSU（ROM based setup utility），电视上都是无信号。事到如此，接显示器这条路算是堵死了。别问我为什么不买个支持VGA的显示器，你自己想想这成本划得来么？\n虽然接显示器不行，但是天无绝人之路啊，地球人都知道，正经服务器上都有一个带外管理系统，Gen 8也不例外，带有惠普的iLO（Integrated Lights-out）带外管理模块。iLO的地址在POST界面左下角有显示，\n\n但是这个地址跟我的网络不在一个网段啊？没办法，拎出来一台带网口的笔记本电脑，把电脑跟iLO直连，给电脑配置静态IP为10.132.190.34，子网掩码255.0.0.0，试一试，果然能ping通，问题解决。开玩笑，哥们可是网络工程专业的。\n接下来就要重置iLO，省得因为上任机主的配置引出什么别的麻烦。但是问题又出现了，不知道什么原因，iLO的网页不是打不开，就是报错Connection with iLO cannot be established。一顿上网冲浪之后，发现还可以用SSH连接，果断掏出putty，好在上任没改iLO的默认管理员密码，不然我就真的抓瞎了。\n登进去之后，反手就是一个Reset /map1来重置iLO的所有配置，过了一会iLO重启完毕，再次SSH登录，把DHCP打开，这样我就能给iLO分配一个我的内网的地址了。\n&lt;/&gt;hpiLO-&gt; cd /map1/dhcpendpt1status=0status_tag=COMMAND COMPLETEDTue Dec 20 15:13:47 2022/map1/dhcpendpt1&lt;/map1/dhcpendpt1&gt;hpiLO-&gt; set EnabledState=yesstatus=0status_tag=COMMAND COMPLETEDTue Dec 20 15:14:28 2022Network settings change applied.Settings change applied, iLO 4 will now be reset.Logged Out: It may take several minutes before you can log back in.CLI session stopped\n\n然后iLO会自动重启，过几分钟重启完成之后，就能在路由器里面看到iLO分配的地址了。\n更新iLO固件上面一顿操作重置iLO之后，很神奇地，iLO的web端能正常打开了。那么接下来，我就要升级iLO固件的版本，因为自带的iLO版本太低，反正自己用着玩，用新不用旧。\n到HPE Integrated Lights-Out 4 (iLO 4)这里进入Online ROM Flash Component for Windows x64 - HPE Integrated Lights-Out 4下载最新的iLO 4固件。目前最新版是2.81。下载好安装程序之后，打开运行，选择Extract把安装包解压缩到一个地方。Install是不能用的，因为只能在服务器上才能自动安装。解压后可以看到一个叫ilo4_281.bin的文件，这个就是iLO的固件。\n进入iLO控制台的Administration -&gt; Firmware页面，File里面选择ilo4_281.bin这个文件。\n\n然后点Upload，静等固件上传和更新完成即可。期间不要刷新页面或者关掉页面内的进度条弹窗，地球人都知道，刷机期间手贱就是找事。\n\n修复iLO降级iLO重启之后，我看到一条报错说iLO Self-Test reports a problem with: Embedded Flash/SD-CARD，在网上一顿冲浪之后，说可以尝试在Diagnostics页面里Reset iLO来修复。\n\n但是我在Reset之后，问题依旧存在，按照网上的说法来看，应该是硬件有问题，要更换板载Flash了。\n更新：发现了另一篇博客[Fix] ILO HP Error – Embedded Flash&#x2F;SD-CARD: Embedded media manager failed initialization，说强制格式化NAND可以修复，我试了下，格式化并重启之后，一开始虽然显示正常，但几分钟后依旧会报同样错误。\n此外根据HPE官方的advisory：\n\nStep 1) Upgrade the iLO 4 firmware to version 2.61Step 2) Perform a NAND formatStep 3) Check the iLO status If the iLO status is normal, then skip to Step 6 If the iLO status is still degraded, continue to Step 4Step 4) Schedule downtime; AC power-cycle and repeat the NAND formatStep 5) Check the iLO status If the iLO status is normal, continue toStep 6 If the iLO status is still degraded, then skip to Step 7Step 6) Perform these final steps if the system board does not need to be replaced: Reboot the server; reinstall IP; and refresh the server in OneView (if server is managed by OneView)Step 7) If steps 1-4 did not resolve the degraded iLO, replace the system board.\n\n确实走到了Step 7，只能replace the system board。╮(╯▽╰)╭\n配置RAID\n这一部分因为不方便复现，故使用HP MicroServer Gen8不疼使用教程 - Eddie’s Blog的文章配图用作参考。向Eddie表示由衷的感谢！\n\n我在3和4盘位插了两块1TB的固态硬盘，1和2盘位暂时空下来。这么做的原因是，1和2盘位是SATA-3，3和4盘位是SATA-2，所以我打算把更快的接口留给日后拓展更大容量的机械硬盘，而固态硬盘本身读写速度就很快，插在一个慢点的接口上也不会造成很大的性能问题。\n插硬盘之前要先关机，因为Gen 8的硬盘位是不支持热插拔的。插好硬盘后开机，在POST界面出现HP Dynamic Smart Array B120i的初始化信息时敲F5来进入RAID配置页面。这里可以多敲几次来保证成功率，就像进BIOS敲del那样。如果进入成功，就会看到一个写着HP的大球，这时候系统正在加载RAID卡的配置界面。\n\n进来之后，就可以点Dynamic Smart Array B120i RAID页面的Configure按钮来配置RAID了。\n\n装ESXi前面洋洋洒洒整了一大堆，但那都是前置任务。到这一步开始主线，装系统。Gen 8主板上带了一个TF卡槽和一个USB 2.0接口。起初我打算把ESXi装在TF卡里面的，但是试了一张之前用过的老卡，安装慢的令人发指，又试了一张几个月前买的卡，结果ESXi安装程序发现不了，怀疑是系统没识别。我懒得折腾卡了，遂拿出一个闲置的U盘，插在了主板的USB口上。\n\n至于ESXi系统，我选择了惠普定制的版本，毕竟不知道VMWare官方版跟定制版差别在哪，还不如直接选厂商定制的版本来的方便。镜像可以到这里下载，记得要选Pre-Gen9 Custom Image。下载好Install CD之后，进入iLO的远程控制台，HTML5的就行，然后点击标题栏的光盘按钮，选CD/DVD - Local *.iso file，就可以把ESXi的安装盘插进服务器的虚拟光驱。\n\n打开服务器电源之后，就会正常从光盘启动进入ESXi的安装程序。安装步骤很简单，这里就不再赘述了，唯一要注意的就是选对安装位置，因为安装程序会把一整块盘当作系统盘，所以我个人建议把它装在TF卡或者U盘里。也不用担心TF卡或者U盘会降低系统性能，我这里实测没有什么明显难忍的性能问题。\n换阵列卡驱动，更新系统\n这里参考了第三篇：虚拟化层 - MicroServer Gen8 + ESXi Server 6.5u3 部署踩坑记 - 天一先生的游牧生活，这里向作者表示感谢！\n\n装好ESXi之后，我先装上了Ubuntu Server 22.10和Windows Server 2019 Datacenter。用了一段时间，总感觉有点奇怪，尽管说这是好多年前的机器，性能不强我是有心理准备的，但也不至于点个开始菜单都要好几秒，打开资源管理器也要将近半分钟。今天等待程序载入的时候看了眼任务管理器，发现磁盘占用率竟然100%，要知道我这是两块固态硬盘组的阵列诶！既然有问题，就上网搜一搜呗，这不冲浪不知道，一冲浪吓一跳，惠普给ESXi 6.x打包的B120i板载阵列卡驱动有性能问题，会导致磁盘读写异常缓慢。针对这个问题，网上最普遍的一个解决方案就是把阵列卡驱动退回到ESXi 5.x中的最后一个版本5.5.0-88。\n那话不多说，搞它！\n首先，把所有虚拟机关机，全部关机后，在ESXi的主机页面里，点击操作菜单中的进入维护模式；在点击操作菜单，进入服务，选择启用 Secure Shell(SSH)，因为后面要SSH进ESXi里面安装驱动。\n下载http://downloads.hpe.com/pub/softlib2/software1/pubsw-linux/p1749737361/v98767/hpvsa-5.5.0-88.zip，完成后，解压这个ZIP包，得到驱动本体scsi-hpvsa-5.5.0-88OEM.550.0.0.1331820.x86_64.vib。将这个vib文件上传到ESXi的/var/log/vmware目录下。上传成功后，SSH进入ESXi，执行如下命令：\n# 卸载当前版本的驱动esxcli software vib remove -n scsi-hpvsa -f# 安装5.5.0-88版驱动esxcli software vib install -v file:scsi-hpvsa-5.5.0-88OEM.550.0.0.1331820.x86_64.vib --force --no-sig-check --maintenance-mode# 重启ESXireboot\n\n重启完了，进ESXi看了眼驱动，发现RAID也没认出来，驱动也变成vmw_achi了，这是没认出来阵列卡驱动啊。搜了一下，发现可以强制禁用vmw_ahci，于是再进SSH，执行esxcli system module set --enabled=false --module=vmw_ahci这条命令，再重启之后，RAID回来了，驱动也变成hpvsa了。\n\n如果你在禁用vmw_ahci之后还不能成功加载hpvsa驱动，那么就得先执行esxcli software vib remove -n scsi-hpvsa -f来卸掉它，重启之后再重装。这时候千万不能操作硬盘，否则可能造成数据损毁。\n换驱动的时候了解到，VMWare也会给ESXi推出一些系统修正补丁，那有补丁当然要打啊。一开始我想着参照ESXi 6.5 - VMware ESXi Patch Tracker这个网站给的命令在线更新，但是一直报Got no data from process这个错误，VMWare Communities上面也有人遇到同样的错误，解决方法是下载补丁然后手动安装。\n\n关于Got no data from process这个错误，根据How to Upgrade from VMware vSphere ESXi 6.7 to 7.0中的描述：\n\nIf you see the error message:\nGot no data from process: LANG&#x3D;en_US.UTF-8 &#x2F;usr&#x2F;lib&#x2F;vmware&#x2F;esxcli-software profile.update  -d “https://hostupdate.vmware.com/software/VUM/PRODUCTION/main/vmw-depot-index.xml“ -p “ESXi-7.0.0-15843807-standard”,\ncheck whether there is a persistent scratch partition. There may not be a persistent scratch partition if you’re running ESXi from a USB flash drive. \n\n而根据How to Run ESXi from a USB Flash Drive所说：\n\nIf ESXi is installed on a USB flash drive or SD card whose size is 8 GB or more, partitions #2 and #3 are not created. The Scratch directory used to store logs is linked to the &#x2F;tmp&#x2F;scratch directory that is located on the RAM drive. \n\n也就是说，如果把ESXi装在大于等于8GB的TF卡或者U盘里面的话，就会缺少一个scratch分区。\n另外根据这个分区图，我还发现，如果把ESXi装在硬盘里的话，它会把剩余空间作为数据存储，而不是像网上一些文章说的只用来装系统，这个我后面有时间会再测试一下。\n\n那么，首先到Product Patches这里，Product选ESXi (Embeeded and Installable)，Version选6.5.0（如果你装的不一样，以你为准），点Search后会列出一堆补丁，选最新那个，点Download Now来下载补丁的ZIP包。然后点View Details，我们要在详细信息页面找到Image Profile的名字。\n\n\n关于什么是Image Profile：VMWare文档说：Image profiles define the set of VIBs that an ESXi installation or update process uses.也就是说，Image Profile定义了包里的哪些VIB会被安装。每个补丁包中都会带有2~4个Image Profile，根据VMware ESXi Image Profiles的描述，各个Image Profile的含义是：\n\nESXi-5.5.0-20140704001-standard - 包含全部补丁\nESXi-5.5.0-20140704001-no-tools - 包含除了VMware Tools之外的全部补丁\nESXi-5.5.0-20140701001s-standard - 只包含安全补丁\nESXi-5.5.0-20140701001s-no-tools - 只包含安全补丁，同时没有VMware Tools\n\n\n因为我要更新所有的VIB，所以我将要用ESXi-6.5.0-20221004001-standard这个Image Profile。SSH进ESXi，执行esxcli software profile update -p ESXi-6.5.0-20221004001-standard -d /vmfs/volumes/63a1b271-3340b157-711e-d0bf9c4694d8/ESXi650-202210001.zip开始安装（记得把-d参数指定的绝对路径改为你的机器上面的路径）。安装过程比较耗时间，这期间可以tail -f /var/log/esxupdate.log来看ESXi正在干什么，毕竟看着日志在跑，心里也有底，不然总害怕系统是不是死掉了。\n更新成功后，esxcli会打出来哪些包被更新了，但是实在是太多，我根本懒得看，直接重启。重启之后，ESXi能正常进系统，存储和虚拟机都正常载入，说明没出啥毛病。这时候就可以退出维护模式，给虚拟机开机。有一说一，现在不管是开机还是打开什么东西，反应确实快了不少，看来驱动导致的硬盘性能问题已经成功解决。\n未完待续\n","categories":["瞎折腾"],"tags":["二手MicroServer Gen8折腾记","Gen8"]},{"title":"Shell脚本中 set -ex 命令的作用","url":"/shell-set-ex.html","content":"刚刚学会了一个很实用的shell命令set -ex，在这里分享一下。\n\n\n稍有常识的人都能看出，这是set命令加上了-e和-x两个参数(废话么这不是)。那么，我就把这两个参数拆开，分别说一下它在脚本中的用处。\nset -e先说说set -e，这个参数的含义是，当命令发生错误的时候，停止脚本的执行。\n通常来说，我们会习惯于使用&amp;&amp;来实现这样的功能，比如：\n#!/bin/bashecho 1 &amp;&amp; rm non-existent-file &amp;&amp; echo 2\n\n但是，写成一行呢，可读性有点差，分成多行的话，也得注意换行符\\和&amp;&amp;号，我就有过好几次忘了加这俩东西，还是挺麻烦的是吧。\n更麻烦的是，&amp;&amp;连接的命令之间不能写注释，也就是说，下面这个示例是不能用的：\n#!/bin/bashecho 1 \\  &amp;&amp; rm non-existent-file \\ # which will fail  &amp;&amp; echo 2\n\n运行之后会是这个德行：\n1rm: non-existent-file: No such file or directoryrm:  #: No such file or directoryrm: which: No such file or directoryrm: will: No such file or directoryrm: fail: No such file or directory./test1.sh: line 5: syntax error near unexpected token `&amp;&amp;&#x27;./test1.sh: line 5: `  &amp;&amp; echo 2&#x27;\n\n现在，就可以写成下面这样了：\n#!/bin/bashset -eecho 1rm non-existent-file # which will failecho 2\n\n猜猜最后输出里面会不会把2打印出来？\nset -x说完了-e，继续说说-x。-x参数的作用，是把将要运行的命令用一个+标记之后显示出来。\n还是拿上面这个脚本举个例子，这次加上-x：\n#!/bin/bashset -execho 1rm non-existent-file # which will failecho 2\n\n然后它的输出就变成了：\n+ echo 11+ rm non-existent-filerm: non-existent-file: No such file or directory\n\n注意第一行和第三行前面那个+，这就是-x参数的作用。\nOne more thing……需要注意，这条命令需要放到整个shell脚本的开头，才会起作用。毕竟用脑子想想就知道，这是俩开关，不论放在中间还是结尾，都不会起到预期的作用。\nCredit感谢这篇文章set -ex - The most useful bash trick of the year为我提供了这条命令的解释，和写作的思路。\n","categories":["学知识"],"tags":["Shell"]},{"title":"Spring Boot启动过程中的后置处理","url":"/spring-boot-application-start-up-post-processing.html","content":"在前一篇博文Spring Boot启动流程分析的调用 beanFactory 的后置处理小节中提到了beanFactory的后置处理。这部分是Spring IoC的重点，但是因为内容很长，所以在这里单独开一篇博文来分析。\n\n\n调用beanFactoryPostProcessors话不多说，继续从AbstractApplicationContext#refresh调用invokeBeanFactoryPostProcessors方法这里开始。\nprotected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) &#123;    // 重点    PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors());    // Detect a LoadTimeWeaver and prepare for weaving, if found in the meantime    // (e.g. through an @Bean method registered by ConfigurationClassPostProcessor)    if (!NativeDetector.inNativeImage() &amp;&amp; beanFactory.getTempClassLoader() == null &amp;&amp;            beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123;        beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory));        beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader()));    &#125;&#125;// PostProcessorRegistrationDelegatepublic static void invokeBeanFactoryPostProcessors(        ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123;    // WARNING: Although it may appear that the body of this method can be easily    // refactored to avoid the use of multiple loops and multiple lists, the use    // of multiple lists and multiple passes over the names of processors is    // intentional. We must ensure that we honor the contracts for PriorityOrdered    // and Ordered processors. Specifically, we must NOT cause processors to be    // instantiated (via getBean() invocations) or registered in the ApplicationContext    // in the wrong order.    //    // Before submitting a pull request (PR) to change this method, please review the    // list of all declined PRs involving changes to PostProcessorRegistrationDelegate    // to ensure that your proposal does not result in a breaking change:    // https://github.com/spring-projects/spring-framework/issues?q=PostProcessorRegistrationDelegate+is%3Aclosed+label%3A%22status%3A+declined%22    // Invoke BeanDefinitionRegistryPostProcessors first, if any.    Set&lt;String&gt; processedBeans = new HashSet&lt;&gt;();    if (beanFactory instanceof BeanDefinitionRegistry registry) &#123;        List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new ArrayList&lt;&gt;();        List&lt;BeanDefinitionRegistryPostProcessor&gt; registryProcessors = new ArrayList&lt;&gt;();        for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123;            if (postProcessor instanceof BeanDefinitionRegistryPostProcessor registryProcessor) &#123;                registryProcessor.postProcessBeanDefinitionRegistry(registry);                registryProcessors.add(registryProcessor);            &#125;            else &#123;                regularPostProcessors.add(postProcessor);            &#125;        &#125;        // Do not initialize FactoryBeans here: We need to leave all regular beans        // uninitialized to let the bean factory post-processors apply to them!        // Separate between BeanDefinitionRegistryPostProcessors that implement        // PriorityOrdered, Ordered, and the rest.        List&lt;BeanDefinitionRegistryPostProcessor&gt; currentRegistryProcessors = new ArrayList&lt;&gt;();        // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered.        String[] postProcessorNames =                beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);        for (String ppName : postProcessorNames) &#123;            if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123;                currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));                processedBeans.add(ppName);            &#125;        &#125;        sortPostProcessors(currentRegistryProcessors, beanFactory);        registryProcessors.addAll(currentRegistryProcessors);        // 这里是重点        invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup());        currentRegistryProcessors.clear();        // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered.        postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);        for (String ppName : postProcessorNames) &#123;            if (!processedBeans.contains(ppName) &amp;&amp; beanFactory.isTypeMatch(ppName, Ordered.class)) &#123;                currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));                processedBeans.add(ppName);            &#125;        &#125;        sortPostProcessors(currentRegistryProcessors, beanFactory);        registryProcessors.addAll(currentRegistryProcessors);        invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup());        currentRegistryProcessors.clear();        // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear.        boolean reiterate = true;        while (reiterate) &#123;            reiterate = false;            postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);            for (String ppName : postProcessorNames) &#123;                if (!processedBeans.contains(ppName)) &#123;                    currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));                    processedBeans.add(ppName);                    reiterate = true;                &#125;            &#125;            sortPostProcessors(currentRegistryProcessors, beanFactory);            registryProcessors.addAll(currentRegistryProcessors);            invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry, beanFactory.getApplicationStartup());            currentRegistryProcessors.clear();        &#125;        // Now, invoke the postProcessBeanFactory callback of all processors handled so far.        invokeBeanFactoryPostProcessors(registryProcessors, beanFactory);        invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory);    &#125;    else &#123;        // Invoke factory processors registered with the context instance.        invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory);    &#125;    // Do not initialize FactoryBeans here: We need to leave all regular beans    // uninitialized to let the bean factory post-processors apply to them!    String[] postProcessorNames =            beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false);    // Separate between BeanFactoryPostProcessors that implement PriorityOrdered,    // Ordered, and the rest.    List&lt;BeanFactoryPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;();    List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;();    List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;();    for (String ppName : postProcessorNames) &#123;        if (processedBeans.contains(ppName)) &#123;            // skip - already processed in first phase above        &#125;        else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) &#123;            priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class));        &#125;        else if (beanFactory.isTypeMatch(ppName, Ordered.class)) &#123;            orderedPostProcessorNames.add(ppName);        &#125;        else &#123;            nonOrderedPostProcessorNames.add(ppName);        &#125;    &#125;    // First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered.    sortPostProcessors(priorityOrderedPostProcessors, beanFactory);    invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory);    // Next, invoke the BeanFactoryPostProcessors that implement Ordered.    List&lt;BeanFactoryPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size());    for (String postProcessorName : orderedPostProcessorNames) &#123;        orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));    &#125;    sortPostProcessors(orderedPostProcessors, beanFactory);    invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory);    // Finally, invoke all other BeanFactoryPostProcessors.    List&lt;BeanFactoryPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size());    for (String postProcessorName : nonOrderedPostProcessorNames) &#123;        nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));    &#125;    invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory);    // Clear cached merged bean definitions since the post-processors might have    // modified the original metadata, e.g. replacing placeholders in values...    beanFactory.clearMetadataCache();&#125;\n\n这一大段代码，实话说看得我云里雾里的，但站在巨人的肩膀上之后，我大概明白invokeBeanDefinitionRegistryPostProcessors这个方法调用是里面的重点，那接下来就随着巨人的指引看看这个方法。\nprivate static void invokeBeanDefinitionRegistryPostProcessors(        Collection&lt;? extends BeanDefinitionRegistryPostProcessor&gt; postProcessors, BeanDefinitionRegistry registry, ApplicationStartup applicationStartup) &#123;    for (BeanDefinitionRegistryPostProcessor postProcessor : postProcessors) &#123;        StartupStep postProcessBeanDefRegistry = applicationStartup.start(&quot;spring.context.beandef-registry.post-process&quot;)                .tag(&quot;postProcessor&quot;, postProcessor::toString);        postProcessor.postProcessBeanDefinitionRegistry(registry);        postProcessBeanDefRegistry.end();    &#125;&#125;\n\ndebug到postProcessBeanDefinitionRegistry这一行，发现postProcessors里面只有ConfigurationClassPostProcessor，那么就进去看看这个类的postProcessBeanDefinitionRegistry在干什么。\n下面代码会大量提到configuration class这个概念，而所谓configuration class，就是带有@Configuration或者@Component注解的类。\n@Overridepublic void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123;    int registryId = System.identityHashCode(registry);    if (this.registriesPostProcessed.contains(registryId)) &#123;        throw new IllegalStateException(                &quot;postProcessBeanDefinitionRegistry already called on this post-processor against &quot; + registry);    &#125;    if (this.factoriesPostProcessed.contains(registryId)) &#123;        throw new IllegalStateException(                &quot;postProcessBeanFactory already called on this post-processor against &quot; + registry);    &#125;    this.registriesPostProcessed.add(registryId);    processConfigBeanDefinitions(registry);&#125;public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) &#123;    List&lt;BeanDefinitionHolder&gt; configCandidates = new ArrayList&lt;&gt;();    String[] candidateNames = registry.getBeanDefinitionNames();    // 遍历所有的bean definition的名字    for (String beanName : candidateNames) &#123;        BeanDefinition beanDef = registry.getBeanDefinition(beanName);        if (beanDef.getAttribute(ConfigurationClassUtils.CONFIGURATION_CLASS_ATTRIBUTE) != null) &#123;            if (logger.isDebugEnabled()) &#123;                logger.debug(&quot;Bean definition has already been processed as a configuration class: &quot; + beanDef);            &#125;        &#125;        // 检查这个bean definition是不是一个配置类        // 1. 取得bean definition的metadata        // 2. 设定configurationClass这个bean definition attribute的值，full或lite（暂不清楚是干什么用的）        // 3. 如果指定了这个配置类的生效顺序，那么将其记录到order这个attribute中        else if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) &#123;            configCandidates.add(new BeanDefinitionHolder(beanDef, beanName));        &#125;    &#125;    // Return immediately if no @Configuration classes were found    if (configCandidates.isEmpty()) &#123;        return;    &#125;    // Sort by previously determined @Order value, if applicable    configCandidates.sort((bd1, bd2) -&gt; &#123;        int i1 = ConfigurationClassUtils.getOrder(bd1.getBeanDefinition());        int i2 = ConfigurationClassUtils.getOrder(bd2.getBeanDefinition());        return Integer.compare(i1, i2);    &#125;);    // Detect any custom bean name generation strategy supplied through the enclosing application context    SingletonBeanRegistry sbr = null;    if (registry instanceof SingletonBeanRegistry _sbr) &#123;        sbr = _sbr;        if (!this.localBeanNameGeneratorSet) &#123;            BeanNameGenerator generator = (BeanNameGenerator) sbr.getSingleton(                    AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR);            if (generator != null) &#123;                this.componentScanBeanNameGenerator = generator;                this.importBeanNameGenerator = generator;            &#125;        &#125;    &#125;    if (this.environment == null) &#123;        this.environment = new StandardEnvironment();    &#125;    // Parse each @Configuration class    ConfigurationClassParser parser = new ConfigurationClassParser(            this.metadataReaderFactory, this.problemReporter, this.environment,            this.resourceLoader, this.componentScanBeanNameGenerator, registry);    Set&lt;BeanDefinitionHolder&gt; candidates = new LinkedHashSet&lt;&gt;(configCandidates);    Set&lt;ConfigurationClass&gt; alreadyParsed = new HashSet&lt;&gt;(configCandidates.size());    do &#123;        StartupStep processConfig = this.applicationStartup.start(&quot;spring.context.config-classes.parse&quot;);        parser.parse(candidates);        parser.validate();        Set&lt;ConfigurationClass&gt; configClasses = new LinkedHashSet&lt;&gt;(parser.getConfigurationClasses());        configClasses.removeAll(alreadyParsed);        // Read the model and create bean definitions based on its content        if (this.reader == null) &#123;            this.reader = new ConfigurationClassBeanDefinitionReader(                    registry, this.sourceExtractor, this.resourceLoader, this.environment,                    this.importBeanNameGenerator, parser.getImportRegistry());        &#125;        this.reader.loadBeanDefinitions(configClasses);        alreadyParsed.addAll(configClasses);        processConfig.tag(&quot;classCount&quot;, () -&gt; String.valueOf(configClasses.size())).end();        candidates.clear();        if (registry.getBeanDefinitionCount() &gt; candidateNames.length) &#123;            String[] newCandidateNames = registry.getBeanDefinitionNames();            Set&lt;String&gt; oldCandidateNames = Set.of(candidateNames);            Set&lt;String&gt; alreadyParsedClasses = new HashSet&lt;&gt;();            for (ConfigurationClass configurationClass : alreadyParsed) &#123;                alreadyParsedClasses.add(configurationClass.getMetadata().getClassName());            &#125;            for (String candidateName : newCandidateNames) &#123;                if (!oldCandidateNames.contains(candidateName)) &#123;                    BeanDefinition bd = registry.getBeanDefinition(candidateName);                    if (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) &amp;&amp;                            !alreadyParsedClasses.contains(bd.getBeanClassName())) &#123;                        candidates.add(new BeanDefinitionHolder(bd, candidateName));                    &#125;                &#125;            &#125;            candidateNames = newCandidateNames;        &#125;    &#125;    while (!candidates.isEmpty());    // Register the ImportRegistry as a bean in order to support ImportAware @Configuration classes    if (sbr != null &amp;&amp; !sbr.containsSingleton(IMPORT_REGISTRY_BEAN_NAME)) &#123;        sbr.registerSingleton(IMPORT_REGISTRY_BEAN_NAME, parser.getImportRegistry());    &#125;    // Store the PropertySourceDescriptors to contribute them Ahead-of-time if necessary    this.propertySourceDescriptors = parser.getPropertySourceDescriptors();    if (this.metadataReaderFactory instanceof CachingMetadataReaderFactory cachingMetadataReaderFactory) &#123;        // Clear cache in externally provided MetadataReaderFactory; this is a no-op        // for a shared cache since it&#x27;ll be cleared by the ApplicationContext.        cachingMetadataReaderFactory.clearCache();    &#125;&#125;\n\n首先这里会遍历所有的bean definition，找到其中的配置类，然后调用parse方法解析它们。\npublic void parse(Set&lt;BeanDefinitionHolder&gt; configCandidates) &#123;    for (BeanDefinitionHolder holder : configCandidates) &#123;        BeanDefinition bd = holder.getBeanDefinition();        try &#123;            // 如果是从Spring Boot项目进来的，那么这里的bd就是主类封装成的AnnotatedGenericBeanDefinition            if (bd instanceof AnnotatedBeanDefinition annotatedBeanDef) &#123;                parse(annotatedBeanDef.getMetadata(), holder.getBeanName());            &#125;            else if (bd instanceof AbstractBeanDefinition abstractBeanDef &amp;&amp; abstractBeanDef.hasBeanClass()) &#123;                parse(abstractBeanDef.getBeanClass(), holder.getBeanName());            &#125;            else &#123;                parse(bd.getBeanClassName(), holder.getBeanName());            &#125;        &#125;        catch (BeanDefinitionStoreException ex) &#123;            throw ex;        &#125;        catch (Throwable ex) &#123;            throw new BeanDefinitionStoreException(                    &quot;Failed to parse configuration class [&quot; + bd.getBeanClassName() + &quot;]&quot;, ex);        &#125;    &#125;    // 加载默认的配置，即自动装配的入口    this.deferredImportSelectorHandler.process();&#125;protected final void parse(AnnotationMetadata metadata, String beanName) throws IOException &#123;    processConfigurationClass(new ConfigurationClass(metadata, beanName), DEFAULT_EXCLUSION_FILTER);&#125;protected void processConfigurationClass(ConfigurationClass configClass, Predicate&lt;String&gt; filter) throws IOException &#123;    if (this.conditionEvaluator.shouldSkip(configClass.getMetadata(), ConfigurationPhase.PARSE_CONFIGURATION)) &#123;        return;    &#125;    ConfigurationClass existingClass = this.configurationClasses.get(configClass);    if (existingClass != null) &#123;        if (configClass.isImported()) &#123;            if (existingClass.isImported()) &#123;                existingClass.mergeImportedBy(configClass);            &#125;            // Otherwise ignore new imported config class; existing non-imported class overrides it.            return;        &#125;        else &#123;            // Explicit bean definition found, probably replacing an import.            // Let&#x27;s remove the old one and go with the new one.            this.configurationClasses.remove(configClass);            this.knownSuperclasses.values().removeIf(configClass::equals);        &#125;    &#125;    // Recursively process the configuration class and its superclass hierarchy.    // 递归处理当前的configuration class及其父类    SourceClass sourceClass = null;    try &#123;        sourceClass = asSourceClass(configClass, filter);        do &#123;            sourceClass = doProcessConfigurationClass(configClass, sourceClass, filter);        &#125;        while (sourceClass != null);    &#125;    catch (IOException ex) &#123;        throw new BeanDefinitionStoreException(                &quot;I/O failure while processing configuration class [&quot; + sourceClass + &quot;]&quot;, ex);    &#125;    this.configurationClasses.put(configClass, configClass);&#125;// ConfigurationClassParser@Nullableprotected final SourceClass doProcessConfigurationClass(        ConfigurationClass configClass, SourceClass sourceClass, Predicate&lt;String&gt; filter)        throws IOException &#123;    // 如果这个configuration class带有@Component注解    // 那么就递归处理其内部类    if (configClass.getMetadata().isAnnotated(Component.class.getName())) &#123;        // Recursively process any member (nested) classes first        processMemberClasses(configClass, sourceClass, filter);    &#125;    // Process any @PropertySource annotations    // 针对@PropertySource注解的处理    for (AnnotationAttributes propertySource : AnnotationConfigUtils.attributesForRepeatable(            sourceClass.getMetadata(), org.springframework.context.annotation.PropertySource.class,            PropertySources.class, true)) &#123;        if (this.propertySourceRegistry != null) &#123;            this.propertySourceRegistry.processPropertySource(propertySource);        &#125;        else &#123;            logger.info(&quot;Ignoring @PropertySource annotation on [&quot; + sourceClass.getMetadata().getClassName() +                    &quot;]. Reason: Environment must implement ConfigurableEnvironment&quot;);        &#125;    &#125;    // Search for locally declared @ComponentScan annotations first.    // 根据@ComponentScan注解扫描项目中的bean    Set&lt;AnnotationAttributes&gt; componentScans = AnnotationConfigUtils.attributesForRepeatable(            sourceClass.getMetadata(), ComponentScan.class, ComponentScans.class,            MergedAnnotation::isDirectlyPresent);    // Fall back to searching for @ComponentScan meta-annotations (which indirectly    // includes locally declared composed annotations).    // 如果没找到直接使用的@ComponentScan注解，那么就尝试找间接的引用    // 如 @SpringBootApplication -&gt; @ComponentScan    if (componentScans.isEmpty()) &#123;        componentScans = AnnotationConfigUtils.attributesForRepeatable(sourceClass.getMetadata(),                ComponentScan.class, ComponentScans.class, MergedAnnotation::isMetaPresent);    &#125;    if (!componentScans.isEmpty() &amp;&amp;            !this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) &#123;        for (AnnotationAttributes componentScan : componentScans) &#123;            // The config class is annotated with @ComponentScan -&gt; perform the scan immediately            // 在这里立即开始扫描            Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions =                    this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName());            // Check the set of scanned definitions for any further config classes and parse recursively if needed            for (BeanDefinitionHolder holder : scannedBeanDefinitions) &#123;                BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition();                if (bdCand == null) &#123;                    bdCand = holder.getBeanDefinition();                &#125;                // 检查扫描到的这个bean是不是一个configuration class                // 如果是，那么递归查找与它关脸的configuration class，如@Bean定义的bean，或带有@Import注解的@Component类                if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) &#123;                    parse(bdCand.getBeanClassName(), holder.getBeanName());                &#125;            &#125;        &#125;    &#125;    // Process any @Import annotations    // 递归处理@Import注解    processImports(configClass, sourceClass, getImports(sourceClass), filter, true);    // Process any @ImportResource annotations    AnnotationAttributes importResource =            AnnotationConfigUtils.attributesFor(sourceClass.getMetadata(), ImportResource.class);    if (importResource != null) &#123;        String[] resources = importResource.getStringArray(&quot;locations&quot;);        Class&lt;? extends BeanDefinitionReader&gt; readerClass = importResource.getClass(&quot;reader&quot;);        for (String resource : resources) &#123;            String resolvedResource = this.environment.resolveRequiredPlaceholders(resource);            configClass.addImportedResource(resolvedResource, readerClass);        &#125;    &#125;    // Process individual @Bean methods    // 处理带有@Bean的方法    Set&lt;MethodMetadata&gt; beanMethods = retrieveBeanMethodMetadata(sourceClass);    for (MethodMetadata methodMetadata : beanMethods) &#123;        configClass.addBeanMethod(new BeanMethod(methodMetadata, configClass));    &#125;    // Process default methods on interfaces    processInterfaces(configClass, sourceClass);    // Process superclass, if any    if (sourceClass.getMetadata().hasSuperClass()) &#123;        String superclass = sourceClass.getMetadata().getSuperClassName();        if (superclass != null &amp;&amp; !superclass.startsWith(&quot;java&quot;) &amp;&amp;                !this.knownSuperclasses.containsKey(superclass)) &#123;            this.knownSuperclasses.put(superclass, configClass);            // Superclass found, return its annotation metadata and recurse            return sourceClass.getSuperClass();        &#125;    &#125;    // No superclass -&gt; processing is complete    return null;&#125;\n\nResource定位这里的重点是Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions = this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName());这个方法调用，它负责处理扫描的具体过程，所以点进parse方法看它干了什么。\npublic Set&lt;BeanDefinitionHolder&gt; parse(AnnotationAttributes componentScan, String declaringClass) &#123;    // 取得扫描器，并设置它的各个属性    ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(this.registry,            componentScan.getBoolean(&quot;useDefaultFilters&quot;), this.environment, this.resourceLoader);    Class&lt;? extends BeanNameGenerator&gt; generatorClass = componentScan.getClass(&quot;nameGenerator&quot;);    boolean useInheritedGenerator = (BeanNameGenerator.class == generatorClass);    scanner.setBeanNameGenerator(useInheritedGenerator ? this.beanNameGenerator :            BeanUtils.instantiateClass(generatorClass));    ScopedProxyMode scopedProxyMode = componentScan.getEnum(&quot;scopedProxy&quot;);    if (scopedProxyMode != ScopedProxyMode.DEFAULT) &#123;        scanner.setScopedProxyMode(scopedProxyMode);    &#125;    else &#123;        Class&lt;? extends ScopeMetadataResolver&gt; resolverClass = componentScan.getClass(&quot;scopeResolver&quot;);        scanner.setScopeMetadataResolver(BeanUtils.instantiateClass(resolverClass));    &#125;    scanner.setResourcePattern(componentScan.getString(&quot;resourcePattern&quot;));    for (AnnotationAttributes includeFilterAttributes : componentScan.getAnnotationArray(&quot;includeFilters&quot;)) &#123;        List&lt;TypeFilter&gt; typeFilters = TypeFilterUtils.createTypeFiltersFor(includeFilterAttributes, this.environment,                this.resourceLoader, this.registry);        for (TypeFilter typeFilter : typeFilters) &#123;            scanner.addIncludeFilter(typeFilter);        &#125;    &#125;    for (AnnotationAttributes excludeFilterAttributes : componentScan.getAnnotationArray(&quot;excludeFilters&quot;)) &#123;        List&lt;TypeFilter&gt; typeFilters = TypeFilterUtils.createTypeFiltersFor(excludeFilterAttributes, this.environment,            this.resourceLoader, this.registry);        for (TypeFilter typeFilter : typeFilters) &#123;            scanner.addExcludeFilter(typeFilter);        &#125;    &#125;    boolean lazyInit = componentScan.getBoolean(&quot;lazyInit&quot;);    if (lazyInit) &#123;        scanner.getBeanDefinitionDefaults().setLazyInit(true);    &#125;    Set&lt;String&gt; basePackages = new LinkedHashSet&lt;&gt;();    // 取得basePackages的值，默认情况下为空，因为@SpringBootApplication里面就没给@ComponentScan配置这个值    String[] basePackagesArray = componentScan.getStringArray(&quot;basePackages&quot;);    for (String pkg : basePackagesArray) &#123;        String[] tokenized = StringUtils.tokenizeToStringArray(this.environment.resolvePlaceholders(pkg),                ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS);        Collections.addAll(basePackages, tokenized);    &#125;    // 通过basePackageClasses里面指定的类找到它们所在的包，    // 并将其加入basePackages    // 默认情况下这个也是空的    for (Class&lt;?&gt; clazz : componentScan.getClassArray(&quot;basePackageClasses&quot;)) &#123;        basePackages.add(ClassUtils.getPackageName(clazz));    &#125;    // 查到最后basePackages还是空的话    // 那就把带着@ComponentScan这个注解的类所在的包加到basePackages里面    // 因为启动类带着这个注解，所以启动类所在的包就会被加进去    if (basePackages.isEmpty()) &#123;        basePackages.add(ClassUtils.getPackageName(declaringClass));    &#125;    scanner.addExcludeFilter(new AbstractTypeHierarchyTraversingFilter(false, false) &#123;        @Override        protected boolean matchClassName(String className) &#123;            return declaringClass.equals(className);        &#125;    &#125;);    // 根据basePackages扫描    return scanner.doScan(StringUtils.toStringArray(basePackages));&#125;\n\n走到scanner.doScan的时候，就完成了定位这一步。接下来继续走进ClassPathBeanDefinitionScanner#doScan看它是怎么载入和注册各个BeanDefinition的。\nprotected Set&lt;BeanDefinitionHolder&gt; doScan(String... basePackages) &#123;    Assert.notEmpty(basePackages, &quot;At least one base package must be specified&quot;);    Set&lt;BeanDefinitionHolder&gt; beanDefinitions = new LinkedHashSet&lt;&gt;();    for (String basePackage : basePackages) &#123;        // 从指定的包中扫描需要装载的Bean        Set&lt;BeanDefinition&gt; candidates = findCandidateComponents(basePackage);        for (BeanDefinition candidate : candidates) &#123;            // 得到这个bean的scope，如singleton、prototype等            ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(candidate);            // 并将其设置到BeanDefinition中            candidate.setScope(scopeMetadata.getScopeName());            String beanName = this.beanNameGenerator.generateBeanName(candidate, this.registry);            if (candidate instanceof AbstractBeanDefinition abstractBeanDefinition) &#123;                postProcessBeanDefinition(abstractBeanDefinition, beanName);            &#125;            if (candidate instanceof AnnotatedBeanDefinition annotatedBeanDefinition) &#123;                AnnotationConfigUtils.processCommonDefinitionAnnotations(annotatedBeanDefinition);            &#125;            // 检查与这个bean名字对应的BeanDefinition是否已经被注册过            if (checkCandidate(beanName, candidate)) &#123;                BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(candidate, beanName);                definitionHolder =                        AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry);                beanDefinitions.add(definitionHolder);                // 将这个bean注册到IoC容器                registerBeanDefinition(definitionHolder, this.registry);            &#125;        &#125;    &#125;    return beanDefinitions;&#125;\n\nBeanDefinition载入上面代码的for循环里首先会执行findCandidateComponents，这里就是负责载入这个basePackage下的所有BeanDefinition的。\npublic Set&lt;BeanDefinition&gt; findCandidateComponents(String basePackage) &#123;    if (this.componentsIndex != null &amp;&amp; indexSupportsIncludeFilters()) &#123;        return addCandidateComponentsFromIndex(this.componentsIndex, basePackage);    &#125;    else &#123;        return scanCandidateComponents(basePackage);    &#125;&#125;private Set&lt;BeanDefinition&gt; scanCandidateComponents(String basePackage) &#123;    Set&lt;BeanDefinition&gt; candidates = new LinkedHashSet&lt;&gt;();    try &#123;        // 根据basePackages生成classpath，        // 如classpath*:com/example/demo/**/*.class        String packageSearchPath = ResourcePatternResolver.CLASSPATH_ALL_URL_PREFIX +                resolveBasePackage(basePackage) + &#x27;/&#x27; + this.resourcePattern;        // 从这个classpath下面扫描所有匹配的资源，也就是类        Resource[] resources = getResourcePatternResolver().getResources(packageSearchPath);        boolean traceEnabled = logger.isTraceEnabled();        boolean debugEnabled = logger.isDebugEnabled();        // 循环处理上面找到的类，将符合条件的类包装成ScannedGenericBeanDefinition并加到candidates中        for (Resource resource : resources) &#123;            String filename = resource.getFilename();            if (filename != null &amp;&amp; filename.contains(ClassUtils.CGLIB_CLASS_SEPARATOR)) &#123;                // Ignore CGLIB-generated classes in the classpath                continue;            &#125;            if (traceEnabled) &#123;                logger.trace(&quot;Scanning &quot; + resource);            &#125;            try &#123;                MetadataReader metadataReader = getMetadataReaderFactory().getMetadataReader(resource);                // 检查这个类是不是既没有命中任何一个exclude filter，并至少命中了一个include filter                // 其中一个include filter就是根据@Component注解来过滤的                if (isCandidateComponent(metadataReader)) &#123;                    ScannedGenericBeanDefinition sbd = new ScannedGenericBeanDefinition(metadataReader);                    sbd.setSource(resource);                    if (isCandidateComponent(sbd)) &#123;                        // 打日志，略                        candidates.add(sbd);                    &#125;                    else &#123;                        // 打日志，略                    &#125;                &#125;                else &#123;                    // 打日志，略                &#125;            &#125;            catch (FileNotFoundException ex) &#123;                // 打日志，略            &#125;            catch (ClassFormatException ex) &#123;                if (shouldIgnoreClassFormatException) &#123;                    // 打日志，略                &#125;                else &#123;                    throw new BeanDefinitionStoreException(&quot;Incompatible class format in &quot; + resource +                            &quot;: set system property &#x27;spring.classformat.ignore&#x27; to &#x27;true&#x27; &quot; +                            &quot;if you mean to ignore such files during classpath scanning&quot;, ex);                &#125;            &#125;            catch (Throwable ex) &#123;                throw new BeanDefinitionStoreException(&quot;Failed to read candidate component class: &quot; + resource, ex);            &#125;        &#125;    &#125;    catch (IOException ex) &#123;        throw new BeanDefinitionStoreException(&quot;I/O failure during classpath scanning&quot;, ex);    &#125;    return candidates;&#125;\n\n注册BeanDefinition回到doScan方法，在得到所有的BeanDefinition之后，就会调用registerBeanDefinition方法来完成BeanDefinition的注册。\nprotected void registerBeanDefinition(BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) &#123;    BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, registry);&#125;// BeanDefinitionReaderUtilspublic static void registerBeanDefinition(        BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry)        throws BeanDefinitionStoreException &#123;    // Register bean definition under primary name.    String beanName = definitionHolder.getBeanName();    // 将这个bean注册到DefaultListableBeanFactory的BeanDefinitionMap    registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition());    // Register aliases for bean name, if any.    String[] aliases = definitionHolder.getAliases();    if (aliases != null) &#123;        for (String alias : aliases) &#123;            registry.registerAlias(beanName, alias);        &#125;    &#125;&#125;\n\n这里就是把传进来的BeanDefinition注册到DefaultListableBeanFactory的BeanDefinitionMap中。这部分在上一篇博文中已经提过，就不重复了。\n在doScan方法的for循环遍历完所有的BeanDefinition之后，所有带有@Component注解的bean就全部注册好了。接下来会回到doProcessConfigurationClass方法，继续处理带有@Import注解的类和带有@Bean注解的方法。\n参考文章\nSpringBoot启动流程分析（四）：IoC容器的初始化过程\n\n","categories":["学知识"],"tags":["Java","Spring Boot"]},{"title":"Spring Boot启动流程分析","url":"/spring-boot-application-starts-up.html","content":"想来用了这么久的Spring Boot，但一直没仔细了解它是怎么启动的。那既然想起来了，不如趁热打铁，从它的入口开始，深入看看Spring Boot在启动时都做了些什么。\n\n\n启动入口入口这部分就没啥说的，跟个Hello world差不多，一个main方法执行SpringApplication#run来启动整个Spring Boot应用。\n@SpringBootApplicationpublic class DemoApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(DemoApplication.class, args);    &#125;&#125;\n\n@SpringBootApplication注解进到@SpringBootApplication注解的源码可以看出，它实际上是@SpringBootConfiguration、@EnableAutoConfiguration和@ComponentScan的组合。\n@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123;     @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),    @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;    // 略&#125;\n\n@SpringBootConfiguration注解@SpringBootConfiguration注解实际上只是@Configuration注解的套娃，区别只有两点：\n\n@SpringBootConfiguration是Spring Boot提供的注解，而@Configuration是Spring提供的注解；\n@SpringBootConfiguration注解在整个应用中只能出现一次，@Configuration注解则可以需要有多少就用多少次。\n\n@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Configuration@Indexedpublic @interface SpringBootConfiguration &#123;    // 略&#125;\n\n@EnableAutoConfiguration注解@EnableAutoConfiguration注解通过引入AutoConfigurationImportSelector来开启Spring Boot的自动配置功能。这部分内容我在另一篇博文Spring Boot 自动配置的原理中有详细的说明，这里就不再重复了。\n@ComponentScan注解@ComponentScan注解用来配置Spring如何扫描组件。我们可以通过设定basePackageClasses或basePackages属性来指定从哪些包中扫描，而在不指定的情况下，Spring就会从带有这个注解的类所在的包开始扫描。\n因为这个注解会在启动类中被引入，而启动类又在项目最顶层的包中（应该没有谁闲的会去挪启动类的位置吧），所以Spring就会从顶层包开始往下扫描组件。\nSpringApplication类从启动类对SpringApplication#run的调用一路追下去，最后会走到SpringApplication类的这个代码块：\npublic static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123;    return new SpringApplication(primarySources).run(args);&#125;\n\nSpringApplication的构造方法可以看到这里先实例化了一个SpringApplication对象，那么顺着对应的构造方法一路追下去，最后会看到这样一个构造方法：\npublic SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123;    this.resourceLoader = resourceLoader;    Assert.notNull(primarySources, &quot;PrimarySources must not be null&quot;);    this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources));    // 判断应用程序的类型    this.webApplicationType = WebApplicationType.deduceFromClasspath();    // 实例化bootstrap registry initializer    this.bootstrapRegistryInitializers = new ArrayList&lt;&gt;(            getSpringFactoriesInstances(BootstrapRegistryInitializer.class));    // 实例化所有可用的initializer    setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class));    // 实例化所有可用的listener    setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));    // 找到主类    this.mainApplicationClass = deduceMainApplicationClass();&#125;\n\n判断应用程序的类型Spring Boot需要判断应用是哪种类型，来决定要不要启动它内嵌的web server，以及启动哪种web server。它会根据这样一个规则来判断当前应用的类型：\n\n如果能找到org.springframework.web.reactive.DispatcherHandler类，同时找不到org.springframework.web.servlet.DispatcherServlet和org.glassfish.jersey.servlet.ServletContainer，那么就判定当前应用是一个reactive web应用，并会在将来启动面向reactive的web server；\n如果org.springframework.web.servlet.DispatcherServlet和org.glassfish.jersey.servlet.ServletContainer都找不到，说明这个应用不是一个web application，将来也不会启动任何web server；\n如果以上条件都不符合，那么就判定这个应用是一个servlet web应用，将来会启动面向servlet的web server。\n\n实例化initializer和listener点进getSpringFactoriesInstances方法的实现并顺着追下去，最终可以看到这样一个代码块：\nprivate &lt;T&gt; List&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, ArgumentResolver argumentResolver) &#123;    return SpringFactoriesLoader.forDefaultResourceLocation(getClassLoader()).load(type, argumentResolver);&#125;\n\n再顺着forDefaultResourceLocation方法的实现，最终会走到这里：\npublic static SpringFactoriesLoader forResourceLocation(String resourceLocation, @Nullable ClassLoader classLoader) &#123;    Assert.hasText(resourceLocation, &quot;&#x27;resourceLocation&#x27; must not be empty&quot;);    ClassLoader resourceClassLoader = (classLoader != null ? classLoader :            SpringFactoriesLoader.class.getClassLoader());    Map&lt;String, SpringFactoriesLoader&gt; loaders = cache.computeIfAbsent(            resourceClassLoader, key -&gt; new ConcurrentReferenceHashMap&lt;&gt;());    return loaders.computeIfAbsent(resourceLocation, key -&gt;            new SpringFactoriesLoader(classLoader, loadFactoriesResource(resourceClassLoader, resourceLocation)));&#125;\n\n看起来好像云里雾里的，下断点调试一下看出来了，这里就是扫描所有META-INF目录下的spring.factories文件，并把里面所有的键值对放到一个Map里。最后我们可以得到一个包含了这个Map的SpringFactoriesLoader对象。\n\n接着看load方法，顺着追下去会走到这个代码块：\npublic &lt;T&gt; List&lt;T&gt; load(Class&lt;T&gt; factoryType, @Nullable ArgumentResolver argumentResolver,        @Nullable FailureHandler failureHandler) &#123;    Assert.notNull(factoryType, &quot;&#x27;factoryType&#x27; must not be null&quot;);    List&lt;String&gt; implementationNames = loadFactoryNames(factoryType);    logger.trace(LogMessage.format(&quot;Loaded [%s] names: %s&quot;, factoryType.getName(), implementationNames));    List&lt;T&gt; result = new ArrayList&lt;&gt;(implementationNames.size());    FailureHandler failureHandlerToUse = (failureHandler != null) ? failureHandler : THROWING_FAILURE_HANDLER;    for (String implementationName : implementationNames) &#123;        T factory = instantiateFactory(implementationName, factoryType, argumentResolver, failureHandlerToUse);        if (factory != null) &#123;            result.add(factory);        &#125;    &#125;    AnnotationAwareOrderComparator.sort(result);    return result;&#125;\n\n这里就是从上面得到的Map中找到factoryType传进来的接口对应的实现类，分别调用它们的构造方法将其实例化，然后把得到的对象放在List里面返回，并交给外层的setInitializers方法来把这个List放在SpringApplication类的成员变量initializers中。\n实例化listener也是一样的流程。至于ApplicationListener则是Spring的事件监听器，利用观察者模式，通过ApplicationEvent和ApplicationListener接口实现对Spring容器全生命周期的监听，同时也可以监听自定义的事件。\n找到主类顺着deduceMainApplicationClass的实现，会注意到这么两个方法：\nprivate Class&lt;?&gt; deduceMainApplicationClass() &#123;    return StackWalker.getInstance(StackWalker.Option.RETAIN_CLASS_REFERENCE)        .walk(this::findMainClass)        .orElse(null);&#125;private Optional&lt;Class&lt;?&gt;&gt; findMainClass(Stream&lt;StackFrame&gt; stack) &#123;    return stack.filter((frame) -&gt; Objects.equals(frame.getMethodName(), &quot;main&quot;))        .findFirst()        .map(StackWalker.StackFrame::getDeclaringClass);&#125;\n\n看起来挺简单粗暴的，就是遍历栈帧，找执行了main方法的那个栈，然后找到这个栈对应的类。\nrun方法经过上面一顿操作，这个SpringApplication对象就初始化好了，接下来就会调用它的run方法开始启动。\npublic ConfigurableApplicationContext run(String... args) &#123;    Startup startup = Startup.create();    if (this.registerShutdownHook) &#123;        SpringApplication.shutdownHook.enableShutdownHookAddition();    &#125;    // 创建 bootstrap context    DefaultBootstrapContext bootstrapContext = createBootstrapContext();    ConfigurableApplicationContext context = null;    // 进入无头模式    configureHeadlessProperty();    // 启动listener    SpringApplicationRunListeners listeners = getRunListeners(args);    listeners.starting(bootstrapContext, this.mainApplicationClass);    try &#123;        ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);        // 准备环境        ConfigurableEnvironment environment = prepareEnvironment(listeners, bootstrapContext, applicationArguments);        Banner printedBanner = printBanner(environment);        // 创建application context        context = createApplicationContext();        context.setApplicationStartup(this.applicationStartup);        prepareContext(bootstrapContext, context, environment, listeners, applicationArguments, printedBanner);        // 刷新application context        refreshContext(context);        afterRefresh(context, applicationArguments);        // 收尾        startup.started();        if (this.logStartupInfo) &#123;            new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), startup);        &#125;        listeners.started(context, startup.timeTakenToStarted());        callRunners(context, applicationArguments);    &#125;    catch (Throwable ex) &#123;        // 略    &#125;    try &#123;        if (context.isRunning()) &#123;            listeners.ready(context, startup.ready());        &#125;    &#125;    catch (Throwable ex) &#123;        // 略    &#125;    return context;&#125;\n\n创建bootstrap context点进createBootstrapContext方法，可以看到这样一个代码块：\nprivate DefaultBootstrapContext createBootstrapContext() &#123;    DefaultBootstrapContext bootstrapContext = new DefaultBootstrapContext();    this.bootstrapRegistryInitializers.forEach((initializer) -&gt; initializer.initialize(bootstrapContext));    return bootstrapContext;&#125;\n\n在构造SpringApplication时准备的this.bootstrapRegistryInitializers在这用上了。看代码的话就是分别执行每个initializer的initailize方法。但是我这个应用里面没有BootstrapRegistryInitializer的实现类，所以也就没法深入进去看它到底干了什么。\n不过看了眼Git Blame，发现了这么一段话：\n\nRefactor BootstrapRegistry support following initial prototype work with the Spring Cloud team.\n\n看起来是跟Spring Cloud相关的，那暂且就不关注了。\n进入无头模式在Oracle的文档Using Headless Mode in the Java SE Platform中提到\n\nHeadless mode is a system configuration in which the display device, keyboard, or mouse is lacking. Sounds unexpected, but actually you can perform different operations in this mode, even with graphic data.\nWhere it is applicable? Let’s say that your application repeatedly generates a certain image, for example, a graphical authorization code that must be changed every time a user logs in to the system. When creating an image, your application needs neither the display nor the keyboard. Let’s assume now that you have a mainframe or dedicated server on your project that has no display device, keyboard, or mouse. The ideal decision is to use this environment’s substantial computing power for the visual as well as the nonvisual features. An image that was generated in the headless mode system then can be passed to the headful system for further rendering.\n\n其实就是，像web服务之类不需要显示界面的应用，就可以让它进入无头模式，让它在没有显示器等输入输出设备时也能启动，还可以提高计算效率。\n启动listener点进getRunListeners的实现可以看到如下代码块：\nprivate SpringApplicationRunListeners getRunListeners(String[] args) &#123;    ArgumentResolver argumentResolver = ArgumentResolver.of(SpringApplication.class, this);    argumentResolver = argumentResolver.and(String[].class, args);    List&lt;SpringApplicationRunListener&gt; listeners = getSpringFactoriesInstances(SpringApplicationRunListener.class,            argumentResolver);    SpringApplicationHook hook = applicationHook.get();    SpringApplicationRunListener hookListener = (hook != null) ? hook.getRunListener(this) : null;    if (hookListener != null) &#123;        listeners = new ArrayList&lt;&gt;(listeners);        listeners.add(hookListener);    &#125;    return new SpringApplicationRunListeners(logger, listeners, this.applicationStartup);&#125;\n\n看到getSpringFactoriesInstances有没有感觉很熟悉？对，这一步就是尝试从spring.factories里尝试找到SpringApplicationRunListener的实现类。默认来说这里只能找到EventPublishingRunListener，它是用来发布各种SpringBootEvent的。在Spring Boot中，事件是一个很重要的东西，通过事件机制我们可以监听Spring Boot容器中正在发生的事件，也可以监听各种自定义的事件。事件机制也为Bean之间的消息传递提供支持。\n除了从spring.factories尝试获取listener之外，Spring Boot也会尝试从SpringApplicationHook中找到hook进来的listener。看了下代码，似乎我们可以在入口的main方法里调用SpringApplication#withHook来添加我们需要的hook，但是我的应用里面也没有什么能用的，所以也没法深挖了。\n在得到这些listener之后，就会实例化一个SpringApplicationRunListeners对象并返回回去，然后在SpringApplicationRunListeners#starting方法中调用各个listener的starting方法。此时，上面得到的EventPublishingRunListener就会广播出去一条ApplicationStartingEvent事件。\nvoid starting(ConfigurableBootstrapContext bootstrapContext, Class&lt;?&gt; mainApplicationClass) &#123;    doWithListeners(&quot;spring.boot.application.starting&quot;, (listener) -&gt; listener.starting(bootstrapContext),            (step) -&gt; &#123;                if (mainApplicationClass != null) &#123;                    step.tag(&quot;mainApplicationClass&quot;, mainApplicationClass.getName());                &#125;            &#125;);&#125;private void doWithListeners(String stepName, Consumer&lt;SpringApplicationRunListener&gt; listenerAction,        Consumer&lt;StartupStep&gt; stepAction) &#123;    StartupStep step = this.applicationStartup.start(stepName);    this.listeners.forEach(listenerAction);    if (stepAction != null) &#123;        stepAction.accept(step);    &#125;    step.end();&#125;\n\n准备环境Spring中的Environment负责两件事：\n\n加载配置好的各种property的值\n后续通过各种方法获取property的值\n\n在Spring Boot里，property的值可以通过YAML文件或properties文件、环境变量，和命令行参数这三种方法配置。此外，Spring Boot会依照一定的优先级来决定采用哪个property值，高优先级的会覆盖低优先级的。常见的几种配置方式会按照如下的优先级排列：\n\n开发者工具Devtools全局配置\n命令行指定的参数（如--server.port=8080）\nJNDI参数\nJava系统参数（通过-D指定的参数）\n系统环境变量\n对应不同环境的application-{profile}.yml配置文件\napplication.yml配置文件\n默认参数\n\n点进prepareEvent方法的实现，可以看到这个代码块（我稍微重新格式化了一下，看起来更舒服一点）：\nprivate ConfigurableEnvironment prepareEnvironment(    SpringApplicationRunListeners listeners,    DefaultBootstrapContext bootstrapContext,     ApplicationArguments applicationArguments) &#123;    // Create and configure the environment    // 获得应用的环境（servlet / reactive）    ConfigurableEnvironment environment = getOrCreateEnvironment();    configureEnvironment(environment, applicationArguments.getSourceArgs());    ConfigurationPropertySources.attach(environment);    listeners.environmentPrepared(bootstrapContext, environment);    DefaultPropertiesPropertySource.moveToEnd(environment);    Assert.state(!environment.containsProperty(&quot;spring.main.environment-prefix&quot;),            &quot;Environment prefix cannot be set via properties.&quot;);    bindToSpringApplication(environment);    if (!this.isCustomEnvironment) &#123;        EnvironmentConverter environmentConverter = new EnvironmentConverter(getClassLoader());        environment = environmentConverter.convertEnvironmentIfNecessary(environment, deduceEnvironmentClass());    &#125;    ConfigurationPropertySources.attach(environment);    return environment;&#125;private ConfigurableEnvironment getOrCreateEnvironment() &#123;    if (this.environment != null) &#123;        return this.environment;    &#125;    ConfigurableEnvironment environment = this.applicationContextFactory.createEnvironment(this.webApplicationType);    if (environment == null &amp;&amp; this.applicationContextFactory != ApplicationContextFactory.DEFAULT) &#123;        environment = ApplicationContextFactory.DEFAULT.createEnvironment(this.webApplicationType);    &#125;    return (environment != null) ? environment : new ApplicationEnvironment();&#125;\n\n首先在getOrCreateEnvironment中，Spring Boot会根据应用类型(reactive、servlet或none)来创建对应的环境。比如我这个是一个servlet应用，那么createEnvironment方法就会返回一个ApplicationServletEnvironment对象，并返回回去。\n接下来到configureEnvironment里面：\nprotected void configureEnvironment(ConfigurableEnvironment environment, String[] args) &#123;    if (this.addConversionService) &#123;        environment.setConversionService(new ApplicationConversionService());    &#125;    configurePropertySources(environment, args);    configureProfiles(environment, args);&#125;protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) &#123;    MutablePropertySources sources = environment.getPropertySources();    // 如果有default properties，那就把它们加到MutablePropertySources里面    if (!CollectionUtils.isEmpty(this.defaultProperties)) &#123;        DefaultPropertiesPropertySource.addOrMerge(this.defaultProperties, sources);    &#125;    // addCommandLineProperties默认是true    // args就是启动时传进来的参数，--server.port=8080之类的    // 如果有指定参数，那就把这些参数包在SimpleCommandLinePropertySource里面，并添加到MutablePropertySources中    if (this.addCommandLineProperties &amp;&amp; args.length &gt; 0) &#123;        String name = CommandLinePropertySource.COMMAND_LINE_PROPERTY_SOURCE_NAME;        if (sources.contains(name)) &#123;            PropertySource&lt;?&gt; source = sources.get(name);            CompositePropertySource composite = new CompositePropertySource(name);            composite.addPropertySource(new SimpleCommandLinePropertySource(&quot;springApplicationCommandLineArgs&quot;, args));            composite.addPropertySource(source);            sources.replace(name, composite);        &#125;        else &#123;            sources.addFirst(new SimpleCommandLinePropertySource(args));        &#125;    &#125;&#125;protected void configureProfiles(ConfigurableEnvironment environment, String[] args) &#123;&#125;\n\n这个conversion service是干啥用的暂时还没搞明白，先留个坑，专注于主线。爬了些文大概看明白了，ConversionService是用来处理各种类型转换的，比如把字符串转换成Long或者日期等。\n这里的话就是把各种参数（比如附加在启动命令里面的命令行参数）给填充到environment对象里。MutablePropertySources就是存放property的载体，在前面调用createEnvironment的时候，ApplicationServletEnvironment继承的AbstractEnvironment类的构造方法就会创建一个新的MutablePropertySources实例。\n接下来ConfigurationPropertySources#attach方法里面，environment的propertySources会被封装成一个ConfigurationPropertySource并添加到environment中。\n环境准备完成后，Spring Boot会发布一个ApplicationEnvironmentPreparedEvent事件。顺着doWithListeners方法一路追下去，在SimpleApplicationEventMulticaster#multicastEvent方法中，查看getApplicationListeners(event, type)的返回结果可以看到，这里会调用多个监听器来处理这个事件。\n\n其中EnvironmentPostProcessorApplicationListener又会调用多个后处理器实现加载系统环境变量（由SystemEnvironmentPropertySourceEnvironmentPostProcessor完成）、在环境中设定启用的profile并把配置文件加载到环境（由ConfigDataEnvironmentPostProcessor完成）等操作。\n其中ConfigDataEnvironmentPostProcessor默认会从classpath:/、classpath:/config/、file:./、file:./config/、file:./config/*/这几个地方查找配置文件，如果有需要也可以通过spring.config.location、spring.config.additional-location、spring.config.import这三个配置来干预它从哪查找配置文件。\n\n在查找到配置文件，并得到当前启用的profile后，ConfigDataEnvironment#applyToEnvironment方法就会将配置文件中的值加载到环境中。\n\n然后Spring Boot会把这个environment对象与SpringApplication绑定起来。但是绑定这部分暂时也没看明白，依旧是专注于主线，以后有时间再说。\n创建application context这里会根据应用的类型（reactive或servlet）来创建对应的application context对象。\nprotected ConfigurableApplicationContext createApplicationContext() &#123;    return this.applicationContextFactory.create(this.webApplicationType);&#125;\n\ndebug进去，走到了DefaultApplicationContextFactory的这两个方法中：\n@Overridepublic ConfigurableApplicationContext create(WebApplicationType webApplicationType) &#123;    try &#123;        return getFromSpringFactories(            webApplicationType,             ApplicationContextFactory::create,            this::createDefaultApplicationContext);    &#125;    catch (Exception ex) &#123;        throw new IllegalStateException(&quot;Unable create a default ApplicationContext instance, &quot;                + &quot;you may need a custom ApplicationContextFactory&quot;, ex);    &#125;&#125;private &lt;T&gt; T getFromSpringFactories(    WebApplicationType webApplicationType,    BiFunction&lt;ApplicationContextFactory, WebApplicationType, T&gt; action,     Supplier&lt;T&gt; defaultResult) &#123;    for (ApplicationContextFactory candidate : SpringFactoriesLoader.loadFactories(ApplicationContextFactory.class,getClass().getClassLoader())) &#123;        T result = action.apply(candidate, webApplicationType);        if (result != null) &#123;            return result;        &#125;    &#125;    return (defaultResult != null) ? defaultResult.get() : null;&#125;\n\n在这个循环里它会分别执行ReactiveWebServerApplicationContextFactory和ServletWebServerApplicationContextFactory里面的create方法，而create方法里会判断当前应用的类型，来决定要不要创建对应的application context。\n@Overridepublic ConfigurableApplicationContext create(WebApplicationType webApplicationType) &#123;    return (webApplicationType != WebApplicationType.SERVLET) ? null : createContext();&#125;private ConfigurableApplicationContext createContext() &#123;    if (!AotDetector.useGeneratedArtifacts()) &#123;        return new AnnotationConfigServletWebServerApplicationContext();    &#125;    return new ServletWebServerApplicationContext();&#125;\n\n创建好application context实例后，Spring Boot会开始准备context的内容。\nprivate void prepareContext(    DefaultBootstrapContext bootstrapContext,     ConfigurableApplicationContext context,    ConfigurableEnvironment environment,     SpringApplicationRunListeners listeners,    ApplicationArguments applicationArguments,     Banner printedBanner) &#123;    // 绑定环境    context.setEnvironment(environment);    // 后置处理    postProcessApplicationContext(context);    addAotGeneratedInitializerIfNecessary(this.initializers);    // 执行各个initializer的initialize方法    applyInitializers(context);    // 发布事件    listeners.contextPrepared(context);    // 发布bootstrap context被关闭的事件    bootstrapContext.close(context);    if (this.logStartupInfo) &#123;        logStartupInfo(context.getParent() == null);        logStartupProfileInfo(context);    &#125;    // 注册启动相关的单例bean    ConfigurableListableBeanFactory beanFactory = context.getBeanFactory();    // 把main方法中的args封装成单例bean注册到容器    beanFactory.registerSingleton(&quot;springApplicationArguments&quot;, applicationArguments);    if (printedBanner != null) &#123;        beanFactory.registerSingleton(&quot;springBootBanner&quot;, printedBanner);    &#125;    // 设置是否允许循环引用，是否允许覆盖注册    if (beanFactory instanceof AbstractAutowireCapableBeanFactory autowireCapableBeanFactory) &#123;        autowireCapableBeanFactory.setAllowCircularReferences(this.allowCircularReferences);        if (beanFactory instanceof DefaultListableBeanFactory listableBeanFactory) &#123;            listableBeanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding);        &#125;    &#125;    // 处理延迟初始化    if (this.lazyInitialization) &#123;        context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor());    &#125;    // 是否要保持JVM持续运行    // 为了应对Java 21引入的虚拟线程产生的问题    // 因为虚拟线程都是守护线程，而在只有守护线程运行时，JVM就会退出    // 所以这里会启动一个非守护线程来保持JVM能持续运行下去    if (this.keepAlive) &#123;        context.addApplicationListener(new KeepAlive());    &#125;    context.addBeanFactoryPostProcessor(new PropertySourceOrderingBeanFactoryPostProcessor(context));    // 不考虑AOT优化时，加载primarySources和sources    if (!AotDetector.useGeneratedArtifacts()) &#123;        // 载入sources        Set&lt;Object&gt; sources = getAllSources();        Assert.notEmpty(sources, &quot;Sources must not be empty&quot;);        // 加载启动类并注入到容器        load(context, sources.toArray(new Object[0]));    &#125;    listeners.contextLoaded(context);&#125;\n\n注册启动相关的单例bean我对注册启动相关的单例bean很感兴趣，于是逐层点进registerSingleton的实现，看到了下列代码：\n// DefaultListableBeanFactory@Overridepublic void registerSingleton(String beanName, Object singletonObject) throws IllegalStateException &#123;    super.registerSingleton(beanName, singletonObject);    updateManualSingletonNames(set -&gt; set.add(beanName), set -&gt; !this.beanDefinitionMap.containsKey(beanName));    clearByTypeCache();&#125;// DefaultSingletonBeanRegistry@Overridepublic void registerSingleton(String beanName, Object singletonObject) throws IllegalStateException &#123;    Assert.notNull(beanName, &quot;Bean name must not be null&quot;);    Assert.notNull(singletonObject, &quot;Singleton object must not be null&quot;);    synchronized (this.singletonObjects) &#123;        Object oldObject = this.singletonObjects.get(beanName);        if (oldObject != null) &#123;            throw new IllegalStateException(&quot;Could not register object [&quot; + singletonObject +                    &quot;] under bean name &#x27;&quot; + beanName + &quot;&#x27;: there is already object [&quot; + oldObject + &quot;] bound&quot;);        &#125;        addSingleton(beanName, singletonObject);    &#125;&#125;protected void addSingleton(String beanName, Object singletonObject) &#123;    synchronized (this.singletonObjects) &#123;        this.singletonObjects.put(beanName, singletonObject);        this.singletonFactories.remove(beanName);        this.earlySingletonObjects.remove(beanName);        this.registeredSingletons.add(beanName);    &#125;&#125;\n\n看下来就是，在注册一个单例bean的时候，会传进来bean的名字和实际的对象。在registerSingleton中首先检查这个名字是不是已经被注册过了，没有被注册过的话就会调用addSingleton来注册。而所谓注册，就是：\n\n在singletonObjects这个Map里面增加一个条目，key是bean的名字，value是bean对应的对象；\n从singletonFactories和earlySingletonObjects中删掉以这个bean名字为key的条目；\n在registeredSingletons这个Set里面记录这次注册的bean的名字。\n\n上面提到的这三个Map实际上就是Spring的三级缓存。\n\nsingletonObjects是一级缓存，存储的是完整创建好的单例bean对象。在创建一个单例bean时，Spring会先从这里尝试获取这个bean的实例，如果找到则直接返回，否则继续创建这个bean；\nearlySingletonObjects是二级缓存，存储的是尚未完全创建好的“半成品”单例bean对象。在创建单例bean时，如果发现这个bean存在循环依赖，那么Spring会先创建这个bean的“半成品”对象并将其存到这里。当循环依赖的bean创建完成后，Spring再将这里存储的代理对象替换为完整的bean对象；\nsingletonFactories是三级缓存，存储的是单例bean的创建工厂。当一个单例bean被创建时，Spring会先将该bean的创建工厂存储到这里，然后再执行工厂的getObject()方法生成该bean的实例对象。在该bean被其他bean引用时，Spring会从这里获取该bean的创建工厂来创建出这个bean的实例，并将这个实例存储到singletonObjects中。\n\n返回到registerSingleton之后，继续执行updateManualSingletonNames方法：\nprivate void updateManualSingletonNames(Consumer&lt;Set&lt;String&gt;&gt; action, Predicate&lt;Set&lt;String&gt;&gt; condition) &#123;    if (hasBeanCreationStarted()) &#123;        // Cannot modify startup-time collection elements anymore (for stable iteration)        synchronized (this.beanDefinitionMap) &#123;            if (condition.test(this.manualSingletonNames)) &#123;                Set&lt;String&gt; updatedSingletons = new LinkedHashSet&lt;&gt;(this.manualSingletonNames);                action.accept(updatedSingletons);                this.manualSingletonNames = updatedSingletons;            &#125;        &#125;    &#125;    else &#123;        // Still in startup registration phase        if (condition.test(this.manualSingletonNames)) &#123;            action.accept(this.manualSingletonNames);        &#125;    &#125;&#125;\n\n在应用启动的时候，方法会走到else这部分，也就是直接向manualSingletonNames这个Set添加这次注册的bean的名字。manualSingletonNames这个Set存放的就是手动注册的各个bean的名字。\n然后执行clearByTypeCache方法，把这两个cache清除。\nprivate void clearByTypeCache() &#123;    this.allBeanNamesByType.clear();    this.singletonBeanNamesByType.clear();&#125;\n\n加载primarySources和sourcesgetAllSources会把primarySources和sources放进一个Set里面返回。\npublic Set&lt;Object&gt; getAllSources() &#123;    Set&lt;Object&gt; allSources = new LinkedHashSet&lt;&gt;();    if (!CollectionUtils.isEmpty(this.primarySources)) &#123;        // 把启动类添加到allSources        allSources.addAll(this.primarySources);    &#125;    if (!CollectionUtils.isEmpty(this.sources)) &#123;        allSources.addAll(this.sources);    &#125;    return Collections.unmodifiableSet(allSources);&#125;\n\n然后走到load方法，把bean加载到application context：\nprotected void load(ApplicationContext context, Object[] sources) &#123;    if (logger.isDebugEnabled()) &#123;        logger.debug(&quot;Loading source &quot; + StringUtils.arrayToCommaDelimitedString(sources));    &#125;    BeanDefinitionLoader loader = createBeanDefinitionLoader(getBeanDefinitionRegistry(context), sources);    if (this.beanNameGenerator != null) &#123;        loader.setBeanNameGenerator(this.beanNameGenerator);    &#125;    if (this.resourceLoader != null) &#123;        loader.setResourceLoader(this.resourceLoader);    &#125;    if (this.environment != null) &#123;        loader.setEnvironment(this.environment);    &#125;    loader.load();&#125;private BeanDefinitionRegistry getBeanDefinitionRegistry(ApplicationContext context) &#123;    if (context instanceof BeanDefinitionRegistry registry) &#123;        return registry;    &#125;    if (context instanceof AbstractApplicationContext abstractApplicationContext) &#123;        return (BeanDefinitionRegistry) abstractApplicationContext.getBeanFactory();    &#125;    throw new IllegalStateException(&quot;Could not locate BeanDefinitionRegistry&quot;);&#125;protected BeanDefinitionLoader createBeanDefinitionLoader(BeanDefinitionRegistry registry, Object[] sources) &#123;    return new BeanDefinitionLoader(registry, sources);&#125;// BeanDefinitionLoaderBeanDefinitionLoader(BeanDefinitionRegistry registry, Object... sources) &#123;    Assert.notNull(registry, &quot;Registry must not be null&quot;);    Assert.notEmpty(sources, &quot;Sources must not be empty&quot;);    this.sources = sources;    // 初始化注解形式的Bean定义读取器    this.annotatedReader = new AnnotatedBeanDefinitionReader(registry);    // 初始化XML形式的Bean定义读取器    this.xmlReader = new XmlBeanDefinitionReader(registry);    this.groovyReader = (isGroovyPresent() ? new GroovyBeanDefinitionReader(registry) : null);    // 初始化classpath扫描器    this.scanner = new ClassPathBeanDefinitionScanner(registry);    // 给classpath扫描器添加排除过滤器    this.scanner.addExcludeFilter(new ClassExcludeFilter(sources));&#125;\n\n首先这里会执行getBeanDefinitionRegistry方法。因为这个应用的application context是一个AnnotationConfigServletWebServerApplicationContext，而它又层层继承于BeanDefinitionRegistry，所以这里返回的就是当前的application context。得到bean defininition registry之后，就会用它来初始化一个BeanDefinitionLoader对象。BeanDefinitionLoader的构造方法里面会初始化各种reader和scanner，来支持从不同的资源（XML、Java Config等）加载bean definition。\n得到BeanDefinitionLoader后，Spring Boot会把bean名字的生成器等自定义部件绑定上去。不过默认情况下它们都是null。然后，就会执行BeanDefinitionLoader的load方法。\nvoid load() &#123;    for (Object source : this.sources) &#123;        load(source);    &#125;&#125;private void load(Object source) &#123;    Assert.notNull(source, &quot;Source must not be null&quot;);    if (source instanceof Class&lt;?&gt; clazz) &#123;        load(clazz);        return;    &#125;    if (source instanceof Resource resource) &#123;        load(resource);        return;    &#125;    if (source instanceof Package pack) &#123;        load(pack);        return;    &#125;    if (source instanceof CharSequence sequence) &#123;        load(sequence);        return;    &#125;    throw new IllegalArgumentException(&quot;Invalid source type &quot; + source.getClass());&#125;private void load(Class&lt;?&gt; source) &#123;    if (isGroovyPresent() &amp;&amp; GroovyBeanDefinitionSource.class.isAssignableFrom(source)) &#123;        // Any GroovyLoaders added in beans&#123;&#125; DSL can contribute beans here        GroovyBeanDefinitionSource loader = BeanUtils.instantiateClass(source, GroovyBeanDefinitionSource.class);        ((GroovyBeanDefinitionReader) this.groovyReader).beans(loader.getBeans());    &#125;    if (isEligible(source)) &#123;        // 将启动类的BeanDefinition注册进beanDefinitionMap        this.annotatedReader.register(source);    &#125;&#125;private boolean isEligible(Class&lt;?&gt; type) &#123;    return !(type.isAnonymousClass() || isGroovyClosure(type) || hasNoConstructors(type));&#125;\n\n虽然这里会遍历sources，但是实际上sources里面只有应用的主启动类，所以最终会走到load(Class&lt;?&gt; source)方法中。因为应用中没有Groovy，所以第一个判断会被跳过，然后经过isEligible中的三个判断之后，走进register方法，并最终进入AnnotatedBeanDefinition#doRegisterBean方法中。\n// AnnotatedBeanDefinitionReaderpublic void register(Class&lt;?&gt;... componentClasses) &#123;    for (Class&lt;?&gt; componentClass : componentClasses) &#123;        registerBean(componentClass);    &#125;&#125;public void registerBean(Class&lt;?&gt; beanClass) &#123;    doRegisterBean(beanClass, null, null, null, null);&#125;private &lt;T&gt; void doRegisterBean(    Class&lt;T&gt; beanClass,     @Nullable String name,    @Nullable Class&lt;? extends Annotation&gt;[] qualifiers,     @Nullable Supplier&lt;T&gt; supplier,    @Nullable BeanDefinitionCustomizer[] customizers) &#123;    // 将传进来的类封装为AnnotatedGenericBeanDefinition    AnnotatedGenericBeanDefinition abd = new AnnotatedGenericBeanDefinition(beanClass);    if (this.conditionEvaluator.shouldSkip(abd.getMetadata())) &#123;        return;    &#125;    abd.setAttribute(ConfigurationClassUtils.CANDIDATE_ATTRIBUTE, Boolean.TRUE);    abd.setInstanceSupplier(supplier);    // 获取该类的scope属性    ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(abd);    // 设定这个bean的scope，singleton、prototype等    abd.setScope(scopeMetadata.getScopeName());    // 生成bean的名字    String beanName = (name != null ? name : this.beanNameGenerator.generateBeanName(abd, this.registry));    // 根据类上的注解设定bean的属性，即处理@Lazy、@Primary等    AnnotationConfigUtils.processCommonDefinitionAnnotations(abd);    if (qualifiers != null) &#123;        for (Class&lt;? extends Annotation&gt; qualifier : qualifiers) &#123;            if (Primary.class == qualifier) &#123;                abd.setPrimary(true);            &#125;            else if (Lazy.class == qualifier) &#123;                abd.setLazyInit(true);            &#125;            else &#123;                abd.addQualifier(new AutowireCandidateQualifier(qualifier));            &#125;        &#125;    &#125;    if (customizers != null) &#123;        for (BeanDefinitionCustomizer customizer : customizers) &#123;            customizer.customize(abd);        &#125;    &#125;    BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(abd, beanName);    definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry);    // 将这个BeanDefinition注册到IoC容器的BeanDefinitionMap中    BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, this.registry);&#125;static void processCommonDefinitionAnnotations(AnnotatedBeanDefinition abd, AnnotatedTypeMetadata metadata) &#123;    AnnotationAttributes lazy = attributesFor(metadata, Lazy.class);    if (lazy != null) &#123;        abd.setLazyInit(lazy.getBoolean(&quot;value&quot;));    &#125;    else if (abd.getMetadata() != metadata) &#123;        lazy = attributesFor(abd.getMetadata(), Lazy.class);        if (lazy != null) &#123;            abd.setLazyInit(lazy.getBoolean(&quot;value&quot;));        &#125;    &#125;    if (metadata.isAnnotated(Primary.class.getName())) &#123;        abd.setPrimary(true);    &#125;    AnnotationAttributes dependsOn = attributesFor(metadata, DependsOn.class);    if (dependsOn != null) &#123;        abd.setDependsOn(dependsOn.getStringArray(&quot;value&quot;));    &#125;    AnnotationAttributes role = attributesFor(metadata, Role.class);    if (role != null) &#123;        abd.setRole(role.getNumber(&quot;value&quot;).intValue());    &#125;    AnnotationAttributes description = attributesFor(metadata, Description.class);    if (description != null) &#123;        abd.setDescription(description.getString(&quot;value&quot;));    &#125;&#125;\n\n这里会处理传进来的类，为它创建一个BeanDefinition并设置各种属性，然后调用BeanDefinitionReaderUtils#registerBeanDefinition把这个bean注册到application context中。因为这一步传进来的只有主启动类，所以只会注册一个由主启动类生成的bean。\npublic static void registerBeanDefinition(        BeanDefinitionHolder definitionHolder,         BeanDefinitionRegistry registry    ) throws BeanDefinitionStoreException &#123;    // Register bean definition under primary name.    // 以bean的名字作为key将其注册到BeanDefinitionMap    String beanName = definitionHolder.getBeanName();    registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition());    // Register aliases for bean name, if any.    // 并以这个bean的各个别名再注册一遍    String[] aliases = definitionHolder.getAliases();    if (aliases != null) &#123;        for (String alias : aliases) &#123;            registry.registerAlias(beanName, alias);        &#125;    &#125;&#125;// GenericApplicationContext@Overridepublic void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123;    this.beanFactory.registerBeanDefinition(beanName, beanDefinition);&#125;@Overridepublic void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123;    Assert.hasText(beanName, &quot;Bean name must not be empty&quot;);    Assert.notNull(beanDefinition, &quot;BeanDefinition must not be null&quot;);    if (beanDefinition instanceof AbstractBeanDefinition abd) &#123;        try &#123;            abd.validate();        &#125;        catch (BeanDefinitionValidationException ex) &#123;            throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName,                    &quot;Validation of bean definition failed&quot;, ex);        &#125;    &#125;    // 检查是否已有同名的bean被注册过了    BeanDefinition existingDefinition = this.beanDefinitionMap.get(beanName);    if (existingDefinition != null) &#123;        if (!isBeanDefinitionOverridable(beanName)) &#123;            throw new BeanDefinitionOverrideException(beanName, beanDefinition, existingDefinition);        &#125;        else if (existingDefinition.getRole() &lt; beanDefinition.getRole()) &#123;            // e.g. was ROLE_APPLICATION, now overriding with ROLE_SUPPORT or ROLE_INFRASTRUCTURE            // 如果允许覆盖注册，且当前bean的级别高于已注册的bean（比如已注册的是一个应用自己的bean，但是现在正在注册一个框架提供的bean），            // 那么就继续            if (logger.isInfoEnabled()) &#123;                logger.info(&quot;Overriding user-defined bean definition for bean &#x27;&quot; + beanName +                        &quot;&#x27; with a framework-generated bean definition: replacing [&quot; +                        existingDefinition + &quot;] with [&quot; + beanDefinition + &quot;]&quot;);            &#125;        &#125;        else if (!beanDefinition.equals(existingDefinition)) &#123;            if (logger.isDebugEnabled()) &#123;                logger.debug(&quot;Overriding bean definition for bean &#x27;&quot; + beanName +                        &quot;&#x27; with a different definition: replacing [&quot; + existingDefinition +                        &quot;] with [&quot; + beanDefinition + &quot;]&quot;);            &#125;        &#125;        else &#123;            if (logger.isTraceEnabled()) &#123;                logger.trace(&quot;Overriding bean definition for bean &#x27;&quot; + beanName +                        &quot;&#x27; with an equivalent definition: replacing [&quot; + existingDefinition +                        &quot;] with [&quot; + beanDefinition + &quot;]&quot;);            &#125;        &#125;        // 覆盖beanDefinitionMap中原有的bean        this.beanDefinitionMap.put(beanName, beanDefinition);    &#125;    else &#123;        if (isAlias(beanName)) &#123;            String aliasedName = canonicalName(beanName);            if (!isBeanDefinitionOverridable(aliasedName)) &#123;                if (containsBeanDefinition(aliasedName)) &#123;  // alias for existing bean definition                    throw new BeanDefinitionOverrideException(                            beanName, beanDefinition, getBeanDefinition(aliasedName));                &#125;                else &#123;  // alias pointing to non-existing bean definition                    throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName,                            &quot;Cannot register bean definition for bean &#x27;&quot; + beanName +                            &quot;&#x27; since there is already an alias for bean &#x27;&quot; + aliasedName + &quot;&#x27; bound.&quot;);                &#125;            &#125;            else &#123;                removeAlias(beanName);            &#125;        &#125;        if (hasBeanCreationStarted()) &#123;            // Cannot modify startup-time collection elements anymore (for stable iteration)            synchronized (this.beanDefinitionMap) &#123;                this.beanDefinitionMap.put(beanName, beanDefinition);                List&lt;String&gt; updatedDefinitions = new ArrayList&lt;&gt;(this.beanDefinitionNames.size() + 1);                updatedDefinitions.addAll(this.beanDefinitionNames);                updatedDefinitions.add(beanName);                this.beanDefinitionNames = updatedDefinitions;                removeManualSingletonName(beanName);            &#125;        &#125;        else &#123;            // Still in startup registration phase            this.beanDefinitionMap.put(beanName, beanDefinition);            this.beanDefinitionNames.add(beanName);            removeManualSingletonName(beanName);        &#125;        this.frozenBeanDefinitionNames = null;    &#125;    if (existingDefinition != null || containsSingleton(beanName)) &#123;        resetBeanDefinition(beanName);    &#125;    else if (isConfigurationFrozen()) &#123;        clearByTypeCache();    &#125;&#125;\n\n在启动的时候，实际会走到registerBeanDefinition方法的// Still in startup registration phase这部分。这里会把传进来的BeanDefinition注册到beanDefinitionMap中然后返回。\n刷新application context在创建application context之后，Spring Boot会刷新它。\nprivate void refreshContext(ConfigurableApplicationContext context) &#123;    if (this.registerShutdownHook) &#123;        shutdownHook.registerApplicationContext(context);    &#125;    refresh(context);&#125;protected void refresh(ConfigurableApplicationContext applicationContext) &#123;    applicationContext.refresh();&#125;// ServletWebServerApplicationContextpublic final void refresh() throws BeansException, IllegalStateException &#123;    try &#123;        super.refresh();    &#125;    catch (RuntimeException ex) &#123;        WebServer webServer = this.webServer;        if (webServer != null) &#123;            webServer.stop();            webServer.destroy();        &#125;        throw ex;    &#125;&#125;// AbstractApplicationContextpublic void refresh() throws BeansException, IllegalStateException &#123;    this.startupShutdownLock.lock();    try &#123;        this.startupShutdownThread = Thread.currentThread();        StartupStep contextRefresh = this.applicationStartup.start(&quot;spring.context.refresh&quot;);        // Prepare this context for refreshing.        // 注入listener等组件，激活application context        prepareRefresh();        // Tell the subclass to refresh the internal bean factory.        // 得到当前application context的bean factory        ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();        // Prepare the bean factory for use in this context.        // 初始化bean factory，包括设定class loader，注入各种组件等        prepareBeanFactory(beanFactory);        try &#123;            // Allows post-processing of the bean factory in context subclasses.            // 设置beanFactory的后置处理            postProcessBeanFactory(beanFactory);            StartupStep beanPostProcess = this.applicationStartup.start(&quot;spring.context.beans.post-process&quot;);            // Invoke factory processors registered as beans in the context.            // 调用beanFactory的后置处理            // 后置处理器的工作时机是在所有BeanDefinition加载完成之后，bean实例化之前            // 它可以修改BeanDefinition的属性信息            invokeBeanFactoryPostProcessors(beanFactory);            // Register bean processors that intercept bean creation.            // 注册bean的后处理器            registerBeanPostProcessors(beanFactory);            beanPostProcess.end();            // 初始化上下文中的消息源            initMessageSource();            // 初始化上下文中的事件机制            initApplicationEventMulticaster();            // Initialize other special beans in specific context subclasses.            // 初始化其他特殊的bean，如创建web server            onRefresh();            // Check for listener beans and register them.            // 把listener beans注入到容器            registerListeners();            // Instantiate all remaining (non-lazy-init) singletons.            // 把剩余尚未实例化的bean实例化            finishBeanFactoryInitialization(beanFactory);            // Last step: publish corresponding event.            // 发布application context刷新完成的事件            finishRefresh();        &#125;        catch (RuntimeException | Error ex ) &#123;            if (logger.isWarnEnabled()) &#123;                logger.warn(&quot;Exception encountered during context initialization - &quot; +                        &quot;cancelling refresh attempt: &quot; + ex);            &#125;            // Destroy already created singletons to avoid dangling resources.            destroyBeans();            // Reset &#x27;active&#x27; flag.            cancelRefresh(ex);            // Propagate exception to caller.            throw ex;        &#125;        finally &#123;            contextRefresh.end();        &#125;    &#125;    finally &#123;        this.startupShutdownThread = null;        this.startupShutdownLock.unlock();    &#125;&#125;\n\n这里首先会一路走到AbstractApplicationContext#refresh方法，完成初始化bean factory，实例化剩余的bean等操作。\n得到当前application context的bean factory点进obtainFreshBeanFactory方法可以看到这样的实现：\n// AbstractApplicationContextprotected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123;    refreshBeanFactory();    return getBeanFactory();&#125;// GenericApplicationContextprivate final AtomicBoolean refreshed = new AtomicBoolean();@Overrideprotected final void refreshBeanFactory() throws IllegalStateException &#123;    if (!this.refreshed.compareAndSet(false, true)) &#123;        throw new IllegalStateException(                &quot;GenericApplicationContext does not support multiple refresh attempts: just call &#x27;refresh&#x27; once&quot;);    &#125;    this.beanFactory.setSerializationId(getId());&#125;@Overridepublic final ConfigurableListableBeanFactory getBeanFactory() &#123;    return this.beanFactory;&#125;\n\n在refreshBeanFactory方法中，Spring会利用AtomicBoolean的compareAndSet方法来保证这个方法绝对只会被执行一次。\n调用beanFactory的后置处理invokeBeanFactoryPostProcessors这里会完成IoC容器初始化的三个步骤，分别是\n\nResource定位在前面Spring Boot已经得到了启动类的BeanDefinition，那么在这里它会解析启动类的BeanDefinition，得到启动类所在的包并将其作为basePackage，这就完成了定位的过程。此外Spring Boot的各种starter是通过SPI机制实现的自动装配，而自动装配也是在这个方法中完成的。还有就是，这个方法中也会处理@EnableXXX注解中通过@Import指定的配置类。\nBeanDefinition载入在上一步得到了basePackage后，Spring Boot会把路径拼接成classpath*:com/example/demo/**/*.class这样的形式，然后PathMatchingResourcePatternResolver类会把这个路径下所有的class都加载进来，然后遍历判断有没有@Component注解。因为@Controller、@Service、@Configuration之类的实际上只是把@Component又包了一层，所以不用单独扫描它们。\n注册BeanDefinition在这一步，BeanDefinitionRegister接口的实现类会把解析到的BeanDefinition向IoC容器注册。\n\n这部分实在是太长，而本文又已经够长了，所以我会单开一篇来细说。\n把剩余尚未实例化的bean实例化protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123;    // Initialize conversion service for this context.    if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp;            beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123;        beanFactory.setConversionService(                beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class));    &#125;    // Register a default embedded value resolver if no BeanFactoryPostProcessor    // (such as a PropertySourcesPlaceholderConfigurer bean) registered any before:    // at this point, primarily for resolution in annotation attribute values.    if (!beanFactory.hasEmbeddedValueResolver()) &#123;        beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal));    &#125;    // Initialize LoadTimeWeaverAware beans early to allow for registering their transformers early.    String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false);    for (String weaverAwareName : weaverAwareNames) &#123;        getBean(weaverAwareName);    &#125;    // Stop using the temporary ClassLoader for type matching.    beanFactory.setTempClassLoader(null);    // Allow for caching all bean definition metadata, not expecting further changes.    // 冻结bean definition，不再允许新的修改    beanFactory.freezeConfiguration();    // Instantiate all remaining (non-lazy-init) singletons.    // 开始实例化bean    beanFactory.preInstantiateSingletons();&#125;public void preInstantiateSingletons() throws BeansException &#123;    if (logger.isTraceEnabled()) &#123;        logger.trace(&quot;Pre-instantiating singletons in &quot; + this);    &#125;    // Iterate over a copy to allow for init methods which in turn register new bean definitions.    // While this may not be part of the regular factory bootstrap, it does otherwise work fine.    List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames);    // Trigger initialization of all non-lazy singleton beans...    for (String beanName : beanNames) &#123;        RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName);        if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123;            if (isFactoryBean(beanName)) &#123;                Object bean = getBean(FACTORY_BEAN_PREFIX + beanName);                if (bean instanceof SmartFactoryBean&lt;?&gt; smartFactoryBean &amp;&amp; smartFactoryBean.isEagerInit()) &#123;                    getBean(beanName);                &#125;            &#125;            else &#123;                getBean(beanName);            &#125;        &#125;    &#125;    // Trigger post-initialization callback for all applicable beans...    for (String beanName : beanNames) &#123;        Object singletonInstance = getSingleton(beanName);        if (singletonInstance instanceof SmartInitializingSingleton smartSingleton) &#123;            StartupStep smartInitialize = getApplicationStartup().start(&quot;spring.beans.smart-initialize&quot;).tag(&quot;beanName&quot;, beanName);            smartSingleton.afterSingletonsInstantiated();            smartInitialize.end();        &#125;    &#125;&#125;\n\n第一个for循环的重点在于getBean方法，逐层点进实现，最终会进入AbstractBeanFactory#doGetBean方法。\nprotected &lt;T&gt; T doGetBean(    String name,     @Nullable Class&lt;T&gt; requiredType,     @Nullable Object[] args,     boolean typeCheckOnly) throws BeansException &#123;    String beanName = transformedBeanName(name);    Object beanInstance;    // Eagerly check singleton cache for manually registered singletons.    // 先检查这个bean是不是一个已经注册过的singleton，并尝试获取。    // 如果它已经注册过，但是尚未实例化的话，这个方法会将其实例化并返回。    // 方法里面是一个复杂的双检锁单例模式。    Object sharedInstance = getSingleton(beanName);    if (sharedInstance != null &amp;&amp; args == null) &#123;        if (logger.isTraceEnabled()) &#123;            if (isSingletonCurrentlyInCreation(beanName)) &#123;                logger.trace(&quot;Returning eagerly cached instance of singleton bean &#x27;&quot; + beanName +                        &quot;&#x27; that is not fully initialized yet - a consequence of a circular reference&quot;);            &#125;            else &#123;                logger.trace(&quot;Returning cached instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;);            &#125;        &#125;        // 检测bean的正确性，如果获取到的是FactoryBean的话，那么还会调用getObject()方法得到最终的bean实例        beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, null);    &#125;    else &#123;        // Fail if we&#x27;re already creating this bean instance:        // We&#x27;re assumably within a circular reference.        if (isPrototypeCurrentlyInCreation(beanName)) &#123;            throw new BeanCurrentlyInCreationException(beanName);        &#125;        // Check if bean definition exists in this factory.        // 检查容器中是否存在这个bean的BeanDefinition，如果在当前的工厂中找不到，那就到父级BeanFactory中去找，        // 如果还找不到那就继续往更上一级去找        BeanFactory parentBeanFactory = getParentBeanFactory();        if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123;            // Not found -&gt; check parent.            String nameToLookup = originalBeanName(name);            if (parentBeanFactory instanceof AbstractBeanFactory abf) &#123;                // 递归调用父级的doGetBean查找                return abf.doGetBean(nameToLookup, requiredType, args, typeCheckOnly);            &#125;            else if (args != null) &#123;                // Delegation to parent with explicit args.                return (T) parentBeanFactory.getBean(nameToLookup, args);            &#125;            else if (requiredType != null) &#123;                // No args -&gt; delegate to standard getBean method.                return parentBeanFactory.getBean(nameToLookup, requiredType);            &#125;            else &#123;                return (T) parentBeanFactory.getBean(nameToLookup);            &#125;        &#125;        if (!typeCheckOnly) &#123;            markBeanAsCreated(beanName);        &#125;        StartupStep beanCreation = this.applicationStartup.start(&quot;spring.beans.instantiate&quot;).tag(&quot;beanName&quot;, name);        try &#123;            if (requiredType != null) &#123;                beanCreation.tag(&quot;beanType&quot;, requiredType::toString);            &#125;            // 根据Bean的名字得到它的BeanDefinition            RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);            checkMergedBeanDefinition(mbd, beanName, args);            // Guarantee initialization of beans that the current bean depends on.            // 获取当前bean的所有依赖bean。            // 这里会递归调用getBean()，直到取到一个没有任何依赖的bean            String[] dependsOn = mbd.getDependsOn();            if (dependsOn != null) &#123;                for (String dep : dependsOn) &#123;                    // 如果被依赖的bean也依赖它，那就循环依赖了，没法处理，抛异常                    if (isDependent(beanName, dep)) &#123;                        throw new BeanCreationException(mbd.getResourceDescription(), beanName,                                &quot;Circular depends-on relationship between &#x27;&quot; + beanName + &quot;&#x27; and &#x27;&quot; + dep + &quot;&#x27;&quot;);                    &#125;                    registerDependentBean(dep, beanName);                    try &#123;                        getBean(dep);                    &#125;                    catch (NoSuchBeanDefinitionException ex) &#123;                        throw new BeanCreationException(mbd.getResourceDescription(), beanName,                                &quot;&#x27;&quot; + beanName + &quot;&#x27; depends on missing bean &#x27;&quot; + dep + &quot;&#x27;&quot;, ex);                    &#125;                &#125;            &#125;            // Create bean instance.            if (mbd.isSingleton()) &#123;                sharedInstance = getSingleton(beanName, () -&gt; &#123;                    try &#123;                        // 创建并注册这个singleton bean                        return createBean(beanName, mbd, args);                    &#125;                    catch (BeansException ex) &#123;                        // Explicitly remove instance from singleton cache: It might have been put there                        // eagerly by the creation process, to allow for circular reference resolution.                        // Also remove any beans that received a temporary reference to the bean.                        destroySingleton(beanName);                        throw ex;                    &#125;                &#125;);                beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);            &#125;            else if (mbd.isPrototype()) &#123;                // It&#x27;s a prototype -&gt; create a new instance.                Object prototypeInstance = null;                try &#123;                    beforePrototypeCreation(beanName);                    prototypeInstance = createBean(beanName, mbd, args);                &#125;                finally &#123;                    afterPrototypeCreation(beanName);                &#125;                beanInstance = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);            &#125;            else &#123;                String scopeName = mbd.getScope();                if (!StringUtils.hasLength(scopeName)) &#123;                    throw new IllegalStateException(&quot;No scope name defined for bean &#x27;&quot; + beanName + &quot;&#x27;&quot;);                &#125;                Scope scope = this.scopes.get(scopeName);                if (scope == null) &#123;                    throw new IllegalStateException(&quot;No Scope registered for scope name &#x27;&quot; + scopeName + &quot;&#x27;&quot;);                &#125;                try &#123;                    Object scopedInstance = scope.get(beanName, () -&gt; &#123;                        beforePrototypeCreation(beanName);                        try &#123;                            return createBean(beanName, mbd, args);                        &#125;                        finally &#123;                            afterPrototypeCreation(beanName);                        &#125;                    &#125;);                    beanInstance = getObjectForBeanInstance(scopedInstance, name, beanName, mbd);                &#125;                catch (IllegalStateException ex) &#123;                    throw new ScopeNotActiveException(beanName, scopeName, ex);                &#125;            &#125;        &#125;        catch (BeansException ex) &#123;            beanCreation.tag(&quot;exception&quot;, ex.getClass().toString());            beanCreation.tag(&quot;message&quot;, String.valueOf(ex.getMessage()));            cleanupAfterBeanCreationFailure(beanName);            throw ex;        &#125;        finally &#123;            beanCreation.end();            if (!isCacheBeanMetadata()) &#123;                clearMergedBeanDefinition(beanName);            &#125;        &#125;    &#125;    return adaptBeanInstance(name, beanInstance, requiredType);&#125;\n\n在检查bean是否存在时调用的getSingleton()是这样实现的：\n@Nullableprotected Object getSingleton(String beanName, boolean allowEarlyReference) &#123;    // Quick check for existing instance without full singleton lock    // 尝试从缓存中取出这个单例的实例    Object singletonObject = this.singletonObjects.get(beanName);    // 如果要获取的bean没有被加载，而且没有正在被创建，那么就主动加载这个bean    if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123;        // 尝试去earlySingletonObjects里面去找        singletonObject = this.earlySingletonObjects.get(beanName);        if (singletonObject == null &amp;&amp; allowEarlyReference) &#123;            synchronized (this.singletonObjects) &#123;                // Consistent creation of early reference within full singleton lock                // 加锁后再次尝试去singletonObjects和earlySingletonObjects里面找                singletonObject = this.singletonObjects.get(beanName);                if (singletonObject == null) &#123;                    singletonObject = this.earlySingletonObjects.get(beanName);                    if (singletonObject == null) &#123;                        // 如果还是找不到，那么就到singletonFactories里面去找对象的实例                        ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName);                        if (singletonFactory != null) &#123;                            singletonObject = singletonFactory.getObject();                            this.earlySingletonObjects.put(beanName, singletonObject);                            this.singletonFactories.remove(beanName);                        &#125;                    &#125;                &#125;            &#125;        &#125;    &#125;    return singletonObject;&#125;\n\n如果Object sharedInstance = getSingleton(beanName)这一步得到的是null，那么容器就会调用getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory)方法来创建这个bean。传给singletonFactory的lambda表达式中调用了createBean()方法，它又调用了doCreateBean()方法完成实际的bean创建操作。\n创建beanprotected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException &#123;    // Instantiate the bean.    // instanceWrapper用来持有创建出来的bean对象    BeanWrapper instanceWrapper = null;    if (mbd.isSingleton()) &#123;        // 如果是单例的话，那么先把缓存中同名的bean清除        instanceWrapper = this.factoryBeanInstanceCache.remove(beanName);    &#125;    // 创建bean第一步：创建实例    if (instanceWrapper == null) &#123;        instanceWrapper = createBeanInstance(beanName, mbd, args);    &#125;    Object bean = instanceWrapper.getWrappedInstance();    Class&lt;?&gt; beanType = instanceWrapper.getWrappedClass();    if (beanType != NullBean.class) &#123;        mbd.resolvedTargetType = beanType;    &#125;    // Allow post-processors to modify the merged bean definition.    synchronized (mbd.postProcessingLock) &#123;        if (!mbd.postProcessed) &#123;            try &#123;                applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName);            &#125;            catch (Throwable ex) &#123;                throw new BeanCreationException(mbd.getResourceDescription(), beanName,                        &quot;Post-processing of merged bean definition failed&quot;, ex);            &#125;            mbd.markAsPostProcessed();        &#125;    &#125;    // Eagerly cache singletons to be able to resolve circular references    // even when triggered by lifecycle interfaces like BeanFactoryAware.    // 当这个bean是一个单例，并且允许循环引用，且这个bean正在创建中时，    // 就提前暴露一个ObjectFactory，来自动解决循环引用    boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp;            isSingletonCurrentlyInCreation(beanName));    if (earlySingletonExposure) &#123;        if (logger.isTraceEnabled()) &#123;            logger.trace(&quot;Eagerly caching bean &#x27;&quot; + beanName +                    &quot;&#x27; to allow for resolving potential circular references&quot;);        &#125;        addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean));    &#125;    // Initialize the bean instance.    Object exposedObject = bean;    try &#123;        // 创建bean第二步：设置实例的属性。依赖注入就发生在这里。        // 如果存在依赖其他bean的属性，那么就递归调用，初始化依赖的bean        populateBean(beanName, mbd, instanceWrapper);        // 创建bean第三步：调用bean的初始化方法，如 init-method 指定的方法        exposedObject = initializeBean(beanName, exposedObject, mbd);    &#125;    catch (Throwable ex) &#123;        if (ex instanceof BeanCreationException bce &amp;&amp; beanName.equals(bce.getBeanName())) &#123;            throw bce;        &#125;        else &#123;            throw new BeanCreationException(mbd.getResourceDescription(), beanName, ex.getMessage(), ex);        &#125;    &#125;    if (earlySingletonExposure) &#123;        Object earlySingletonReference = getSingleton(beanName, false);        if (earlySingletonReference != null) &#123;            if (exposedObject == bean) &#123;                exposedObject = earlySingletonReference;            &#125;            else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123;                String[] dependentBeans = getDependentBeans(beanName);                Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length);                for (String dependentBean : dependentBeans) &#123;                    if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123;                        actualDependentBeans.add(dependentBean);                    &#125;                &#125;                if (!actualDependentBeans.isEmpty()) &#123;                    throw new BeanCurrentlyInCreationException(beanName,                            &quot;Bean with name &#x27;&quot; + beanName + &quot;&#x27; has been injected into other beans [&quot; +                            StringUtils.collectionToCommaDelimitedString(actualDependentBeans) +                            &quot;] in its raw version as part of a circular reference, but has eventually been &quot; +                            &quot;wrapped. This means that said other beans do not use the final version of the &quot; +                            &quot;bean. This is often the result of over-eager type matching - consider using &quot; +                            &quot;&#x27;getBeanNamesForType&#x27; with the &#x27;allowEagerInit&#x27; flag turned off, for example.&quot;);                &#125;            &#125;        &#125;    &#125;    // Register bean as disposable.    try &#123;        registerDisposableBeanIfNecessary(beanName, bean, mbd);    &#125;    catch (BeanDefinitionValidationException ex) &#123;        throw new BeanCreationException(                mbd.getResourceDescription(), beanName, &quot;Invalid destruction signature&quot;, ex);    &#125;    return exposedObject;&#125;\n\n那么doCreateBean()方法是怎么创建bean的实例的呢？我们继续看createBeanInstance()方法。\nprotected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) &#123;    // Make sure bean class is actually resolved at this point.    // 首先确认这个bean所属的类可以被实例化    Class&lt;?&gt; beanClass = resolveBeanClass(mbd, beanName);    if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123;        throw new BeanCreationException(mbd.getResourceDescription(), beanName,                &quot;Bean class isn&#x27;t public, and non-public access not allowed: &quot; + beanClass.getName());    &#125;    Supplier&lt;?&gt; instanceSupplier = mbd.getInstanceSupplier();    if (instanceSupplier != null) &#123;        return obtainFromSupplier(instanceSupplier, beanName, mbd);    &#125;    // 如果有工厂方法，那么就用工厂方法来初始化bean    // 比如配置时在XML中指定factory-method属性，或把带有@Bean注解的方法的返回值作为bean    if (mbd.getFactoryMethodName() != null) &#123;        return instantiateUsingFactoryMethod(beanName, mbd, args);    &#125;    // Shortcut when re-creating the same bean...    boolean resolved = false;    boolean autowireNecessary = false;    if (args == null) &#123;        synchronized (mbd.constructorArgumentLock) &#123;            if (mbd.resolvedConstructorOrFactoryMethod != null) &#123;                resolved = true;                autowireNecessary = mbd.constructorArgumentsResolved;            &#125;        &#125;    &#125;    // 如果在重新创建一个bean    if (resolved) &#123;        if (autowireNecessary) &#123;            // 需要调用有参数的构造方法完成注入的话，那就通过构造方法完成依赖注入            return autowireConstructor(beanName, mbd, null, null);        &#125;        else &#123;            // 否则调用默认构造方法            return instantiateBean(beanName, mbd);        &#125;    &#125;    // Candidate constructors for autowiring?    // 寻找可以用来初始化的构造方法    // 如果有的话，那么再逐个匹配，并调用合适的构造方法完成初始化    Constructor&lt;?&gt;[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName);    if (ctors != null || mbd.getResolvedAutowireMode() == AUTOWIRE_CONSTRUCTOR ||            mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123;        return autowireConstructor(beanName, mbd, ctors, args);    &#125;    // Preferred constructors for default construction?    ctors = mbd.getPreferredConstructors();    if (ctors != null) &#123;        return autowireConstructor(beanName, mbd, ctors, null);    &#125;    // No special handling: simply use no-arg constructor.    // 否则调用默认构造方法完成实例化    return instantiateBean(beanName, mbd);&#125;\n\n设置被注入属性的值完成实例化后就会开始填充这个bean的属性了，继续看populateBean()的实现。\nprotected void populateBean(String beanName, RootBeanDefinition mbd, @Nullable BeanWrapper bw) &#123;    if (bw == null) &#123;        if (mbd.hasPropertyValues()) &#123;            throw new BeanCreationException(                    mbd.getResourceDescription(), beanName, &quot;Cannot apply property values to null instance&quot;);        &#125;        else &#123;            // Skip property population phase for null instance.            return;        &#125;    &#125;    if (bw.getWrappedClass().isRecord()) &#123;        if (mbd.hasPropertyValues()) &#123;            throw new BeanCreationException(                    mbd.getResourceDescription(), beanName, &quot;Cannot apply property values to a record&quot;);        &#125;        else &#123;            // Skip property population phase for records since they are immutable.            return;        &#125;    &#125;    // Give any InstantiationAwareBeanPostProcessors the opportunity to modify the    // state of the bean before properties are set. This can be used, for example,    // to support styles of field injection.    if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123;        for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123;            if (!bp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123;                return;            &#125;        &#125;    &#125;    // 取得在BeanDefinition中设定的property值    // 这些值可以在XML中用 &lt;property&gt; 属性配置    PropertyValues pvs = (mbd.hasPropertyValues() ? mbd.getPropertyValues() : null);    int resolvedAutowireMode = mbd.getResolvedAutowireMode();    if (resolvedAutowireMode == AUTOWIRE_BY_NAME || resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123;        MutablePropertyValues newPvs = new MutablePropertyValues(pvs);        // Add property values based on autowire by name if applicable.        // 用by name的方式完成属性注入        if (resolvedAutowireMode == AUTOWIRE_BY_NAME) &#123;            autowireByName(beanName, mbd, bw, newPvs);        &#125;        // Add property values based on autowire by type if applicable.        // 用by type的方式完成属性注入        if (resolvedAutowireMode == AUTOWIRE_BY_TYPE) &#123;            autowireByType(beanName, mbd, bw, newPvs);        &#125;        pvs = newPvs;    &#125;    if (hasInstantiationAwareBeanPostProcessors()) &#123;        if (pvs == null) &#123;            pvs = mbd.getPropertyValues();        &#125;        for (InstantiationAwareBeanPostProcessor bp : getBeanPostProcessorCache().instantiationAware) &#123;            // 完成@Autowire、@Inject等注解的依赖注入            PropertyValues pvsToUse = bp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName);            if (pvsToUse == null) &#123;                return;            &#125;            pvs = pvsToUse;        &#125;    &#125;    boolean needsDepCheck = (mbd.getDependencyCheck() != AbstractBeanDefinition.DEPENDENCY_CHECK_NONE);    if (needsDepCheck) &#123;        PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching);        checkDependencies(beanName, mbd, filteredPds, pvs);    &#125;    if (pvs != null) &#123;        // 注入配置文件中&lt;property&gt;配置的属性        applyPropertyValues(beanName, mbd, bw, pvs);    &#125;&#125;\n\n上面代码中PropertyValues pvsToUse = bp.postProcessProperties(pvs, bw.getWrappedInstance(), beanName)这一行完成了@Autowire等注解的注入，那么继续看它的实现。\n// AutowiredAnnotationBeanPostProcessorpublic PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) &#123;    // 查找@Autowired、@Inject等注解，并解析出依赖注入的元数据    InjectionMetadata metadata = findAutowiringMetadata(beanName, bean.getClass(), pvs);    try &#123;        // 完成注入        metadata.inject(bean, beanName, pvs);    &#125;    catch (BeanCreationException ex) &#123;        throw ex;    &#125;    catch (Throwable ex) &#123;        throw new BeanCreationException(beanName, &quot;Injection of autowired dependencies failed&quot;, ex);    &#125;    return pvs;&#125;\n\n我创建了一个DemoController，并在其中用属性注入的方式声明需要注入DemoService（@Autowired private DemoService demoService），然后在findAutowiringMetadata这行下个断点调试，就能看到这样的结果：\n\n具体的注入是这样实现的：\nprotected void inject(Object bean, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable &#123;    // 得到要注入的属性    Field field = (Field) this.member;    Object value;    if (this.cached) &#123;        try &#123;            value = resolveCachedArgument(beanName, this.cachedFieldValue);        &#125;        catch (BeansException ex) &#123;            // Unexpected target bean mismatch for cached argument -&gt; re-resolve            this.cached = false;            logger.debug(&quot;Failed to resolve cached argument&quot;, ex);            value = resolveFieldValue(field, bean, beanName);        &#125;    &#125;    else &#123;        // 通过BeanFactory解决依赖关系        value = resolveFieldValue(field, bean, beanName);    &#125;    // 解决依赖关系并得到被注入对象的实例后，通过反射把值设置进去    if (value != null) &#123;        ReflectionUtils.makeAccessible(field);        field.set(bean, value);    &#125;&#125;\n\n调用bean的初始化方法populateBean()执行完毕后就会调用initializeBean()方法来调用bean的初始化方法（XML或@Bean注解配置的initMethod，或Bean实现的InitializingBean#afterPropertiesSet()方法）\nprotected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) &#123;    // 调用一系列的aware接口实现    invokeAwareMethods(beanName, bean);    Object wrappedBean = bean;    if (mbd == null || !mbd.isSynthetic()) &#123;        // 初始化前调用BeanPostProcessor        wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName);    &#125;    try &#123;        // 调用初始化方法        invokeInitMethods(beanName, wrappedBean, mbd);    &#125;    catch (Throwable ex) &#123;        throw new BeanCreationException(                (mbd != null ? mbd.getResourceDescription() : null), beanName, ex.getMessage(), ex);    &#125;    if (mbd == null || !mbd.isSynthetic()) &#123;        // 初始化后调用BeanPostProcessor        wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName);    &#125;    return wrappedBean;&#125;private void invokeAwareMethods(String beanName, Object bean) &#123;    if (bean instanceof Aware) &#123;        if (bean instanceof BeanNameAware beanNameAware) &#123;            beanNameAware.setBeanName(beanName);        &#125;        if (bean instanceof BeanClassLoaderAware beanClassLoaderAware) &#123;            ClassLoader bcl = getBeanClassLoader();            if (bcl != null) &#123;                beanClassLoaderAware.setBeanClassLoader(bcl);            &#125;        &#125;        if (bean instanceof BeanFactoryAware beanFactoryAware) &#123;            beanFactoryAware.setBeanFactory(AbstractAutowireCapableBeanFactory.this);        &#125;    &#125;&#125;\n\n启动部分的收尾工作在完成刷新application context之后，Spring Boot会发布ApplicationStartedEvent和ApplicationReadyEvent事件，调用各个Runner，然后应用正式启动开始运行\n参考文章\nspring boot 启动流程分析\nspring boot 中的spring factories 机制\n应用启动过程——准备应用上下文\n应用启动过程——BootstrapContext\nSpringApplication中文文档\nSpring Boot应用Main函数入口 Primary Source\nSpring源码分析之ConversionService\nSpring ConversionService 类型转换（一）Converter\n走心Springboot源码解析： 三、prepareEnvironment()环境配置 解析配置文件信息\n\n","categories":["学知识"],"tags":["Java","Spring Boot"]},{"title":"分环境为Git指定单独的配置","url":"/specify-different-git-config-for-different-environments.html","content":"有的小伙伴，在公司需要使用公司的Git来干活，同时自己也有些托管在GitHub等其他仓库的代码，在这种情况下会有一个烦恼，就是怎么样在多个仓库之间使用不同的配置。比如在公司仓库和个人仓库之间使用不同的身份信息，或者在公司仓库和个人仓库之间，使用不同的GPG签名。\n在2.13版本之前，大概你就只能单独为每一个仓库单独设定这些信息了，但是，在2.13这个版本中，Git引入了一个名为“按条件引入”(Conditional includes)的功能。这个功能允许用户通过指定一定的条件，来使Git从不同的配置文件中取得配置项。\n\n\n准备工作“按条件引入”功能可以根据目录或者分支来决定使用哪个配置，显然根据分支无法区分工作环境和私人环境，所以首先需要分别为工作环境和私人环境创建各自的目录。比如将目录划分为这样：\nprojects├── work     &lt;== 工作相关└── personal &lt;== 私人项目\n\n拆分配置接下来，需要为全局、工作、私人分别编写配置文件。本文中，全局配置文件.gitconfig位于$HOME下，各环境的配置文件位于~/.config/git下。\n全局配置文件[core]  excludesfile = ~/.gitignore_global  editor = vim[includeIf &quot;gitdir:~/projects/work/&quot;]  path = ~/.config/git/gitconfig-work[includeIf &quot;gitdir:~/projects/personal/&quot;]  path = ~/.config/git/gitconfig-personal\n\n第4行和第6行的includeIf段，就是“按条件引入”的配置。在这里需要注意这几点：\n\ngitdir:参数用于匹配本地Git仓库的路径。\n如果文件系统是大小写不敏感的，并且路径同时包含大写和小写字母，那么需要使用gitdir/i:来匹配。\n如果要匹配某个目录下面的所有子目录，那么在路径最后需要加上/或者/**。（实际上，Git会自动在末尾的/后面附加**）\n反之，如果只要匹配到某一个目录，而不递归包含其下面的所有子目录，那么路径末尾就不要有/。\n\n\npath指定了在满足条件时要使用的配置文件\n\n综上所述，位于~/projects/work/下的所有Git仓库，都套用~/.config/git/gitconfig-work中的配置；位于~/projects/personal/下的所有Git仓库，都套用~/.config/git/gitconfig-personal中的配置。\n私人环境和工作环境的配置文件这两个环境各自的配置文件就没有什么新鲜的东西了，就只有面向各个环境的身份信息、GPG签名信息等。\n[user]  signingkey = 1122334455667788  name = Your Name  email = test@example.com[commit]  gpgsign = true  template = ~/.stCommitMsg[gpg]  program = /usr/local/bin/gpg\n\n检查配置写完了配置文件，还是要检查一下是不是生效了的。首先，在一个不包含任何Git仓库的位置执行git config -l，返回的信息中就只有全局配置中的值。\ncore.excludesfile=~/.gitignore_globalcore.editor=vimincludeif.gitdir:~/projects/mininglamp/.path=~/.config/git/gitconfig-mininglampincludeif.gitdir:~/projects/personal/.path=~/.config/git/gitconfig-default\n\n然后，进入私人环境的某个Git仓库，再执行git config -l，就可以看到私人环境相关的配置文件被引入了。（仓库自己的配置其实也会被显示出来，这里为了减小篇幅将其略掉了）\ncore.excludesfile=~/.gitignore_globalcore.editor=vimincludeif.gitdir:~/projects/mininglamp/.path=~/.config/git/gitconfig-mininglampincludeif.gitdir:~/projects/personal/.path=~/.config/git/gitconfig-defaultuser.signingkey=1122334455667788user.name=Your Nameuser.email=test@example.comcommit.gpgsign=truecommit.template=~/.stCommitMsggpg.program=/usr/local/bin/gpg\n\n参考资料\nGit官方文档的Includes部分\nGit 2.13 有趣的新功能介绍 - GitHub\nConditional Includes For Git Config\n\n","categories":["小技巧"],"tags":["Git"]},{"title":"在Spring Boot应用中启用HTTPS","url":"/spring-boot-enable-https.html","content":"在application.yml中添加如下配置，即可在Spring Boot项目中开启HTTPS。\nserver:  # 如果有需要的话，配置应用监听HTTPS的默认端口  port: 443  ssl:    # 我把证书放在了 resources/cert/certificate.pfx下    # 使用时需要按照实际情况修改下面的路径    key-store: classpath:cert/certificate.pfx    key-store-password: pA55w0Rd    # 证书类型需要按照实际类型填写    # 本例使用PKCS12格式，所以写PKCS12。如果是JKS格式，那么需要填写JKS    # 因为PKCS12比JKS有更好的通用型，所以个人建议使用PKCS12格式    key-store-type: PKCS12    enabled: true\n\n\n\n使用如下命令即可将JKS证书转换为PKCS12证书：\nkeytool -importkeystore -srckeystore keystore.jks -destkeystore keystore.pfx -deststoretype pkcs12\n\n[^1]: How to enable HTTPS in a Spring Boot Java application[^2]: Spring Boot Docs - Configure SSL\n","categories":["学知识"],"tags":["Java","Spring Boot"]},{"title":"在同一个类的两个方法内部互相调用中，如何使AOP生效","url":"/spring-how-to-make-aop-effective-in-internal-calls.html","content":"熟悉Spring AOP的都知道，如果同一个类中的两个方法在内部互相调用，那么此时AOP是不会生效的，因为Spring AOP是通过代理类来实现的，而类内部的方法调用并不会走到代理对象。那么，有没有办法让内部调用的时候也让AOP生效呢？万能的ChatGPT告诉我，方法是有的，还有好几种。\n\n\n使用@Autowired通过代理调用这个方法的思路是，利用@Autowired注解注入自身的代理对象，然后通过代理对象完成方法调用。\n@Servicepublic class DemoService &#123;    @Autowired    private DemoService demoService;    public String helloWorld() &#123;        demoService.test();        return &quot;Hello World!&quot;;    &#125;    public String test() &#123;        return &quot;Test&quot;;    &#125;&#125;\n\n这个操作我也在本地的一个Spring Boot项目中验证确实是可行的，前提是要在application.properties里面配置允许循环引用spring.main.allow-circular-references=true。而且这个操作有一个缺点，就是自身代理对象demoService必须通过字段注入的方式完成依赖注入，如果用构造方法注入，启动的时候就会报循环依赖错误导致项目无法成功启动（不用想也知道，在构造方法里面依赖自己肯定不行啊）。\n我个人并不喜欢这个方法，一个原因是因为我不喜欢循环引用，另一个原因是我不喜欢字段注入，毕竟Spring早就推荐改成构造方法注入了。所以，我们继续看下一个方法。\n使用AopContext.currentProxy()另一个方法是使用AopContext#currentProxy()静态方法获取到当前的代理对象，也就是对象自己，然后再通过这个代理对象进行方法调用。\n@Servicepublic class DemoService &#123;    public String helloWorld() &#123;        final DemoService demoService = (DemoService) AopContext.currentProxy();        demoService.test();        return &quot;Hello World!&quot;;    &#125;    public String test() &#123;        return &quot;Test&quot;;    &#125;&#125;\n\n但是如果就这样运行，你就会得到这样一条错误信息：\njava.lang.IllegalStateException: Cannot find current proxy: Set &#x27;exposeProxy&#x27; property on Advised to &#x27;true&#x27; to make it available, and ensure that AopContext.currentProxy() is invoked in the same thread as the AOP invocation context.\n\n看来ChatGPT没把话说全啊，好在我的IDEA里面装了通义灵码，把代码上下文和这个异常抛给它之后，它告诉我要通过注解@EnableAspectJAutoProxy(exposeProxy = true)配置Spring AOP允许暴露当前代理对象。那么按照它的说法，我给切面配置类加上这个注解：\n@Aspect@Component@EnableAspectJAutoProxy(exposeProxy = true) // &lt;-- 就这个，它的默认值是falsepublic class DemoAopAdvice &#123;    private static final Logger LOGGER = LoggerFactory.getLogger(DemoAopAdvice.class);    @Pointcut(&quot;execution(* com.example.demo.DemoService.*(..))&quot;)    public void test() &#123;    &#125;    @Before(&quot;test()&quot;)    public void before() &#123;        LOGGER.info(&quot;Before&quot;);    &#125;&#125;\n\n再重启应用，就发现demoService里面的内部调用成功触发了AOP切面。\n通义灵码还提醒我，在多线程环境中，要确保AopContext#currentProxy()必须在与AOP调用相同的线程中调用。此外，根据currentProxy()的JavaDoc，调用它的方法也必须经过了AOP调用，否则会抛出IllegalStateException异常。点进currentProxy()方法的实现，发现它内部是用ThreadLocal来保存代理对象的，同时在这个类中还有一个setCurrentProxy(Object)方法来把当前的代理对象保存到ThreadLocal中。下断点调试后发现，setCurrentProxy(Object)这个方法会先被执行，然后再走到我们实际调用的方法。这正好解释了为什么要注意在相同的线程中调用AopContext#currentProxy()，并且调用它的方法必须是经过了AOP调用的，因为不这样的话ThreadLocal中根本就没东西可拿。\n","categories":["学知识"],"tags":["Spring","AOP"]},{"title":"在Spring中通过配置类注入配置文件的值","url":"/spring-injecting-property-value-with-configuration-bean.html","content":"我们在开发过程中，为了保证项目的灵活性，经常会选择将一些值放在配置文件中，并在代码中将它注入并使用。将值注入代码最常见的一种方法，则是使用@Value()注解搭配SpEL直接注入我们需要的属性。但是鲁迅先生有云：从来如此，便对吗？这里，我想介绍一个我个人认为更好的实践：通过配置类来注入属性的值。\n\n\n旧的做法有什么问题假设我们现在有这样一个application.yml，其中credentials部分是我自定义的一个属性：\nserver:  port: 9999credentials:  token: A_VERY_SECRET_TOKEN\n\n然后，我们会在用到它的地方，直接通过@Value注解把它注入进来，就像这样：\n@Componentpublic class SomeService &#123;  @Value(&quot;$&#123;credientials.token&#125;&quot;)  private String token;&#125;\n\n好像没什么问题对吧，直接用表达式把值拿进来，然后该怎么用就怎么用。但是我不知道你们有没有注意过，这种做法其实既不利于后期重构，也不利于为代码生成好的文档。\n比如说，这个值在多个类中都有被引用，但某一天，我们觉得这个名字不够直观，我们想改成contactServiceAppToken，那么我们就只能先改掉属性的名字，然后在代码里面全文替换，把credentials.token批量替换成credentials.contactServiceAppToken。我不知道你们是怎么想的，我每次做这种文本批量替换都很慌，生怕一个没看见而改掉了不应该改的东西。\n而对于生成文档，我们都知道，在Java代码上面我们可以使用JavaDoc来编写文档，阐明这个类的作用等等。而对于YAML文件，则没有类似的东西，我们只能在属性上面写普通的注释。可是，大篇幅的注释又有可能会影响YAML文件的可读性，更不用说有谁会在看代码的时候专程去看YAML文件？\n所以，我会建议团队使用配置类，也就是本文下面要讲的这个东西，来管理和注入这些自定义的属性。\n来个示例首先，我们需要创建一个配置类，来给这些属性找一个家。\n// 这个注解是重点，说明我们要把配置文件的 credentials 部分映射到这里@ConfigurationProperties(&quot;credentials&quot;)public class Credentials &#123;  // 属性名与变量名保持一致即可，Spring会自动处理两者的绑定关系  // 同时，Spring会自动完成不同命名方式的转换，比如 kebab-case 变成 camelCase  private String token;  //getters and setters&#125;\n\n接下来，在要使用这些属性的地方，把这个配置类注入，然后直接get属性的值，就可以了。\n@Componentpublic class SomeService &#123;  private final Credentials credentials;  public SomeService(Credentials credentials) &#123;    this.credentials = credentials;  &#125;  public void someMethod() &#123;    final String token = credentials.getToken();    // ......  &#125;&#125;\n\n与直接取值的方法比较起来，使用配置类有这么几个优点：\n\n如果在重构的时候要改变属性名，那么我们只需要修改配置文件里面的属性名，和配置类里面的属性名。当然要记得使用IDE里面的重构功能改名，这样IDE会自动分析这个属性的引用，并自动改正过来。\n使用配置类还可以方便我们生成文档。如果直接在配置文件里面写文档，一方面是不一定易读，另一方面，也不是所有人都会想到在配置文件里面还有文档。而使用配置类的话，我们只需要在类上面加上JavaDoc就好了。\n而且，我们还不需要担心打错字，导致@Value注入失败而使得应用起不来。虽然这不是什么大问题，改正就行了，但毕竟还是麻烦。\n\n多层属性怎么办上面只是演示了只有一级子属性的情况，如果下面包含了多层属性，那配置类应该怎么写呢？\n假设现在配置文件变成了这样：\ncredentials:  token:    contact: TOKEN_FOR_CONTACT_API    user: TOKEN_FOR_USER_API  oauth:    client-id: CLIENT_ID    client-secret: CLIENT_SECRETendpoint:  contact:    v1: URL_FOR_CONTACT_API_VERSION_1    v2: URL_FOR_CONTACT_API_VERSION_2\n\n对于credentials部分，因为里面子属性的名字大致是确定的，我们用一个内部类就可以搞定（其实写在单独的类里面也可以，只是我不喜欢那么做）。\n@ConfigurationProperties(&quot;credentials&quot;)public class Credentials &#123;  private Token token;  private Oauth oauth;  //getters and setters    public static class Token &#123;    private String contact;    private String user;    //getters and setters  &#125;  public static class Oauth &#123;    private String clientId;    private String clientSecret;    //getters and setters  &#125;&#125;\n\n取值的时候呢，逐层取到就好了：\n@Componentpublic class SomeService &#123;  private final Credentials credentials;  public SomeService(Credentials credentials) &#123;    this.credentials = credentials;  &#125;  public void someMethod() &#123;    final String contactApiToken = credentials.getToken().getContact();    // ......  &#125;&#125;\n\n但是对于endpoint部分，因为里面的值是某个API各个版本的URL，考虑到API还有可能会有新版本，每加一个版本都要再改配置类有点麻烦，所以我们可以直接用一个Map来存放。\n@ConfigurationProperties(&quot;endpoint&quot;)public class Endpoint &#123;  private Map&lt;String, String&gt; contact;  //getters and setters&#125;\n\n在取值的时候，就还是一样的套路，注入这个配置类，然后从Map中取值就行了。Map的key就是属性名，比如v1，值就是属性的值。当然这样做的话，就要处理一下取到null的情况。\n@Componentpublic class SomeService &#123;  private final Endpoint endpoint;  public SomeService(Endpoint endpoint) &#123;    this.endpoint = endpoint;  &#125;  public void someMethod() &#123;    final String contactV1Url = endpoint.getContact().get(&quot;v1&quot;);    if (contactV1Url == null) &#123;      // handle it here    &#125;    // ......  &#125;&#125;\n\n给配置文件加上自动提示其实，Configuration properties配置类除了可以方便我们管理属性之外，他还可以搭配spring-boot-configuration-processor来实现配置文件的自动提示，当然这也需要IDE的支持。\n在pom.xml中加入如下依赖：\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;    &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;\n\n然后在编写完配置类之后，执行一下build操作，或者mvn compile，来让它帮我们生成一个additional-spring-configuration-metadata.json文件。有了这个文件之后，IDE就会参照它在配置文件里面给我们提供自动提示。\n","categories":["小技巧"],"tags":["Java","Spring Boot","ConfigurationProperties"]},{"title":"使用SSH config简化SSH连接","url":"/ssh-config-file.html","content":"如果你有很多的服务器要连接，如果对你来说记住那些服务器的地址、端口实在是一种痛苦，如果你一直在寻找一种能够简化在命令行下连接SSH服务器的办法，那么，本文将给你提供一种解决问题的思路，那就是，使用SSH的config文件。\n\n\nSSH config文件是什么Open SSH客户端配置文件，允许你以配置项的形式，记录各个服务器的连接信息，并允许你使用一个定义好的别名来代替其对应的ssh命令参数。\nSSH config文件该怎么用创建SSH config文件通常来说，该文件会出现在两个地方，一个是/etc/ssh/ssh_config，一个是~/.ssh/config。\n/etc/ssh/ssh_config文件通常用来定义全局范围上的SSH客户端参数，而~/.ssh/config则被用来定义每个用户自己的SSH客户端的配置。我们将要修改的，就是位于用户目录下的config文件。\n如果~/.ssh/config文件不存在，那么也不用着急，这是正常的，只需要执行如下命令，即可新建一个空白的config文件\ntouch ~/.ssh/config\n\n编写config条目假如说，我们想连接到一台服务器，它的地址是example.server.com，端口号是2222，以用户admin登陆，并使用~&#x2F;.ssh&#x2F;id_rsa这个私钥验证身份。那么，我们需要在命令行里输入：\nssh admin@example.server.com -p 2222 -i ~/.ssh/id_rsa\n\n嗯好吧，-i参数可以省略，但即使这样，命令还是很长，对吧？\n那么我们把这个服务器的连接参数写到config文件里，就变成了这个样子：\n# 此处我为了美观起见，给每个子条目都缩进了一层，实际使用时缩进不影响文件的效果。Host sample    Hostname        example.server.com    Port            2222    User            admin    Identityfile    ~/.ssh/id_rsa\n\n嗯，在这里，它还有了一个新名字，叫sample。\n然后，我们只需要：\nssh sample\n\n就可以连接到这台主机了。\n创建通配符规则有的时候，我们需要连接多台不同的主机，那难道我们需要针对每个主机都写一遍规则吗？\n答案是不一定。如果你要连接的主机，它们的域名有一定规律可循，那么我们可以用通配符来匹配这一系列的主机。\n比如，公司里针对开发和测试环境，各创建了一系列集群，同时各个集群中又根据负责的业务不同，有多台负载均衡的主机，那么我们可以这样写：\n# 首先匹配所有主机，在这里配置好我们的用户名、私钥等参数Host *    User            boris1993    IdentityFile    ~/.ssh/id_rsa# 开发环境Host dev-*    HostName    %h.dev.mycompany.com# 测试环境Host staging-*    HostName    %h.staging.mycompany.com\n\n接下来，我们只需要执行类似ssh dev-user，就可以连接到开发环境的负责用户管理的服务器上了。\n而实现这个操作的重点，我想你已经注意到了，一个是Host配置中的*，另一个就是配置文件里面的%h。星号我们都知道，是个通配符，放在这里就意味着它会匹配所有以dev-*开头的主机名；而%h是一个占位符，它会把你在SSH命令中destination部分的输入取出来放在这里，所以当我们执行ssh dev-user的时候，实际上命令会被展开成ssh dev-user.dev.mycompany.com。\n配置通过跳板机连接如果公司规定所有服务器都需要先SSH到跳板机，然后才能连接到具体的服务器，那该怎么办？把ssh_config文件放在跳板机上吗？\n确实这是一个解决方案，但不是唯一解。因为我们还有一个配置参数叫ProxyJump。它，就是用来指定，在连接这个服务器之前，需要通过哪个跳板机。\n写起来，是这样的：\n# 其他部分略# 跳板机Host bastion    HostName    bastion.mycompany.com# 服务器Host dev-*    HostName    %h.dev.mycompany.com     ProxyJump   bastion\n\n如果你的SSH版本早于7.3，那么很可惜它是不支持ProxyJump这条配置的。但是不要灰心，它支持另一条配置，ProxyCommand。\n# 其他部分略# 跳板机Host bastion    HostName    bastion.mycompany.com# 服务器Host dev-*    HostName        %h.dev.mycompany.com     ProxyCommand    ssh -W %h:%p bastion \n\n然后，我们继续执行ssh dev-user就可以了，SSH会自动先连接到跳板机，然后在跳板机中再连接到我们要去的服务器。\n如果你们公司安全做的非常好，需要通过数个跳板机来连接服务器，那么只需要给每个跳板机都配置上更外层的跳板就好了，像这样：\n# 其他部分略# 最外层跳板机Host bastion-outer    HostName    bastion-outer.mycompany.com# 下一层跳板机Host bastion-inner    HostName    bastion-inner.mycompany.com    ProxyJump   bastion-outer# 服务器Host dev-*    HostName    %h.dev.mycompany.com     ProxyJump   bastion-inner\n\n这样，SSH就会先连接到bastion-outer，然后连到bastion-inner，然后连到dev-user。理论上，你可以在这里无限套娃（逃\n这玩意有意思，我还想了解更多！好吧，为了满足你的好奇心，我这里为你提供了3篇博客供你参考。当然，这三篇博客也是我编写本文时的参考文档。\n多个 SSH KEY 的管理\nHow To Configure Custom Connection Options for your SSH Client\nSimplify Your Life With an SSH Config File\n另外，您也可以阅读ssh_config的手册页，来获得最原始的信息，阅读该手册的命令是：\nman ssh_config\n","categories":["小技巧"],"tags":["Shell","SSH","ssh_config","ProxyCommand","ProxyJump","跳板机"]},{"title":"使用Tailscale部署虚拟局域网","url":"/setting-up-tailscale.html","content":"最近折腾了下用Tailscale搭建虚拟局域网，在这里记录一下折腾的过程和一些心得。\n\n\n什么是Tailscale说起Tailscale，就不得不提到最近很火的Mesh VPN这个概念。\n经典的VPN是作为一个网关，让外网用户通过它来访问内网。比如传统的L2TP VPN：\n\n但是Mesh VPN提出了一种全新的拓扑结构，这种结构中每个节点之间都相互连接，不再存在中心的VPN服务器。\n\n上图提到的Wireguard就是一个Mesh VPN的实现，但是Wireguard配置比较繁琐，维护成本也比较高，所以Tailscale在基于Wireguard的基础上做了一些优化，比如用于NAT穿透的DERP协议、建立了公网coordinator服务器用于下发配置，以及集成了第三方SSO用于认证和权限管理。\n如何安装和使用Tailscale安装Tailscale很简单（除了国内群晖），跟着官网的Quickstart走就行了。简单来说就是先在官网注册账号，然后在电脑上安装Tailscale并使用相同账号登陆，然后这些机器就处于同一个虚拟局域网上了。\n登陆的时候注意，千万不要用GitHub或者Google认证，原因我不用说，你懂的。就算电脑上你有办法，手机上也会很闹心。\n在机器成功加入Tailscale虚拟局域网后，就可以在Tailscale控制面板里面看到这些机器的IP地址了。\n\n这时候，就可以直接使用这些IP地址访问到对应的机器了。\n国内群晖安装Tailscale因为国内群晖的套件商店下架了所有VPN相关的套件，所以只能到官网的Tailscale Packages - stable track手动下载对应DSM版本的spk，然后到套件中心手动安装。安装成功后就可以在主菜单中看到Tailscale的图标，点击启动并在新页面中使用相同账号登陆，然后就能用了。\nTailnet和Magic DNS当然Tailscale并不仅支持通过IP地址访问机器，还可以通过域名访问。这里请放心，Tailnet的域名解析是通过你本机的Tailnet客户端解析的。\n要启用这个功能，首先你需要选择一个Tailnet域名。Tailscale会给你一系列域名让你选，选好之后就会在控制面板的DNS页面出现。\n\n然后，在Magic DNS栏目下面点Enable Magic DNS就可以了。\n\n在这之后，就可以直接用控制面板中的设备名，或者设备名.Tailnet域名的方式访问你的设备了。比如我要SSH到ds218plus这台机器上，我就可以ssh ds218plus或ssh ds218plus.tailnet-a8cf.ts.net。\n访问内网机器很慢在我试用了几天之后，我发现了一个问题，如果节点双方都有IPv6的话，互联速度很快，但是如果节点一方没有IPv6的话，速度就出奇的慢。遂用tailscale status命令看了下连接情况，不出意外，看到了这样的返回：\n$ tailscale status100.98.89.5     macbook-pro          boris1993@   macOS   -100.88.234.12   boris-iphone         boris1993@   iOS     offline100.105.198.123 ds218plus            boris1993@   linux   active; relay &quot;sfo&quot;, tx 1778904 rx 63968652\n\n划重点，relay &quot;sfo&quot;，这说明我连接到ds218plus的通信，是经过位于旧金山的DERP服务器中转的，所以会慢。\n虽然在What firewall ports should I open to use Tailscale?里面说，\n\nNearly all of the time, you don’t need to open any firewall ports for Tailscale. Tailscale uses various NAT traversal techniques to safely connect to other Tailscale nodes without manual intervention—it “just works.”\n\n也没错，它确实work，但是我真的没法说它work的well。好在，文章下面提到，除了443/tcp之外，Tailscale还用到41641/udp来让Wireguard节点互联，以及3478/udp用于STUN协议。\n在我的这个使用场景中，ds218plus作为服务器，那么我就在ds218plus所在的网络中做了两件事，给41641/udp配置端口转发，并在防火墙上放行这个端口。然后我在作为客户端的macbook-pro上执行tailscale down和tailscale up来重启Tailscale连接。这时候再用tailscale status命令检查连接情况，发现输出变成了这样：\n$ tailscale status100.98.89.5     macbook-pro          boris1993@   macOS   -100.88.234.12   boris-iphone         boris1993@   iOS     offline100.105.198.123 ds218plus            boris1993@   linux   active; direct 121.70.200.131:41641, tx 5884 rx 8500\n\n接着划重点，direct，说明现在节点之间已经是直连了，随便做点什么测试一下，果然快了很多。\n不过端口转发只是权宜之计，毕竟没法同时让41641/udp转发给多个机器，只能解决一台机器不能直连的问题。要想彻底解决，还是得用IPv6。\n","categories":["瞎折腾"],"tags":["Tailscale","内网穿透","VPN","Virtual LAN","虚拟局域网"]},{"title":"SwiftUI学习记录 - 切换TabView的tab时触发震动反馈","url":"/swiftui-trigger-haptic-feedback-when-changing-tabs.html","content":"最近在折腾iOS开发的时候，想实现在切换TabView的tab时触发震动反馈，因为一边写一边学嘛，基础不牢，只能在网上瞎搜，试了各种方法都不奏效。直到前两天看到Ice Cubes: for Mastodon的这段代码，豁然开朗。\n\n\n之前写的时候，只知道给selection传一个绑定的对象，但是看完IceCubes的代码，再一看selection参数的文档，恍然大悟，这个参数要的其实是一个SelectionValue的绑定对象，所以我其实可以在这个对象的set方法上面做文章。\nstruct ContentView: View &#123;    @State    private var selectedTab = Tab.Timeline    var body: some View &#123;        TabView(selection: .init(            get: &#123;                selectedTab            &#125;,            set: &#123; newTab in                 selectedTab = newTab                // 在TabView更新selection的时候触发震动反馈就行了                if (globalState.isHapticFeedbackEnabled) &#123;                    HapticsHelper.playHapticFeedback()                &#125;            &#125;        )) &#123;            // Views        &#125;    &#125;&#125;\n\n顺便提一嘴我之前走过的一个弯路，就是试图在TabView上用.onTapGesture()方法，结果虽然能震了，但是tab不会切换，当时还花了好长时间找怎么调用原本的.onTapGesture()来切tab，当然最后一无所获，最后认输，把震动反馈这个功能忍痛砍了……\n","categories":["学知识"],"tags":["iOS开发","SwiftUI","TabView","Haptic Feedback","震动反馈"]},{"title":"如何将项目部署至Tomcat的根下","url":"/tomcat-deploy-to-root.html","content":"有两种方法可以实现将项目部署到Tomcat的根下。\n\n删掉自带的ROOT&#x2F;目录，然后将项目的war包重命名为ROOT.war\n\n将项目正常部署(假设war包名为your_project.war)，然后修改conf/server.xml中的Context Root为如下内容：\n\n\n&lt;Context path=&quot;&quot; docBase=&quot;your_project&quot; debug=&quot;0&quot; reloadable=&quot;true&quot;&gt;&lt;/Context&gt;\n\n参考文献：https://stackoverflow.com/questions/5328518/deploying-my-application-at-the-root-in-tomcat\n","categories":["小技巧"],"tags":["Tomcat"]},{"title":"配置Tomcat监控class和lib变更并自动重新加载","url":"/tomcat-monitor-classes-and-auto-reload.html","content":"在context.xml的Context标签中，设定reloadable=&quot;true&quot;即可。\n&lt;Context reloadable=&quot;true&quot;&gt;    &lt;!-- Other configurations --&gt;&lt;/Context&gt;\n\n配置完毕后重启Tomcat使配置生效，然后Tomcat在监控到项目的class或lib有变化后，就会自动重新加载这个webapp。\n但是这个功能会显著增加Tomcat的性能消耗，故不建议在生产环境中使用。\n","categories":["小技巧"],"tags":["Tomcat"]},{"title":"SwiftUI学习记录 - 使用Alamofire加载JSON并初始化列表","url":"/swiftui-initialize-list-with-remote-data.html","content":"这两天又捡起来了之前开的一个老坑，继续完成X岛揭示板的iOS客户端，而且刚刚完成了从JSON初始化版面列表的功能。这部分感觉最难的还是上手Alamofire，因为它返回结果不像我平时做Web开发那样通过方法返回（也有可能是我没学到位），而是要把反序列化得到的对象传给一个回调方法。而这个思路的差异也导致我刚开始学的时候非常的痛苦，因为怎么也找不到我想要的那种返回方式。我相信应该不止我一个人会遇到这种情况，所以打算在这里把完整的实现过程记录在这里，并希望后面有类似情况的同志能因为这篇文章而少掉几根头发。\n\n\n定义JSON对应的结构体X岛揭示板的版面列表API会返回一个类似这样的JSON：\n[  &#123;    &quot;id&quot;: &quot;4&quot;,    &quot;sort&quot;: &quot;1&quot;,    &quot;name&quot;: &quot;综合&quot;,    &quot;status&quot;: &quot;n&quot;,    &quot;forums&quot;: [      &#123;        &quot;id&quot;: &quot;-1&quot;,        &quot;name&quot;: &quot;时间线&quot;,        &quot;msg&quot;: &quot;这里是匿名版最新的串&quot;      &#125;,      &#123;        &quot;id&quot;: &quot;23&quot;,        &quot;fgroup&quot;: &quot;3&quot;,        &quot;sort&quot;: &quot;12&quot;,        &quot;name&quot;: &quot;暴雪游戏&quot;,        &quot;showName&quot;: &quot;暴雪游戏&quot;,        &quot;msg&quot;: &quot;•本版发文间隔为15秒。&quot;,        &quot;interval&quot;: &quot;15&quot;,        &quot;safe_mode&quot;: &quot;0&quot;,        &quot;auto_delete&quot;: &quot;0&quot;,        &quot;thread_count&quot;: &quot;72&quot;,        &quot;permission_level&quot;: &quot;0&quot;,        &quot;forum_fuse_id&quot;: &quot;0&quot;,        &quot;createdAt&quot;: &quot;2012-05-25 21:21:21&quot;,        &quot;updateAt&quot;: &quot;2015-04-21 12:30:39&quot;,        &quot;status&quot;: &quot;n&quot;      &#125;    ]  &#125;]\n\n所以，我们可以创建一个这样的结构体来用来反序列化它：\nstruct ForumGroup: Codable, Identifiable &#123;    var id: String    var sort: String    var name: String    var status: String    var forums: [Forum]    private enum CodingKeys: String, CodingKey &#123;        case id        case sort        case name        case status        case forums    &#125;&#125;struct Forum: Codable, Identifiable &#123;    var id: String = &quot;&quot;    var fGroup: String? = &quot;&quot;    var sort: String? = &quot;&quot;    var name: String = &quot;&quot;    var showName: String? = &quot;&quot;    var msg: String = &quot;&quot;    var interval: String? = &quot;&quot;    var threadCount: String? = &quot;&quot;    var permissionLevel: String? = &quot;&quot;    var forumFuseId: String? = &quot;&quot;    var createdAt: String? = &quot;&quot;    var updateAt: String? = &quot;&quot;    var status: String? = &quot;&quot;    private enum CodingKeys: String, CodingKey &#123;        case id        case fGroup        case sort        case name        case showName        case msg        case interval        // 因为X岛揭示板的API存在CamelCase和snake_case混用的情况        // 所以需要CodingKeys来配置正确的映射        case threadCount = &quot;thread_count&quot;        case permissionLevel = &quot;permission_level&quot;        case forumFuseId = &quot;forum_fuse_id&quot;        case createdAt        case updateAt        case status    &#125;&#125;\n\n编写网络请求创建一个新的Swift文件AnoBbsApiClient，编写如下代码：\nfinal class AnoBbsApiClient &#123;    private static let logger = LoggerHelper.getLoggerForNetworkRequest(name: &quot;AnoBbsApiClient&quot;)    public static func loadForumGroups(        completion:@escaping ([ForumGroup]) -&gt; Void,        failure:@escaping (String) -&gt; Void    ) &#123;        let url = URL(string: XdnmbAPI.GET_FORUM_LIST)!        AF.request(url, method: .get, interceptor: .retryPolicy) &#123; $0.timeoutInterval = 10 &#125;            .cacheResponse(using: .cache)            .validate()            .responseDecodable(of: [ForumGroup].self) &#123; response in                switch response.result &#123;                case .success(let data):                    completion(data)                case .failure(let error):                    failure(error.localizedDescription)                &#125;            &#125;    &#125;&#125;\n\n是的，就这几行代码，花了我大概一整天时间来学明白，定稿之前不知道来来回回试了多少遍。前面都很好懂，重点就是responseDecodable这个方法调用，of参数指明我希望把返回的JSON反序列化成一个ForumGroup列表，后面的方法块中根据成功反序列化和发生任何错误的情况，分别调用completion和failure这两个回调方法。\n渲染列表现在回到展示版面的ForumsView，在body里面做如下实现：\nvar body: some View &#123;    NavigationStack &#123;        List &#123;            ForEach($forumGroups) &#123; $forumGroup in                Section &#123;                    ForEach(forumGroup.forums) &#123; forum in                        NavigationLink(destination: CookieListView(globalState: globalState)) &#123;                            if (forum.showName == nil || forum.showName!.isEmpty) &#123;                                Text(forum.name)                            &#125; else &#123;                                Text(forum.showName!)                            &#125;                        &#125;                    &#125;                &#125; header: &#123;                    Text(forumGroup.name)                &#125;            &#125;        &#125;    &#125;    .onAppear &#123;        if (!isContentLoaded) &#123;            globalState.loadingStatus = String(localized: &quot;msgLoadingForumList&quot;);            shouldDisplayProgressView = true;            AnoBbsApiClient.loadForumGroups &#123; forumGroups in                self.forumGroups = forumGroups                isContentLoaded = true                shouldDisplayProgressView = false;            &#125; failure: &#123; error in                showErrorToast(message: error)                shouldDisplayProgressView = false;            &#125;        &#125;    &#125;    .toast(isPresenting: $isErrorToastShowing) &#123;        AlertToast(type: .regular, title: errorMessage)    &#125;&#125;\n\n在这个View展示时，如果版面列表没有被加载，那么就调用刚刚写的loadForumGroups方法获取版面列表，后面的第一个代码块就是completion这个回调的实现，负责把loadForumGroups方法得到的结果传给一个@State变量forumGroups，以及标记内容已经成功载入，并隐藏载入提示的风火轮；第二个代码块是failure这个回调的实现，负责显示一个带有错误信息的Toast并隐藏风火轮。\n在这个View的NavigationStack里面，就可以监听forumGroups这个@State变量，并用变量里面的内容来渲染整个列表了。最后，我们就可以得到这样一个结果：\n\n","categories":["学知识"],"tags":["iOS开发","SwiftUI","List","Alamofire"]},{"title":"监听Tomcat的启动、停止事件","url":"/tomcat-servlet-context-listener.html","content":"当Servlet容器启动或终止Web应用时，会触发ServletContextEvent事件，该事件由ServletContextListener来处理。在Servlet API中有一个ServletContextListener接口，接口中定义了处理ServletContextEvent事件的两个方法，它能够监听ServletContext对象的生命周期，实际上就是监听Web应用的生命周期。我们可以通过实现这两个方法，来实现在Tomcat启动和停止时执行一定的操作。\n\n\n监听器类编写新建一个监听器类TomcatListener并实现ServletContextListener接口\npublic class DemoListener implements ServletContextListener &#123;    @Override    public void contextInitialized(ServletContextEvent servletContextEvent) &#123;        System.out.println(&quot;Tomcat Started&quot;);    &#125;    @Override    public void contextDestroyed(ServletContextEvent servletContextEvent) &#123;        System.out.println(&quot;Tomcat Destroyed&quot;);    &#125;&#125;\n\n配置web.xml在web.xml中添加listener条目\n&lt;listener&gt;    &lt;listener-class&gt;com.project.name.listener.DemoListener&lt;/listener-class&gt;&lt;/listener&gt;\n\n验证将WAR包部署到Tomcat并启动，检查catalina.out\n在Tomcat启动时看到如下日志：\n15-Aug-2018 15:58:44.632 INFO [localhost-startStop-1] org.apache.catalina.startup.HostConfig.deployWAR Deploying web application archive [/usr/local/Cellar/tomcat@8/8.5.28/libexec/webapps/tomcatlistener.war]Tomcat Started\n\n在Tomcat停止时看到如下日志：\n15-Aug-2018 16:02:22.582 INFO [main] org.apache.catalina.core.StandardService.stopInternal Stopping service [Catalina]Tomcat Destroyed\n\n输出内容与TomcatListener所写内容一致，Q.E.D.\n","categories":["学知识"],"tags":["Tomcat"]},{"title":"Docker中的PID 1和tini：为什么你的容器不响应Ctrl-C","url":"/translation-docker-pid-1-and-tini.html","content":"之前我写过一篇文，讲我是怎么处置Docker容器产生的僵尸进程的，正巧前两天上网乱刷，看到有个人也被容器中的僵尸进程困扰，有一条回复提到了一个关键词tini，说能根治这个问题，于是继续上网冲浪，翻到了Medium上的这篇文章，感觉很有用，所以翻译出来。\n以下内容除特别注明外，皆翻译自原文。我亦不对内容做任何的担保，并不对任何可能产生的后果（包括但不限于文件丢失）负责。\n\n\n在使用Docker的时候，你有可能会遇到这么一种很难受的情况，就是你敲了Ctrl-C想停掉这个容器，但这个容器却无动于衷。或者又可能你的容器停止了，但留下了一堆僵尸进程。这些问题通常来自于一个开发者们从一开始就没想明白的问题 —— 如果你的程序成为了容器中的PID 1会怎么样。\n什么是PID 1在Linux系统中，PID 1（进程号1）是在系统启动过程中第一个启动的进程。它扮演着一个特殊的角色，即系统的初始化者。它将负责启动和管理所有其他的进程。在Docker容器中，你启动的第一个进程将默认成为PID 1。\n例如：\ndocker run -it node:18 node\n\n这时候，Node.js的进程就是PID 1。\n为什么PID 1很特别？PID 1的进程在Linux中会有如下几种特殊的行为：\n\n响应信号的方式不同大多数UNIX进程会自动接收并处理类似SIGINT（来自Ctrl-C）和SIGTERM（来自docker stop命令）的信号。但是PID 1的进程默认不会接收这些信号，除非它们主动监听。\n收割僵尸进程如果PID 1不等待子进程，那么这些子进程就会变成僵尸。尽管它们已经退出了，但仍然会消耗系统资源。（译者注：我理解就是在容器停止的时候，主进程不等待它的子进程全部退出成功再退出，而是就自己拍拍屁股走人了）这会慢慢地拖慢这个容器的性能，或让这个容器的行为变得失控。\n\n问题示例假设我们有这样一个简单的Node.js应用：\n// app.jssetInterval(() =&gt; console.log(&#x27;Running...&#x27;), 1000);\n\n这个应用会运行在一个Docker容器中：\ndocker run -it node:18 node app.js\n\n如果此时你尝试使用Ctrl-C退出，那么什么都不会发生。很奇怪对吧？因为：\n\nNode.js在容器中是PID 1进程\n当它是PID 1的时候，它没有正确转发或监听SIGINT信号\n\n最终，你可能会一边纳闷为啥Ctrl-C不好使，一边反复敲它。这时候就是tini出场的时候了。\ntini：轻量的inittini是个简化的初始化（init）系统，体积只有几KB，并且专为容器环境设计。Docker甚至集成了它，你只需要用--init参数就能开启。\ntini负责干什么？\n信号转发它会监听类似SIGINT和SIGTERM之类的信号，并正确地将其转发到你的应用程序。这样一来，Ctrl-C或docker stop就会按照预期工作了。\n收割僵尸进程tini会收割死亡的子进程，这样它们就不会变成僵尸了。\n就像一个负责任的PID 1一样干活它工作起来就像一个真正的Linux初始化系统，只是更小了点。\n\n怎么用tini？方法1:使用Docker内置的功能Docker在引擎内部已经集成了tini，所以你在运行容器时加上--init参数，那么Docker就会自动使用tini作为容器内的PID 1进程。你不需要额外安装或配置什么东西，只需要在命令中加上--init，就像这样：\ndocker run --init -it node:18 node app.js\n\n方法2:在Dockerfile中手动添加tini你也可以显式地为Dockerfile添加tini：\nFROM node:18# 安装tiniRUN apt-get update &amp;&amp; apt-get install -y tini# 使tini成为入口点ENTRYPOINT [&quot;/usr/bin/tini&quot;, &quot;--&quot;]# 运行你的应用CMD [&quot;node&quot;, &quot;app.js&quot;]\n\n然后你可以像往常一样构建和运行它：\ndocker build -t node-app .docker run -it node-app\n\n为什么在生产环境会很重要？在生产环境（比如Kubernetes）中，无法处理信号可能会引发这些问题：\n\n应用程序无法干净的退出\n数据因为处理停机的逻辑没有触发而导致损坏\n僵尸进程造成资源泄漏\n\n使用tini就可以用最小的成本避免这些问题。\n最后的一点想法尽管容器内部的行为很容易被忽略，但理解PID 1如何工作，以及tini解决了什么问题，能让你的容器变得更加干净、安全，并更容易维护。所以下次你遇到哪个应用不响应Ctrl-C，记得叫来小小的tini。\n\n译者注：如果你使用Docker Compose编排容器的话，那么在配置文件中指定init: true就可以引入tini了，就像这样：\nservices:    app:        image: alpine:latest        init: true\n","categories":["翻译"],"tags":["Docker","tini"]},{"title":"用裸git仓库管理点文件的最佳方式","url":"/translation-dotfiles-best-way-to-store-in-a-bare-git-repository.html","content":"前些天在Atlassian的文档中看到一篇关于管理点文件（即.bashrc之类以点开头的文件）的文章《Dotfiles: Best way to store in a bare git repository》，感觉很有参考价值，遂决定翻译出来，希望能帮到更多的人。\n以下内容除特别注明外，皆翻译自原文。我亦不对内容做任何的担保，并不对任何可能产生的后果（包括但不限于文件丢失）负责。\n\n\n\n免责声明：标题有些夸大其词了，而且针对这个问题也有其他好用的解决方案。但我确实觉得这也是一个优雅的技巧。\n最近我在Hacker News中看到一篇帖子，是讨论人们怎么管理他们的点文件的。在这篇帖子中，用户StreakyCobra分享了一个他的很优雅的配置，而且我觉得非常的靠谱。正巧我也在开始用同样的技巧来管理我的系统。这个技巧也只有一个前置条件：你安装了Git。\n按他的说法，这个技巧：\n\n不需要别的工具，不需要创建符号链接，文件都被版本控制系统跟踪，你可以用不同的分支来管理不同的系统，在全新的系统中你也可以轻松复用你的配置。\n\n这个技巧包括两个部分：用一个单独的文件夹（如$HOME/.cfg或$HOME/.myconfig）作为裸Git仓库；和一个专门用来操作这个仓库的命令别名（译者注：alias）。\n从零开始如果你还没有用一个Git仓库来跟踪你的配置文件，那么你可以从下面这几个简单的命令开始，一步步实现这个技巧。\ngit init --bare $HOME/.cfgalias config=&#x27;/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME&#x27;config config --local status.showUntrackedFiles noecho &quot;alias config=&#x27;/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME&#x27;&quot; &gt;&gt; $HOME/.bashrc\n\n\n第一条命令会创建一个用来跟踪文件的裸Git仓库~/.cfg（配置仓库）\n接下来创建一个命令别名config，在我们要操作配置仓库时就会用这个命令，而不是git命令\n设定一个仅对配置仓库生效的设定 —— 不显示未追踪的文件，这样在我们执行config status等命令的时候，我们不想跟踪的文件就不会以untracked状态被显示出来\n此外我们可以把这个别名的声明添加到.bashrc里面，方便以后使用\n\n我把上面这些命令集合成了一个代码片段放到了Bitbucket上，并给它做了一个短链接，所以你可以用下面的命令一键执行：\ncurl -Lks http://bit.do/cfg-init | /bin/bash\n\n（译者注：在执行来自网络的代码之前，一定记得先看看内容。闭著眼执行远程命令是一件很危险的事，因为你不知道这个链接的内容是否还正确，比如曾经课本上本来指向《历朝历代咏武侯诗词大全》的网址，在域名过期之后被黄网搞去了……）\n在配置完成后，$HOME目录下的文件就可以用刚刚创建的config别名来进行版本控制了，比如：\nconfig statusconfig add .vimrcconfig commit -m &quot;Add .vimrc&quot;config add .bashrcconfig commit -m &quot;Add .bashrc&quot;config push\n\n把这些点文件安装到新系统（或迁移到本系统）如果你已经用Git仓库管理你的配置或者点文件，那你可以跟着下面的步骤，来把这个配置迁移到新的系统：\n\n首先确认你已经配置好了命令别名\n\nalias config=&#x27;/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME&#x27;\n\n\n在克隆你的配置仓库前，记得让git忽略它，以避免各种奇怪的问题\n\necho &quot;.cfg&quot; &gt;&gt; .gitignore\n\n\n然后你就可以把你的点文件克隆到一个存在于“点目录”的裸仓库中\n\ngit clone --bare &lt;git-repo-url&gt; $HOME/.cfg\n\n\n在当前shell会话中定义好config这个别名\n\nalias config=&#x27;/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME&#x27;\n\n\n从裸仓库中把配置checkout到$HOME\n\nconfig checkout\n这一步可能会报这样的错误\nerror: The following untracked working tree files would be overwritten by checkout:    .bashrc    .gitignorePlease move or remove them before you can switch branches.Aborting\n这是因为你的$HOME目录可能已经有这些文件，而checkout操作可能会让它们被覆盖。解决方法也很简单：这些文件要是有用，那就备份出来，没用那就删了。你可以用我这个比较粗糙的命令，来一键把有冲突的文件移动到备份目录：\nmkdir -p .config-backup &amp;&amp; \\config checkout 2&gt;&amp;1 | egrep &quot;\\s+\\.&quot; | awk &#123;&#x27;print $1&#x27;&#125; | \\xargs -I&#123;&#125; mv &#123;&#125; .config-backup/&#123;&#125;\n然后重新执行config checkout\n\n为配置仓库设定不显示未跟踪的文件\n\nconfig config --local status.showUntrackedFiles no\n\n\n到此就完成了，接下来你就可以和之前一样用config命令来管理你的点文件了。\n\n同样，我也提供了一个一键脚本，把它作为代码片段放在了Bitbucket上。你可以用这个命令来执行：\ncurl -Lks http://bit.do/cfg-install | /bin/bash\n\n为了文章的完整起见，这是我最终得到的脚本（已经在很多个全新启动的Alpine Linux容器中做过测试）\ngit clone --bare https://bitbucket.org/durdn/cfg.git $HOME/.cfgfunction config &#123;   /usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME $@&#125;mkdir -p .config-backupconfig checkoutif [ $? = 0 ]; then  echo &quot;Checked out config.&quot;;else  echo &quot;Backing up pre-existing dot files.&quot;;  config checkout 2&gt;&amp;1 | egrep &quot;\\s+\\.&quot; | awk &#123;&#x27;print $1&#x27;&#125; | xargs -I&#123;&#125; mv &#123;&#125; .config-backup/&#123;&#125;fi;config checkoutconfig config status.showUntrackedFiles no\n\n（译者注：原文的if-else部分似乎缩进有问题，我调整了下格式，未对脚本内容做改动）\n结语我希望你们能觉得这个管理点文件的小技巧能产生帮助。如果你有兴趣的话，可以到这里看我的点文件。另外别忘了关注@durdn或我那炫酷的小组@atlassiandev。\n","categories":["翻译"],"tags":["git","dotfiles","点文件"]},{"title":"在TypeScript的catch代码块中获取错误信息","url":"/translation-get-a-catch-block-error-message-with-typescript.html","content":"最近在试着用TypeScript写点东西，在用catch (error) {}代码块处理异常的时候，看到了一个很难理解的错误TS18046: error is of type unknown。网上一顿冲浪之后，看到了Kent C. Dodds的一篇博客Get a catch block error message with TypeScript。我跟着文章的内容成功解决了这个问题，并且解答了我的疑惑，所以想要翻译出来帮助到更多的人。\n以下内容除特别注明外，皆翻译自原文。我亦不对内容做任何的担保，并不对任何可能产生的后果（包括但不限于文件丢失或功能异常）负责。\n\n\n\n好吧，咱们看看这个代码：\nconst reportError = (&#123; message &#125;) =&gt; &#123;    // 把错误信息发给我们的日志服务...&#125;try &#123;    throw new Error(&#x27;Oh no!&#x27;)&#125; catch (error) &#123;    // 我们会让代码继续执行，但先把错误报告出去    reportError(&#123; message: error.message &#125;)&#125;\n\n这么写应该足够好了吧？嘛，毕竟这是JavaScript。要是换成TypeScript的话：\nconst reportError = (&#123; message &#125;: &#123; message: string &#125;) =&gt; &#123;    // 把错误信息发给我们的日志服务...&#125;try &#123;    throw new Error(&#x27;Oh no!&#x27;)&#125; catch (error) &#123;    // 我们会让代码继续执行，但先把错误报告出去    reportError(&#123; message: error.message &#125;)&#125;\n\n这时候reportError的error.message这部分就要报错了。因为（就在最近）TypeScript把error的类型定义成了unknown。这倒也是事实，因为它确实没法保证抛出的错误的类型。哦对，这也是你不能用promise的泛型(Promise&lt;ResolvedValue, NopeYouCantProvideARejectedValueType&gt;)，来给promise reject的.catch(error =&gt; {})指定类型的原因。而且，被抛出来的东西可能都不是一个error，它可以是任何东西：\nthrow &#x27;啥玩意？！&#x27; // 译者注：原文为 &#x27;What the!?&#x27;，请尝试用东北口音理解throw 7throw &#123; wut: &#x27;is this&#x27; &#125; // 译者注：wut可以用来表达“傻眼”语境下的what，类似“什么鬼”throw nullthrow new Promise(() =&gt; &#123;&#125;)throw undefined\n\n说真的，你可以想throw啥就throw啥，啥东西都行。那，要解决上面提到的错误信息好像挺简单对吧，我们在catch中声明代码只会抛出error不就行了？\ntry &#123;    throw new Error(&#x27;Oh no!&#x27;)&#125; catch (error: Error) &#123;    // 我们会让代码继续执行，但先把错误报告出去    reportError(&#123; message: error.message &#125;)&#125;\n\n想的美！现在你会得到这么一条TypeScript的编译错误：\nCatch clause variable type annotation must be &#x27;any&#x27; or &#x27;unknown&#x27; if specified. ts(1196)\n\n报这个错的原因是，尽管看起来在我们的代码里不可能会抛出来其他的东西，但JavaScript就这么逗，一个第三方库完全有可能做点什么奇怪的事，比如给Error的构造函数来个猴子补丁（译者注：monkey-patching），让它抛出点不一样的东西：\nError = function () &#123;    throw &#x27;Flowers&#x27;&#125; as any\n\n那咱们开发者该怎么办？我们只能尽力，比如这样：\ntry &#123;    throw new Error(&#x27;Oh no!&#x27;)&#125; catch (error) &#123;    let message = &#x27;Unknown Error&#x27;    if (error instanceof Error) message = error.message    // 我们会让代码继续执行，但先把错误报告出去    reportError(&#123; message &#125;)&#125;\n\n妥了这不！现在TypeScript也不跟我们嚷嚷有问题了，而且万一这个error是什么奇怪的东西，我们也用了合适的办法来处理它了。而且这段代码我们还能继续优化成这样：\ntry &#123;    throw new Error(&#x27;Oh no!&#x27;)&#125; catch (error) &#123;    let message    if (error instanceof Error) message = error.message    else message = String(error)    // 我们会让代码继续执行，但先把错误报告出去    reportError(&#123; message &#125;)&#125;\n\n那么现在，如果这个error不是一个Error对象，那么我们就直接把它变成一个字符串并祈祷这个错误信息能有点用。\n然后，我们还能把这段代码抽出来做成一个工具方法来给所有的catch块用：\nfunction getErrorMessage(error: unknown) &#123;    if (error instanceof Error) return error.message    return String(error)&#125;const reportError = (&#123; message &#125;: &#123; message: string &#125;) =&gt; &#123;    // 把错误信息发给我们的日志服务...&#125;try &#123;    throw new Error(&#x27;Oh no!&#x27;)&#125; catch (error) &#123;    // 我们会让代码继续执行，但先把错误报告出去    reportError(&#123; message: getErrorMessage(error) &#125;)&#125;\n\n这个写法在我的项目里面特别好用。希望也能帮助到你！\n更新：Nicolas针对error对象并不真的是error的情况提了一个很好的建议。此外Jesse也提出了一个建议，在可能的情况下把error对象也转换成字符串。把这些结合起来，我们就得到了这样的一份代码：\ntype ErrorWithMessage = &#123;    message: string&#125;function isErrorWithMessage(error: unknown): error is ErrorWithMessage &#123;    return (        typeof error === &#x27;object&#x27; &amp;&amp;        error !== null &amp;&amp;        &#x27;message&#x27; in error &amp;&amp;        typeof (error as Record&lt;string, unknown&gt;).message === &#x27;string&#x27;    )&#125;function toErrorWithMessage(maybeError: unknown): ErrorWithMessage &#123;    if (isErrorWithMessage(maybeError)) return maybeError    try &#123;        return new Error(JSON.stringify(maybeError))    &#125; catch &#123;        // fallback in case there&#x27;s an error stringifying the maybeError        // like with circular references for example.        return new Error(String(maybeError))    &#125;&#125;function getErrorMessage(error: unknown) &#123;    return toErrorWithMessage(error).message&#125;\n\n简直太好使了！\n结论\n我觉得关键在于，尽管TypeScript有一些奇怪的地方，但也绝对不要因为你觉得这不可能就忽略TypeScript抛出的编译错误或警告。大多数情况下，意外是非常有可能发生的，而TypeScript很好的强制你去处理这些“不太可能发生”的情况……然后你也很有可能会发现，这些情况并没有你想的那么少见。\n\n译者的碎碎念：作为一个TypeScript纯新手的我，最后这段代码给我看傻了。Java里面非常常见的try {...} catch (Exception e) {...}在TypeScript里面竟然能玩这么花……\n","categories":["翻译"],"tags":["TypeScript"]},{"title":"让nohup不输出 appending output to 'nohup.out'","url":"/suppress-appending-to-nohup-out-message.html","content":"在使用nohup的时候，它总会打印一条nohup: appending output to &#39;nohup.out&#39;这样的信息，并且必须敲一下回车。\n因为nohup: appending output to &#39;nohup.out&#39;这条信息是打印到STDERR的，所以解决的方法很简单，把STDERR重定向至STDOUT就可以了，比如这样：\nnohup doSomething &gt; nohup.out 2&gt;&amp;1 &amp;\n","categories":["小技巧"],"tags":["Linux","nohup"]},{"title":"深入了解Redis的Pub/Sub","url":"/translation-redis-pubsub-in-depth.html","content":"看到一篇深入讲解Redis中Pub&#x2F;Sub的文章Redis Pub&#x2F;Sub In-Depth，所以打算将它翻译出来，顺便深化自己的理解。\n\n\nPub&#x2F;Sub（即publish&#x2F;subscribe的简称）是一个在分布式系统中给不同组件互相通信的一种消息传递技术。这种消息传递技术与传统的点对点通信（即一个服务直接向另一个服务发送消息）不同，它是一种异步且可伸缩的消息服务，并且它可以将负责发布消息的服务与负责处理消息的服务分隔开。\n在这篇博客中，我们会探索Pub&#x2F;Sub的原理，以及Redis是如何实现这个通信模型的。我们会分析Redis中错综复杂的实现，将目光聚焦并深入到内存级别的实现的细节上，来让我们完全理解Pub&#x2F;Sub机制，以及通过Redis完成的项目实践。\nPub&#x2F;Sub入门Pub&#x2F;Sub是一个消息模型，它可以让分布式系统中的不同组件互相通信。发布者向一个主题（topic）发送信息，订阅者则从这个主题中接收信息。发送者在这个过程中可以保持匿名，当然如果在消息中包含了发送者的信息，那么订阅者也可以通过它来确定发送者的身份。Pub&#x2F;Sub系统保证了消息可以被送达到所有对这个主题感兴趣的订阅者。在恰当的配置下，这将会是一个高度可扩展并且很可靠的消息传递系统，它将可以处理大量的数据。此外，在适当的大小、网络状况，以及订阅者的处理时间的前提下，Pub&#x2F;Sub允许服务以1毫秒的延迟进行异步通信，因此它非常适合现代快速的分布式应用程序。\nPub&#x2F;Sub模型Pub&#x2F;Sub的模型很简单，消息中介从发布者收到消息，然后将消息分发到各个订阅者。订阅者在得到消息后，就可以根据实际的场景对消息进行处理。基于发布者和订阅者的数量，这个模型通常可以被归为四类，即一对一、一对多、多对一、多对多。\n\n\n\nPub&#x2F;Sub类型\n描述\n\n\n\n一对一\n包括一个发布者和一个订阅者。消息从发布者直接发送到订阅者。\n\n\n一对多\n包括一个发布者和多个订阅者。发布者向主题发送消息，所有订阅这个主题的订阅者都将收到这个消息。\n\n\n多对一\n包括多个发布者和一个订阅者。多个发布者都向某个特定的主题发送消息，订阅者会从这个主题收到消息。\n\n\n多对多\n包括多个发布者和多个订阅者。各个发布者都向某个主题发送消息，而各个订阅者都将收到这些消息。\n\n\nPub&#x2F;Sub核心概念在我们深入去了解Pub&#x2F;Sub的细节和实现之前，我们需要先对Pub&#x2F;Sub相关的核心概念有所了解。Pub&#x2F;Sub系统包含多个组件，下表描述了其中的一些主要的组件：\n\n\n\n组件\n描述\n\n\n\n发布者\n发布者是一个应用或者服务，它会发出消息。\n\n\n订阅者\n订阅者是一个应用或者服务，它会接收消息。\n\n\n主题(topic)\n主题即消息的标题或信息源。发布者可以向一个主题发送消息，这些消息会被广播至订阅者。\n\n\n消息\n消息包含将在系统中被接收或传递的消息。\n\n\n中介(broker)\n中介负责指引消息在系统中的流动。它扮演着一个中间人的角色，负责在发布者和订阅者之间建立通信，并在它们之间交换信息。中介可以维护一个关于主题及其订阅者的列表，来帮助中介将从发布者收到的信息发送到正确的订阅者。\n\n\n路由(routing)\n路由指在系统中，信息从发布者流向订阅者，并依靠特定的订阅保障信息被送往正确的订阅者的过程。\n\n\nRedis Pub&#x2F;Sub现在我们已经了解了关于Pub&#x2F;Sub的一些抽象概念，并且知道了它是如何工作的。接下来我们需要深入到Redis针对Pub&#x2F;Sub的实现，来了解消息从发布者发布出来，到结束于订阅者这个过程中，系统是如何通信的。\nRedis通过在客户端之间实现一个简单而高效的消息系统来实现Pub&#x2F;Sub。在Redis中，一个客户端可以“发布”一条消息到一个命名的通道，其他客户端则可以“订阅”这个通道来接收消息。\n当一个客户端向这个通道发布了一条消息，Redis将会把这个消息发送到所有订阅了这个通道的客户端。这样，应用程序的不同组件之间就可以实时地通信并交换信息。\nRedis Pub&#x2F;Sub提供了一个轻量级、高速，并且可扩展的消息传递解决方案，并且可以用于各种场景下，比如实现一个实时通知、在不同微服务之间发送消息，或在一个应用的不同组件之间通信。\n同步通信Redis Pub&#x2F;Sub是同步的。为了保证消息可以成功被传递，订阅者和发布者必须同时连接到Redis。\n你可以将它想象成一个收音机电台，在调频到这个频道之后，你就可以收听它的内容。然而，在收音机被关掉之后，你就没法再收听了。\n\n这意味着如果一个订阅者断开了连接，过了一会又连了上来，那么它将无法收到断开期间发出来的消息。也就是说，这限制了Redis Pub&#x2F;Sub只能用在允许数据可能丢失的场景下。\n发射后不管 (Fire &amp; Forget)“发射后不管”是一种信息传递模式，在这种模式下，发送方不会期望从接收方明确确认信息已经被收到。发送方只会将消息发出去，然后就继续开始做后面的事，不管消息有没有被接收方收到。\n\n仅向外播散 (Fan-out Only)Redis Pub&#x2F;Sub只管向外播散消息，也就是说，当发布者发布了一条消息后，这条消息会被广播到所有当前活动的订阅者。所有订阅者都会收到这条消息的副本，不管它们是不是对这条消息感兴趣。\n掀开Redis Pub&#x2F;Sub的引擎盖Redis最著名的功能是键值服务器。当客户端连接到Redis服务器后，它将会与服务器建立TCP连接，并开始向服务器发送命令。\n\n但是Redis也是个消息服务器！一个对topicA感兴趣的客户端可以向Redis服务器建立一个TCP连接，发送SUBSCRIBE topicA命令，然后等待与topicA相关的新闻。“新闻机构”也可以连接到Redis服务器，发送PUBLISH topicA message-data，然后所有订阅了这个主题的客户端都将收到这条价值连城的消息。\n\n如果放大了看Redis里面发生了什么，我们可以想象Redis会跟踪每个套接字(socket)的订阅集：\n\n咱们继续深入看看Redis是怎么做的。\n最初的Pub&#x2F;Sub实现允许客户端发送三个新的命令：PUBLISH，SUBSCRIBE，和UNSUBSCRIBE。Redis使用了一个全局变量pubsub_channels来跟踪各个订阅，在其中Redis维护了一个通道名和订阅它的客户端对象的映射关系。每个客户端对象代表一个TCP连接的客户端，并通过其对应的文件描述符来跟踪。\n\n当一个客户端发送SUBSCRIBE命令后，它对应的客户端对象就会被加到对应通道的客户端对象集中。在发布信息时，Redis会从pubsub_channel中找到这个主题对应的订阅者，并针对每个客户端启动一个计划任务来向对应的套接字发送信息。\n处理连接断开客户端的连接可能会因为客户端断开连接，或者网线被拔掉而断开。当有连接断开时，Redis必须将对应的订阅清理掉。假设客户端A断开了连接，那么为了将这个客户端从pubsub_channels中清除，Redis需要遍历每个通道(“topicA”和”topicB”)，并从它们的客户端对象集中删掉它。\n可想而知，遍历所有的通道的效率是非常低的。按道理，Redis只需要访问”topicA”这个通道，因为客户端A只订阅了它。为了将这个理论变成实际，Redis标记了各个客户端及其订阅的通道，并将其与pubsub_channels同步。这样，Redis就只需要访问这个客户端相关的通道，而不需要低效的遍历所有的通道。我们在图中可以将这些标记画成绿色的圆：\n\n落实理论我们已经知道，全局变量pubsub_channels的数据结构基本上就是一个Map&lt;ChannelName, Set&lt;Client&gt;&gt;，而每个客户端的订阅集就是个Set&lt;ChannelName&gt;。但是这些都只是一个抽象的数据结构，没法说明内存中是如何表现的。所以我们继续放大，看看内存中的样子。\npubsub_channels这个Map实际上是个哈希表，其中通道名会被哈希，然后放到一个2^n长的数组中的对应位置。\n\n包含了8个桶的数组pubsub_channels是一块独立分配出来的内存空间。要发布消息到一个通道，首先它会将通道名进行哈希，找到它对应的桶，然后遍历这个通道相关联的各个客户端。然而不同的通道名又可能会被哈希到同一个桶中，为了解决这个哈希冲突问题，Redis使用了一个叫“哈希链”的方案，即每个桶都会指向一个保存着通道的链表。比如在上面的例子中，“topicA”和“topicB”都被哈希到了第三个桶中。实际上为了以防万一，Redis会在启动时为它的哈希函数选择一个随机的种子来尝试避免出现哈希碰撞，这也可以防止被恶意用户蓄意订阅到会被哈希到同一个位置的大量通道而导致性能下降。\n在下图中，绿色的字符串是通道的名字，作为哈希表的键；紫色的是客户端的集合，作为哈希表的值。但“集合”在这里也是个抽象的概念，它在Redis中是怎样表现的呢？答案是，另一个链表！\n\n我们可能会觉得字符串“topicA”和“topicB”是在哈希链里面的，但实际不是。每个字符串都是独立分配的一块内存空间。字符串在Redis中被广泛使用着，它们甚至有自己的名字叫“简单动态字符串（Simple Dynamic Strings）”，其内容分三部分：已占用的空间、剩余的空间，和一个字符数组。\n\n到现在我们已经快要到内存级别了，但是还有一件事没有提到，那就是每个通道的客户端集。在这里，Redis并没有选择用链表，而是用了另一个哈希表，其中通道的名字就是哈希表的键。\n\n我暂时还不清楚为什么Redis选择用链表保存通道的客户端集，而又选择哈希表来保存客户端的通道集。我怀疑Redis选择在通道的客户端集上用链表是因为，链表更适用于发布的场景，因为在发布时会需要遍历它；而选择在客户端的通道集上用哈希表则是因为它更适用于订阅和取消订阅的场景，因为此时要做的是在里面做查找。如果你有更深入的理解，请告诉我。\n需要注意的是，在每个客户端的哈希链上，指向它的值的指针是被忽略的，这片内存没有被用到。当用哈希表来表示一个集合的时候，我们只会用到它的键。相比较于我们从代码复用中得到的收益，这部分浪费不值一提。\n终于，我们快要接近真相了：图中的每个方块都代表redis-server进程中分配的一块内存。现在我们来回顾一下PUBLISH和SUBSCRIBE的算法：\n\n在PUBLISH的时候，首先对通道的名字做一次哈希，接下来遍历哈希链，将要发布的通道名与哈希链中的各个通道名做比较。在找到我们要的那个通道后，从中得到它的客户端列表。然后遍历客户端列表，将消息发至各个客户端。\n在SUBSCRIBE的时候，首先像PUBLISH一样找到保存着客户端的那个链表，然后将新的客户端附加在链表的尾部。并且将要订阅的通道添加到客户端维护的哈希表中。\n\n可以实时操作的哈希表要注意这些哈希表的大小是不同的，而且都大概与其元素数量成比例。Redis会根据元素数量调整哈希表的大小。但是，Redis的设计目标是低延迟，而调整哈希表又是个比较耗时的操作。那么Redis是怎么做到在调整哈希表大小时不产生延迟尖峰的？\n答案是：Redis会逐步的调整哈希表的大小。它在底层维护了新旧两个哈希表。假设pubsub_channels这个哈希表正在经历调整。\n每当Redis对哈希表进行操作时（查询、插入、删除……），它都会稍微调整一下哈希表的大小。它会跟踪有多少老的桶被移动到了新的桶中，然后在每次操作中，它都会再移动几个过去。这样一来，调整大小的工作量就会被限制，使得Redis在调整期间也可以保证响应。\n\n不再订阅昂贵的东西（Expensive unsubscribed）在Pub&#x2F;Sub中还有一个很重要的操作：UNSUBSCRIBE，它所做的与SUBSCRIBE正相反，它会使客户端不再订阅到指定的通道，也不会再收到来自这个通道的消息。那么利用上面说过的数据结构，你会怎么实现UNSUBSCRIBE功能呢？Redis是这样做的：\n在UNSUBSCRIBE时，首先找到这个通道对应的客户端链表，然后遍历整个链表直到找到我们要删除的那个客户端。\n也就是说，UNSUBSCRIBE操作的耗时是O(n)，其中n就是客户端的数量。如果一个通道有大量的客户端订阅，那么UNSUBSCRIBE操作就会变得很昂贵。所以，你要么需要限制你的客户端数量，要么需要限制它们订阅的通道的数量。Pusher的一个优化就是去掉重复的订阅，数百万的Pusher订阅会被折叠为数量更少的Redis订阅。\nRedis可以通过把存放客户端订阅的链表换成哈希表来优化，但也可能不会很理想，会导致发布消息稍稍变慢，因为遍历哈希表比遍历链表慢。Redis选择针对PUBLISH操作去做优化，因为它们比变更订阅更加常见。\n按照表达式匹配订阅（Pattern subscriptions）最初的Pub&#x2F;Sub提供了PUBLISH、SUBSCRIBE和UNSUBSCRIBE三个命令。不久之后，Redis引入了一个名为“按照表达式匹配订阅”的新功能及其命令PSUBSCRIBE，这个可以让客户端订阅所有匹配了某个正则表达式的主题。\n现在，如果一个客户端发送了命令PSUBSCRIBE food.donuts.*，然后一个发布者发送了命令PUBLISH food.donuts.glazed 2-for-£2，那么订阅者也会收到消息，因为food.donuts.glazed匹配了正则表达式food.donuts.*。\n按照表达式订阅的实现是与普通的订阅完全不同的。除了全局的pubsub_channels哈希表之外，还有一个全局的pubsub_patterns列表。这是一个包含了pubsubPattern对象的列表，每个对象与一个客户端订阅的一个表达式相关联。类似地，每个客户端对象也有一个包含着它订阅的表达式的链表。下图展示了当客户端B订阅了drink?，同时客户端A和客户端B订阅了food.*之后，redis-server进程的内存的样子：\n\n图示的左侧是一个全局的链表，每个节点都指向一个pubsubPattern，每个表达式在内存中都是字符串字面量。在图示的右侧，每个客户端都有一个关联着它的订阅的链表。\n当一个客户端发送命令PUBLISH food.donuts 5-for-£1，Redis就会遍历pubsub_patterns列表，并将字符串food.donuts与每个表达式匹配。每遇到一个符合的匹配，Redis就会发送消息5-for-£1到关联的客户端。\n这个实现可能有一个让你惊讶的点：如果多个客户端订阅到同一个表达式时，这些订阅不会被组合在一起。如果有10000个客户端都订阅了food.*，那么你将会得到一个包含了10000个表达式的链表，其中每个元素在发布消息的时候都会被匹配。这种设计假定模式订阅集很小，而且各不相同。\n另一个会让你惊讶的点是，表达式都是以其原文存储的，它们并没有被编译。这一点尤其有趣，因为Redis的匹配函数stringmatch有一些有趣的坏情况。如下展示了Redis是如何把字符串aa与表达式*a*a*b相匹配的：\nstringmatch(&quot;*a*a*b&quot;, &quot;aa&quot;)    stringmatch(&quot;a*a*b&quot;, &quot;aa&quot;)        stringmatch(&quot;*a*b&quot;, &quot;a&quot;)            stringmatch(&quot;a*b&quot;, &quot;a&quot;)                stringmatch(&quot;*b&quot;, &quot;&quot;)                    stringmatch(&quot;b&quot;, &quot;&quot;)                        false            stringmatch(&quot;a*b&quot;, &quot;&quot;)                false    stringmatch(&quot;a*a*b&quot;, &quot;a&quot;)        stringmatch(&quot;*a*b&quot;, &quot;&quot;)            stringmatch(&quot;a*b&quot;, &quot;&quot;)                false    stringmatch(&quot;a*a*b&quot;, &quot;&quot;)        false\n\n这种带有许多glob的恶意表达式会导致匹配的执行时间爆炸性增长。Redis的表达式语言虽然可以被编译成确定有限状态自动机（DFA），来让匹配耗时变得线性，但却没这么做。\n所以长话短说，你不应该把Redis的按表达式匹配功能开放给不被信任的客户端，因为这可能会引来两种攻击：一种是订阅大量的表达式，另一种就是刻意制造的表达式。在Pusher，我们会非常小心地处理Redis的按表达式订阅功能。\nPub&#x2F;Sub的使用场景我们现在知道了Pub&#x2F;Sub的技术细节，以及在Redis中是怎么实现的。接下来我们看看有哪些使用场景。\nRedis提供的异步集成功能提高了系统的整体灵活性和稳定性，使其能够满足各种场景，包括：\n\n实时消息和聊天\n物联网设备\n新闻发布和警告\n分布式计算和微服务\n事件驱动的架构\n组件间解耦，以及减少依赖\n扇入处理（Fan-in processing）—— 将多个消息组合成一个消息的过程被称为扇入处理。\n刷新分布式缓存\n\n结论Redis是实现Pub&#x2F;Sub最常用的工具之一。它因扩展性、低延迟和易于集成而广为人知。我们也在内存块级别探讨了Redis的工作原理。\nRedis Pub&#x2F;Sub是一个高效的分发消息的方法，但我们也要知道它在哪些方面做了优化，以及在哪有陷阱。要想彻底了解的话，那就去研究源码吧！一句话，记得只在可信的环境中使用Redis，并要限制客户端的数量，并且小心处理按照表达式的订阅。\n","categories":["翻译"],"tags":["Redis","Pub/Sub"]},{"title":"使用Docker配置Redis主从复制","url":"/redis-replication-in-docker.html","content":"这篇文章主要介绍如何使用Docker在本机搭建一个带有主从复制功能的Redis环境，内容包括涉及的目录结构、docker-compose.yml的编写，以及结果的验证。\n\n\n目录结构本文将采用如下的目录结构，其中data目录将用于存放各个容器的数据，server目录存放docker-compose.yml以及针对master和slave节点的配置文件。\n.├── data└── server    ├── docker-compose.yml    ├── redis-master.conf    ├── redis-slave1.conf    └── redis-slave2.conf\n\n配置Redis节点配置master节点编辑redis-master.conf，修改下列配置：\nbind 127.0.0.1# 启用保护模式# 即在没有使用bind指令绑定具体地址时# 或在没有设定密码时# Redis将拒绝来自外部的连接protected-mode yes# 监听端口port 6379# 启动时不打印logo# 这个不重要，想看logo就打开它always-show-logo no# 设定密码认证requirepass redis# 禁用KEYS命令# 一方面 KEYS * 命令可以列出所有的键，会影响数据安全# 另一方面 KEYS 命令会阻塞数据库，在数据库中存储了大量数据时，该命令会消耗很长时间# 期间对Redis的访问也会被阻塞，而当锁释放的一瞬间，大量请求涌入Redis，会造成Redis直接崩溃rename-command KEYS &quot;&quot;# 此外还应禁止 FLUSHALL 和 FLUSHDB 命令# 这两个命令会清空数据，并且不会失败\n\n配置slave节点创建redis-slave1.conf，修改下列配置：\nbind 127.0.0.1# 启用保护模式# 即在没有使用bind指令绑定具体地址时# 或在没有设定密码时# Redis将拒绝来自外部的连接protected-mode yes# 监听端口port 6380# 启动时不打印logo# 这个不重要，想看logo就打开它always-show-logo no# 设定密码认证requirepass redis# 禁用KEYS命令# 一方面 KEYS * 命令可以列出所有的键，会影响数据安全# 另一方面 KEYS 命令会阻塞数据库，在数据库中存储了大量数据时，该命令会消耗很长时间# 期间对Redis的访问也会被阻塞，而当锁释放的一瞬间，大量请求涌入Redis，会造成Redis直接崩溃rename-command KEYS &quot;&quot;# 此外还应禁止 FLUSHALL 和 FLUSHDB 命令# 这两个命令会清空数据，并且不会失败# 配置master节点信息# 格式：#slaveof &lt;masterip&gt; &lt;masterport&gt;# 此处masterip所指定的redis-server-master是运行master节点的容器名# Docker容器间可以使用容器名代替实际的IP地址来通信slaveof 127.0.0.1 6379# 设定连接主节点所使用的密码masterauth &quot;redis&quot;\n\n创建redis-slave2.conf，修改监听端口号为6381，其余配置与redis-slave1.conf相同。\n配置及启动容器编写docker-compose.yml本例中使用docker-compose编排相关容器。要说为什么不用Kubernetes，那是因为对于一个示例来说这玩意太重了。说的一套一套的还不是因为不会用\n---version: &#x27;3&#x27;services:  # 主节点的容器  redis-server-master:    image: redis    container_name: redis-server-master    restart: always    # 为了规避Docker中端口映射可能带来的问题    # 这里选择使用host网络    network_mode: host    # 指定时区，保证容器内时间正确    environment:      TZ: &quot;Asia/Shanghai&quot;    volumes:      # 映射配置文件和数据目录      - ./redis-master.conf:/usr/local/etc/redis/redis.conf      - ../data/redis-master:/data    sysctls:      # 必要的内核参数      net.core.somaxconn: &#x27;511&#x27;    command: [&quot;redis-server&quot;, &quot;/usr/local/etc/redis/redis.conf&quot;]  # 从节点1的容器  redis-server-slave-1:    image: redis    container_name: redis-server-slave-1    restart: always    network_mode: host    depends_on:      - redis-server-master    environment:      TZ: &quot;Asia/Shanghai&quot;    volumes:      - ./redis-slave1.conf:/usr/local/etc/redis/redis.conf      - ../data/redis-slave-1:/data    sysctls:      net.core.somaxconn: &#x27;511&#x27;    command: [&quot;redis-server&quot;, &quot;/usr/local/etc/redis/redis.conf&quot;]  # 从节点2的容器  redis-server-slave-2:    image: redis    container_name: redis-server-slave-2    restart: always    network_mode: host    depends_on:      - redis-server-master    environment:      TZ: &quot;Asia/Shanghai&quot;    volumes:      - ./redis-slave2.conf:/usr/local/etc/redis/redis.conf      - ../data/redis-slave-2:/data    sysctls:      net.core.somaxconn: &#x27;511&#x27;    command: [&quot;redis-server&quot;, &quot;/usr/local/etc/redis/redis.conf&quot;]\n\n启动容器在docker-compose.yml所在位置执行docker-compose up即可启动上述三个容器，docker-compose会将容器日志打印到终端，在日志中可以看到三个Redis服务器在启动过程中的动作，以及从节点加入主节点的信息。\n启动成功后，可以在本机使用redis-cli连接至主节点。连接成功后，可以使用info replication命令检查主从复制的信息。\n127.0.0.1:6379&gt; auth redisOK127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=6381,state=online,offset=14,lag=1slave1:ip=127.0.0.1,port=6380,state=online,offset=14,lag=1master_replid:156ad6d206def2c9d423ef95f5311365d454f57dmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:14second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:14\n\n其中第5行的role:master指示该节点为主节点，第6行的connected_slaves:2说明当前有2个从节点，第7、8行则是两个从节点的信息，包括它们的地址、端口号，和状态。\n如果此时查看该项目的目录结构，则可以发现在data目录中增加了三个Redis服务器的数据目录。\n.├── data│   ├── redis-master│   │   └── dump.rdb│   ├── redis-slave-1│   │   └── dump.rdb│   └── redis-slave-2│       └── dump.rdb└── server    ├── docker-compose.yml    ├── redis-master.conf    └── redis-slave.conf\n\n测试一下光是启动成功还不够，还需要测试一下从节点是否能同步主节点的数据。\n首先连接到主节点，新增一个set：\n127.0.0.1:6379&gt; auth redisOK127.0.0.1:6379&gt; set foo barOK127.0.0.1:6379&gt; get foo&quot;bar&quot;\n\n好的，在主节点里面成功添加了一条数据。那么接下来连接到slave-1，看一下数据有没有同步过去：\n127.0.0.1:6380&gt; auth redisOK127.0.0.1:6380&gt; get foo&quot;bar&quot;127.0.0.1:6380&gt; set foo baz(error) READONLY You can&#x27;t write against a read only replica.\n\n看来slave-1成功的从主节点同步了数据，并且这个节点也按照设定，是一个只读的节点。那么slave-2呢？\n127.0.0.1:6381&gt; auth redisOK127.0.0.1:6381&gt; get foo&quot;bar&quot;127.0.0.1:6381&gt; set foo baz(error) READONLY You can&#x27;t write against a read only replica.\n\nOK，slave-2也成功的同步了数据，并且正在作为一个只读节点运行着。\n参考文档\nConfiguring replication in Docker and NAT - Redis Replication Document\n\n系列博文\n使用Docker配置Redis主从复制\n使用Docker配置Redis哨兵\n\n","categories":["学知识"],"tags":["Redis"]},{"title":"简单理解NTP v3协议","url":"/understanding-ntp-protocol.html","content":"前段时间瞎折腾，给自己的黑莓Bold 9900写了个通过NTP同步时间的小工具，顺便在这里记录一下我在实现一个NTP客户端时对这个协议的理解。\n\n\n端口号NTP协议使用UDP作为传输层协议，服务器监听UDP端口123，在收到有效的报文后，服务器会发送响应报文，否则服务器将直接忽略不做响应。\n时间格式NTP协议使用三种时间格式。\nNTP短时间格式短时间格式长度为32位，其中高16位代表从NTP时间戳0秒至现在的秒数，低16位代表1秒以内的分数部分。这个格式只会在NTP报文的delay和dispersion字段中用到。\n 0                   1                   2                   3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|          Seconds              |           Fraction            |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\nNTP时间戳NTP时间戳格式长度为64位，其中高32位代表从NTP时间戳0秒至现在的秒数，低32位代表1秒以内的分数部分。\n 0                   1                   2                   3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                            Seconds                            |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                            Fraction                           |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\nNTP日期格式NTP日期格式长度为128位，其中高32位用来表示NTP时间纪元，然后用32位表示从当前纪元开始经过的秒数，最后用64位表示1秒以内的分数部分。\n 0                   1                   2                   3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                           Era Number                          |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                           Era Offset                          |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                                                               ||                           Fraction                            ||                                                               |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n报文格式一个NTP v3的报文必须包含如下字段：\n\nLI - Leap Indicator，2 bit整型数，指示当月最后一分钟是否包含闰秒\nVN - Version Number，3 bit整型数，指示NTP协议的版本号。如NTP v3就是3。\nMODE - 3 bit整型数，指示发包方的工作模式。通常来说客户端使用3(client)请求时间，服务端使用4(server)返回时间。\nSTRATUM - 8 bit整型数，代表NTP层数。0代表时钟源，如装备有GPS接收机的主服务器；1-15逐层作为下游服务器，16被定义为“无法同步”。\nPOLL - 8 bit有符号整型数，代表在间隔多少秒后再进行下一次同步。值由log2(second)计算得出。\nPRECISION - 8 bit有符号整型数，代表系统时钟的精确度。\nROOT DELAY - NTP短时间格式，指示从客户端到根服务器(stratum 1的服务器)的延迟。\nROOT DISPERSION - NTP短时间格式，指示数据从根服务器到客户端之间可能引入的误差。\nREFERENCE ID - 32 bit代码，用于标识一个特定的服务器，或一个参考时钟。\n对于stratum 0的数据包，该字段为4个ASCII字符，称作“kiss code”，用于调试和监控。\n对于stratum 1的数据包，该字段为参考时钟的标识符。标识符由IANA维护，此外以“X”开头的标识符都被预留给未注册的试验和开发用途。\n对于stratum 2~15的数据包，该字段为服务器的标识符。当服务器使用IPv4时，该字段为服务器的IP地址；当服务器使用IPv6时，该字段为IPv6地址的前四段。\n\n\nREFERENCE TIMESTAMP - NTP时间戳格式，内容为客户端最后同步的时间。\nORIGIN TIMESTAMP - NTP时间格式，内容为数据包离开客户端的时间。\nRECEIVE TIMESTAMP - NTP时间格式，内容为数据包抵达服务器的时间。\nTRANSMIT TIMESTAMP - NTP时间格式，内容为数据包离开服务器的时间。\nDESTINATION TIMESTAMP - NTP时间格式，内容为数据包抵达客户端的时间。\n注：DESTINATION TIMESTAMP并不会包含在数据包中，而是在客户端收到数据包之后，它的数值才会被确定。\n\n\n\n那么全部组合起来，就是这个样子的：\n 0                   1                   2                   3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|LI | VN  |Mode |    Stratum     |     Poll      |  Precision   |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                         Root Delay                            |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                         Root Dispersion                       |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                          Reference ID                         |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                                                               |+                     Reference Timestamp (64)                  +|                                                               |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                                                               |+                      Origin Timestamp (64)                    +|                                                               |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                                                               |+                      Receive Timestamp (64)                   +|                                                               |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                                                               |+                      Transmit Timestamp (64)                  +|                                                               |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n然而上述字段并不需要全部填写数据，实际上除了LI、VN、MODE、STRATUM之外，剩下的所有字段都可以填零。如下就是一个我用来测试的数据包：\nHEX:DB 10 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n拆开来看的话：\nBIN:LI = 0b11 = 3 unknown (clock unsyncronized)VN = 0b011 = 3MODE = 0b011 = 3 clientSTRATUM = 0b00010000 = 16POLL = 0b00000000 = 0PRECISION = 0b00000000 = 0ROOT DELAY = 0b00000000000000000000000000000000ROOT DISPERSION = 0b00000000000000000000000000000000REFERENCE ID = 00000000000000000000000000000000REFERENCE TIMESTAMP = 00000000000000000000000000000000 00000000000000000000000000000000ORIGIN TIMESTAMP = 00000000000000000000000000000000 00000000000000000000000000000000RECEIVE TIMESTAMP = 00000000000000000000000000000000 00000000000000000000000000000000TRANSMIT TIMESTAMP = 00000000000000000000000000000000 00000000000000000000000000000000\n\n计算second和fraction计算second很简单，取出timestamp的高32位就可以了；但是从fraction计算毫秒数比较麻烦，需要通过fraction * 10^6 / 2^32计算得到毫秒数。\n这里我给出一个Java的代码片段：\nfinal long seconds = (ntpTimestamp &gt;&gt;&gt; 32) &amp; 0xFFFFFFFFL;final long secondsInMilliseconds = seconds * 1000;final long fractionInTimestamp = (ntpTimestamp &amp; 0xFFFFFFFFL);final long milliseconds = fractionInTimestamp * Math.pow(10, 6) / Math.pow(2, 32);\n\n然后计算1900年1月1日 00:00:00的UNIX时间戳作为基准UNIX时间戳，再加上secondsInMilliseconds和milliseconds，就可以得到NTP返回的当前时间了。\n参考文档\nNetwork Time Protocol Version 4: Protocol and Algorithms Specification - RFC\nNetwork Time Protocol (NTP) 网络时间协定 - Jan Ho 的网络世界\nThe Root of All Timing: Understanding root delay and root dispersion in NTP\nNTP Timestamp - Thompson’s Technological Insight\nA Very Short Introduction to NTP Timestamps\nNtpPacketUtils#getNtpTimestampMilliseconds - blackberry_time_sync_ntp - GitHub\n\n","categories":["学知识"],"tags":["NTP"]},{"title":"常用lsof命令备忘","url":"/useful-lsof-commands.html","content":"记录lsof命令常见用法备忘\n\n\n网络# 显示所有网络连接lsof -i\n\n根据协议类型筛选# 仅显示IPv4网络连接lsof -i 4# 仅显示IPv6网络连接lsof -i 6# 仅显示TCP连接lsof -iTCP# 仅显示UDP连接lsof -iUDP\n\n根据目标地址和端口号筛选# 根据目标地址筛选lsof -i@$&#123;HOSTNAME_OR_IP_ADDRESS&#125;# 根据端口号筛选lsof -i :$&#123;PORT_NUMBER&#125;# 组合lsof -i@$&#123;HOSTNAME_OR_IP_ADDRESS&#125;:$&#123;PORT_NUMBER&#125;\n\n根据端口状态筛选lsof -i -sTCP:$&#123;STATE&#125;# 示例lsof -i -sTCP:LISTENlsof -i -sTCP:ESTABLISHED\n\n查看某进程端口占用lsof -p $&#123;PID&#125;\n\n用户# 显示当前用户打开的文件lsof -u $&#123;USER&#125;# 显示除当前用户以外的用户打开的文件lsof -u ^$&#123;USER&#125;\n\n进程# 仅显示PID而不是所有输出信息lsof -t# 根据程序名筛选lsof -c $&#123;COMMAND&#125;\n","categories":["小技巧"],"tags":["Shell","lsof"]},{"title":"梦幻电子宠物 - 雪儿","url":"/virtual-pet-xue-er.html","content":"突然想起来小时候玩过的一个电子宠物游戏《梦幻电子宠物》。\n\n\n\n主窗口\n宠物状态\n关于\n\n\n\n\n\n\n\n\n这个小游戏的存档机制很有意思，是在C盘根目录下创建一个ini文件，记录宠物的出生时间和各项指标，然后它会根据这个时间来计算宠物的状态。也就是说，即使你把游戏关掉，宠物也不会停止成长。所以玩家就必须经常打开游戏照顾这个宠物，不然很有可能下次打开的时候，宠物就已经变成一个墓碑了……\n这个小游戏我上传到蓝奏云了，有兴趣的可以去下载来玩。不过因为64位的Windows已经不支持16位应用，所以你需要一个老版本系统的虚拟机，或者用otvdm来运行。地址：https://boris1993.lanzouj.com/b03queu3c密码：15l3\nP.S.：在找原版的时候，我注意到关于页面里“碧晓拱籍 设计”，以为这是原作者的名字，于是就拿它一顿找啊，但一无所获。突然灵光一现，这会不会是……乱码？鬼使神差把这四个字按照GBK编码保存，然后再用SHIFT-JIS编码打开，得到一串片假名ｱﾌﾏ�ｹｰｼｮ，定睛一看这不就是application的日语アプリケション嘛，虽然里面又掺了个乱码……\n","categories":["忆往昔"],"tags":["电子宠物","梦幻电子宠物","雪儿","小游戏"]},{"title":"使用TRANSMIT和RECEIVE命令打包、解包文件","url":"/zos-xmit.html","content":"当需要从z&#x2F;OS中下载一个PS文件，或者下载一个Member时，我们可以简单地使用FTP或者IND$FILE将文件下载回来，但是如果想要下载一整个PDS呢？总不能一个个地去下载吧，此时，XMIT命令就派上用场了。\n\n\n\nTRANSMIT命令TRANSMIT命令用于将指定文件打包成XMIT档案以方便传输。\n语法使用一条命令前，必须要知道这条命令的语法。那么XMIT命令的语法如下：注：TRANSMIT命令可以简写为XMIT\nXMIT (nodeid.username) DSNAME(&#x27;input.dataset.name&#x27;) [MEMBERS(member1, member2, ...)] OUTDSN(&#x27;output.dataset.name&#x27;)\n其中：\n\n(nodeid.username) 为目标机器的JES2 Node名，以及接收用户的TSOID，不过亲测在这里写自己的Node名和TSOID也能正常使用。通常来说，本机的Node name为N1\nDSNAME 为要打包的数据集名字\n如果只打包这个PDS中的一部分member，则可以在MEMBERS参数中指定。因为目前还没有用过，所以不知道可不可以指定通配符\nOUTDSN 为打包之后输出文件的数据集名字\n\nTRANSMIT命令的官方手册在 这里 ，完整的TRANSMIT命令的语法在 这里 ，参数的详细说明在 这里 。\n另外，根据IBM员工 Isabel Arnold 的建议，在打包之前最好先创建一个 DSORG=FB,LRECL=80,BLKSIZE=3120 的文件供TRANSMIT用作输出文件。\n示例如果我想要打包 IBMUSER.COBOL.SRC 这个PDS中的所有member，打包输出文件名为 IBMUSER.COBOL.SRC.XMIT 那么命令可以这样写：\nXMIT (N1.IBMUSER) DSN(&#x27;IBMUSER.COBOL.SRC&#x27;) OUTDSN(&#x27;IBMUSER.COBOL.SRC.XMIT&#x27;)\n\n\nRECEIVE命令RECEIVE命令用于解包XMIT档案。\n语法同样，这里先展示RECEIVE命令的语法：\nRECEIVE INDSN(&#x27;xmit.dataset.name&#x27;)\n其中 INDSN 为XMIT档案的文件名。\n示例如果现在我在另一台主机上接收到了这个XMIT档案，那么我可以使用如下命令解包这个文件：\nRECEIVE INDSN(&#x27;IBMUSER.COBOL.SRC.XMIT&#x27;)\n\n在RECEIVE命令成功识别指定的XMIT档案之后，会输出如下信息：\nINMR901I Dataset IBMUSER.COBOL.SRC from IBMUSER on NODENAMEINMR906A Enter restore parameters or &#x27;DELETE&#x27; or &#x27;END&#x27; +\n\n此时RECEIVE命令等待用户输入解包信息，我们可以回复如下命令：\nDA(&#x27;IBMUSER.COBOL.SRC&#x27;)\n来将内容解包至 IBMUSER.COBOL.SRC 中。\n此处需要注意的是，如果目标数据集不存在，则RECEIVE会自动创建一个同名数据集，但接下来的解包过程可能会因这个数据集的空间不够用于存放解包出来的文件而报出 ABEND B37 。为避免这种情况发生，建议在RECEIVE前预先创建好需要的数据集，并保证数据集的空间足够。\n在输入DA命令后，RECEIVE将会试图向指定位置解包，并且会将结果输出至终端。\n对INMR906A消息的回复对 INMR906A 的消息，有如下三种回复：\n\nDATASET(‘output.dataset.name’) - 将XMIT档案解包至指定位置，可简写为DA()\nDELETE - 删除该XMIT档案\nEND - 退出，不执行任何操作\n\n参考文档\nTRANSMIT command\nTRANSMIT command syntax\nTRANSMIT command operands\nRECEIVE command\nReceiving Data Sets with the RECEIVE Command\nTransfering Load Modules between Mainframes using XMIT and ftp\n\n","categories":["学知识"],"tags":["Mainframe","z/OS","TRANSMIT","XMIT","RECEIVE"]},{"title":"为UWP的TextBlock实现鼠标悬浮时显示和隐藏内容","url":"/uwp-textblock-show-and-hide-content-on-hover.html","content":"这两天给我的窗边岛项目实现了X岛揭示板网页中的防剧透功能。这个功能本质上就是，当鼠标悬浮在文字上时显示原本的内容，当鼠标移出文字时则用黑块代替。即\n\n\n\nX岛揭示板\n窗边岛\n\n\n\n\n\n\n\n虽然说起来很简单，但是好像网上并没有针对这个需求有什么相关的内容，所以在这里记录下我的实现，权当抛砖引玉。\n\n\n背景在X岛揭示板中，防剧透是通过[h][/h]这个自定义标签实现的，所以上面GIF图中的文字其实是正常文字--[h]防剧透文字[/h]--正常文字--[h]防剧透文字[/h]--。那么这里要做的就有两件事：解析这个自定义标签，以及在TextBlock控件中实现黑块和正常文字的互相替换。\n为TextBlock对象填充内容TextBlock对象有两种填充内容的方式：\n\n直接将内容放入TextBlock.Text属性中。这种方式适合不包含防剧透标签的内容。\n\nnew TextBlock&#123;    VerticalAlignment = VerticalAlignment.Top,    HorizontalAlignment = HorizontalAlignment.Stretch,    Text = content,    TextWrapping = TextWrapping.Wrap,    IsTextSelectionEnabled = textSelectionEnabled,&#125;;\n\n\n将内容分散到各个Run对象中，并将这些Run对象放在TextBlock.Inlines属性中。我就是搭配这种方式实现的防剧透功能。\n\nvar run1 = new Run &#123; Text = &quot;Run 1&quot; &#125;;var run2 = new Run &#123; Text = &quot;Run 2&quot; &#125;;var textBlock = new TextBlock&#123;    VerticalAlignment = VerticalAlignment.Top,    HorizontalAlignment = HorizontalAlignment.Stretch,    TextWrapping = TextWrapping.Wrap,    IsTextSelectionEnabled = textSelectionEnabled,&#125;;textBlock.Inlines.Add(run1);textBlock.Inlines.Add(run2);\n\n保存黑块下的原本内容因为防剧透本质就是，平时用黑块替换掉要遮挡的内容，仅在鼠标悬浮时再用真正的内容替换掉黑块，所以我们需要一个地方来保存原本的内容。本来我想直接在Run对象上下功夫，但是可惜Run不像TextBlock有一个DataContext属性可以放东西，所以最后我还是把目光放在了TextBlock上。\nTextBlock.DataContext是一个object类型的属性，所以我们可以随意放任何我们想放的东西。\n当然为了扩展性考虑，我们最好还是给它创建一个类。\nclass TextBlockDataContext&#123;    // key用来放应该被防剧透的Run在Inline里的下标    // value是这个Run实际的内容    public Dictionary&lt;int, string&gt; IndexAndOriginalTextOfHiddenContent = new Dictionary&lt;int, string&gt;();&#125;\n\n然后我在给一个段落创建TextBlock时，就可以把这个TextBlockDataContext对象放在DataContext属性中备用。\ntextBlock = new TextBlock&#123;    VerticalAlignment = VerticalAlignment.Top,    HorizontalAlignment = HorizontalAlignment.Stretch,    TextWrapping = TextWrapping.Wrap,    IsTextSelectionEnabled = textSelectionEnabled,    DataContext = new TextBlockDataContext &#123; &#125;,&#125;;\n\n解析标签并生成Run对象这部分的思路就是，整行文字会被[h]和[/h]标签切割成各自的Run，因为TextBlock.Inlines是一个有序的列表，所以在切割和生成Run对象时，我可以在TextBlockDataContext.IndexAndOriginalTextOfHiddenContent中记录下要防剧透的Run的下标和它实际的内容。同时，针对要防剧透的Run，我先用黑块字符█填充它的Text属性。\nint indexOfRun = 0;var totalLength = content.Length;var enteredHiddenBlock = false;Run run;while (content.Length &gt; 0)&#123;    var indexOfBeginHideMark = content.IndexOf(&quot;[h]&quot;);    if (!enteredHiddenBlock &amp;&amp; indexOfBeginHideMark &gt;= 0)    &#123;        run = new Run        &#123;            Text = content.Substring(0, indexOfBeginHideMark),        &#125;;        textBlock.Inlines.Add(run);        content = content.Substring(indexOfBeginHideMark + 3);        enteredHiddenBlock = true;        indexOfRun++;        continue;    &#125;    var indexOfEndHideMark = content.IndexOf(&quot;[/h]&quot;);    if (indexOfEndHideMark &gt; 0)    &#123;        var text = content.Substring(0, indexOfEndHideMark);        run = new Run        &#123;            Text = new string(&#x27;█&#x27;, text.Length),        &#125;;        textBlock.Inlines.Add(run);        content = content.Substring(indexOfEndHideMark + 4);        (textBlock.DataContext as TextBlockDataContext).IndexAndOriginalTextOfHiddenContent.Add(indexOfRun, text);        enteredHiddenBlock = false;        indexOfRun++;        continue;    &#125;    run = new Run    &#123;        Text = content,    &#125;;    textBlock.Inlines.Add(run);    indexOfRun++;    break;&#125;\n\n实现鼠标悬浮时显示真实内容TextBlock提供了两个事件PointerEntered和PointerExited，分别对应鼠标指针进入和离开TextBlock范围。所以我们就可以给这两个事件分别绑定UnhidingContent方法和HidingContent方法来实现鼠标悬浮时显示真正内容。\nprivate static void HidingContent(object sender, PointerRoutedEventArgs pointerRoutedEventArgs)&#123;    var textBlock = sender as TextBlock;    if (textBlock.DataContext is TextBlockDataContext dataContext)    &#123;        foreach (var indexAndOriginalText in dataContext.IndexAndOriginalTextOfHiddenContent)        &#123;            var textLength = indexAndOriginalText.Value.Length;            (textBlock.Inlines.ElementAt(indexAndOriginalText.Key) as Run).Text = new string(&#x27;█&#x27;, textLength);        &#125;    &#125;&#125;private static void UnhidingContent(object sender, PointerRoutedEventArgs pointerRoutedEventArgs)&#123;    var textBlock = sender as TextBlock;    if (textBlock.DataContext is TextBlockDataContext dataContext)    &#123;        foreach (var indexAndOriginalText in dataContext.IndexAndOriginalTextOfHiddenContent)        &#123;            (textBlock.Inlines.ElementAt(indexAndOriginalText.Key) as Run).Text = indexAndOriginalText.Value;        &#125;    &#125;&#125;\n\n至此，与X岛揭示板网页端类似的防剧透功能就完成实现了。完整的代码可以参考对应的GitHub commit。\n","categories":["学知识"],"tags":["UWP","TextBlock"]}]